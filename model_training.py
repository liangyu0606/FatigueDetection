import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from data_preprocessing import YawDDPreprocessor

class FatigueDataset(Dataset):
    def __init__(self, sequences, labels):
        self.sequences = sequences
        self.labels = labels

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        return torch.FloatTensor(self.sequences[idx]), torch.FloatTensor([self.labels[idx]])


class SimpleFatigueModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleFatigueModel, self).__init__()

        # ç®€åŒ–çš„LSTMæ¨¡å‹
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=2,
            batch_first=True,
            dropout=0.3,
            bidirectional=True
        )

        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.Tanh(),
            nn.Linear(hidden_size, 1),
            nn.Softmax(dim=1)
        )

        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size * 2, 64),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, output_size)
        )

    def forward(self, x):
        # LSTMå¤„ç†
        lstm_out, _ = self.lstm(x)  # (batch_size, seq_len, hidden_size*2)

        # æ³¨æ„åŠ›æœºåˆ¶
        attention_weights = self.attention(lstm_out)  # (batch_size, seq_len, 1)
        attended_output = torch.sum(lstm_out * attention_weights, dim=1)  # (batch_size, hidden_size*2)

        # åˆ†ç±»
        output = self.classifier(attended_output)
        return output


def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs=50):
    best_val_loss = float('inf')
    best_val_acc = 0.0
    patience = 10
    patience_counter = 0

    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        for inputs, targets in tqdm(train_loader, desc=f"Train Epoch {epoch + 1}"):
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()

            # æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()
            train_loss += loss.item()
            pred = (torch.sigmoid(outputs) > 0.5).float()
            train_total += targets.size(0)
            train_correct += (pred == targets).sum().item()

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        all_outputs = []
        all_targets = []

        with torch.no_grad():
            for inputs, targets in tqdm(val_loader, desc=f"Val Epoch {epoch + 1}"):
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                val_loss += loss.item()
                pred = (torch.sigmoid(outputs) > 0.5).float()
                val_total += targets.size(0)
                val_correct += (pred == targets).sum().item()

                all_outputs.extend(torch.sigmoid(outputs).cpu().numpy())
                all_targets.extend(targets.cpu().numpy())

        # è®¡ç®—æŒ‡æ ‡
        train_acc = 100.0 * train_correct / train_total
        val_acc = 100.0 * val_correct / val_total
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(avg_val_loss)
        current_lr = optimizer.param_groups[0]['lr']

        # æ‰“å°è¯¦ç»†è®­ç»ƒè¿›åº¦
        print(f"Epoch {epoch + 1}/{epochs}")
        print(f"  Train - Loss: {avg_train_loss:.4f}, Acc: {train_acc:.2f}%")
        print(f"  Val   - Loss: {avg_val_loss:.4f}, Acc: {val_acc:.2f}%")
        print(f"  LR: {current_lr:.6f}")
        if all_outputs:
            all_outputs_array = np.array(all_outputs).flatten()
            print(f"  Output range: [{all_outputs_array.min():.3f}, {all_outputs_array.max():.3f}]")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), "models/best_fatigue_model.pth")
            print(f"  *** New best model saved! Val Acc: {best_val_acc:.2f}% ***")
            patience_counter = 0
        else:
            patience_counter += 1

        # æ—©åœ
        if patience_counter >= patience:
            print(f"Early stopping triggered after {patience} epochs without improvement")
            break

        print("-" * 60)

    return model

def save_dataset_by_fps(X, y, fps, output_dir="data"):
    os.makedirs(output_dir, exist_ok=True)
    fps_int = int(round(fps))
    np.save(os.path.join(output_dir, f"X_{fps_int}.npy"), X)
    np.save(os.path.join(output_dir, f"y_{fps_int}.npy"), y)
    print(f"âœ… å·²ä¿å­˜å¸§ç‡ä¸º {fps} çš„æ•°æ®é›†åˆ° {output_dir}/")

def load_dataset_by_fps(fps, data_dir="data"):
    fps_int = int(round(fps))
    x_path = os.path.join(data_dir, f"X_{fps_int}.npy")
    y_path = os.path.join(data_dir, f"y_{fps_int}.npy")

    if not os.path.exists(x_path) or not os.path.exists(y_path):
        return None, None

    X = np.load(x_path)
    y = np.load(y_path)
    print(f"âœ… åŠ è½½äº†å¸§ç‡ä¸º {fps} çš„æ•°æ®é›†: X.shape={X.shape}, y.shape={y.shape}")
    return X, y


def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    target_fps = 60  # ç»Ÿä¸€å¸§ç‡è®¾ç½®
    data_dir = "F:\\College\\Pycharm_project\\PLJC\\dataset"
    predictor_path = "shape_predictor_68_face_landmarks.dat"

    # å°è¯•åŠ è½½å·²æœ‰çš„ç›®æ ‡å¸§ç‡æ•°æ®
    X, y = load_dataset_by_fps(target_fps)
    preprocessor = YawDDPreprocessor(predictor_path, seq_length=target_fps)

    if X is not None:
        print(f"ğŸ“Š ä½¿ç”¨å·²å­˜åœ¨çš„å¸§ç‡ {target_fps} çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒ")
    else:
        print(f"ğŸ”„ æ²¡æœ‰æ‰¾åˆ°å¸§ç‡ {target_fps} çš„æ•°æ®ï¼Œå¼€å§‹é¢„å¤„ç†...")

        # åˆå§‹åŒ–é¢„å¤„ç†å™¨å¹¶åŠ è½½åŸå§‹æ•°æ®
        print("Processing dataset...")
        X, y = preprocessor.process_dataset(data_dir)

        print(f"Dataset info:")
        print(f"  Total samples: {len(X)}")
        print(f"  Feature shape: {X.shape}")
        print(f"  Label distribution: {np.bincount(y)}")

        # ä¿å­˜ä¸ºæŒ‡å®šå¸§ç‡çš„æ ¼å¼
        save_dataset_by_fps(X, y, target_fps)

        print(f"ğŸ’¾ å¸§ç‡ {target_fps} çš„æ•°æ®å·²æˆåŠŸç”Ÿæˆå¹¶åŠ è½½")

    print(f"Dataset info:")
    print(f"  Total samples: {len(X)}")
    print(f"  Feature shape: {X.shape}")
    print(f"  Label distribution: {np.bincount(y)}")

    print("Splitting and scaling data...")
    X_train, X_val, X_test, y_train, y_val, y_test = preprocessor.split_and_scale_data(X, y)

    print(f"Split info:")
    print(f"  Train: {len(X_train)} samples, {np.bincount(y_train)}")
    print(f"  Val: {len(X_val)} samples, {np.bincount(y_val)}")
    print(f"  Test: {len(X_test)} samples, {np.bincount(y_test)}")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = FatigueDataset(X_train, y_train)
    val_dataset = FatigueDataset(X_val, y_val)
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # å‡å°batch size
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)

    # åˆå§‹åŒ–æ¨¡å‹
    input_size = X_train.shape[2]  # ç‰¹å¾ç»´åº¦
    print(f"Model input size: {input_size}")

    model = SimpleFatigueModel(
        input_size=input_size,
        hidden_size=32,  # æ›´å°çš„éšè—å±‚
        output_size=1
    ).to(device)

    # æ‰“å°æ¨¡å‹å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")

    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    # ä½¿ç”¨åŠ æƒæŸå¤±æ¥å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
    pos_weight = torch.tensor([len(y_train) / (2 * np.sum(y_train))]).to(device)
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)

    # æ³¨æ„ï¼šSimpleFatigueModelçš„classifieræœ€åä¸€å±‚å·²ç»æ˜¯Linear(32, output_size)
    # BCEWithLogitsLosså†…éƒ¨åŒ…å«Sigmoidï¼Œæ‰€ä»¥ä¸éœ€è¦é¢å¤–ä¿®æ”¹

    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5, verbose=True
    )

    # è®­ç»ƒæ¨¡å‹
    print("Starting model training...")
    os.makedirs("models", exist_ok=True)
    trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs=100)

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save(trained_model.state_dict(), "models/final_fatigue_model.pth")
    print("Model training completed and saved!")


if __name__ == "__main__":
    main()