"""
è®­ç»ƒè„šæœ¬ - æ‰“å“ˆæ¬ æ£€æµ‹æ¨¡å‹
"""
import torch
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import time
from datetime import datetime

from config import *
from model import create_model, ImprovedFocalLoss, count_parameters
from dataset import load_processed_data, create_data_loaders, analyze_dataset
from utils import setup_logging

class YawnDetectionTrainer:
    def __init__(self, resume_from_checkpoint=False):
        self.logger = setup_logging()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

        # åˆ›å»ºæ¨¡å‹
        self.model = create_model().to(self.device)
        self.logger.info(f"æ¨¡å‹å‚æ•°æ•°é‡: {count_parameters(self.model):,}")

        # æ”¹è¿›çš„æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        # è®¡ç®—ç±»åˆ«æƒé‡
        class_weights = torch.tensor([1.0, 3.0]).to(self.device)  # ç»™æ‰“å“ˆæ¬ ç±»åˆ«æ›´é«˜æƒé‡
        self.criterion = ImprovedFocalLoss(alpha=class_weights, gamma=2, label_smoothing=0.1)

        # ä½¿ç”¨AdamWä¼˜åŒ–å™¨ï¼Œæ›´å¥½çš„æƒé‡è¡°å‡
        self.optimizer = optim.AdamW(self.model.parameters(), lr=LEARNING_RATE, weight_decay=1e-3)

        # æ”¹è¿›çš„å­¦ä¹ ç‡è°ƒåº¦
        self.scheduler = optim.lr_scheduler.OneCycleLR(
            self.optimizer,
            max_lr=LEARNING_RATE * 3,
            epochs=NUM_EPOCHS,
            steps_per_epoch=1,  # å°†åœ¨è®­ç»ƒä¸­æ›´æ–°
            pct_start=0.3,
            anneal_strategy='cos'
        )

        # è®­ç»ƒè®°å½•
        self.train_losses = []
        self.val_losses = []
        self.train_accuracies = []
        self.val_accuracies = []
        self.best_val_acc = 0.0
        self.best_model_path = None
        self.start_epoch = 0

        # æ–­ç‚¹è®­ç»ƒ
        if resume_from_checkpoint:
            self.load_checkpoint()

        # TensorBoard
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        if resume_from_checkpoint:
            timestamp += "_resumed"
        self.writer = SummaryWriter(log_dir=os.path.join(LOG_PATH, f"run_{timestamp}"))
    
    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        all_predictions = []
        all_labels = []
        
        pbar = tqdm(train_loader, desc="è®­ç»ƒ")
        for batch_idx, (faces, landmarks, labels) in enumerate(pbar):
            faces = faces.to(self.device)
            landmarks = landmarks.to(self.device)
            labels = labels.to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            outputs = self.model(faces, landmarks)
            loss = self.criterion(outputs, labels)
            
            # åå‘ä¼ æ’­
            loss.backward()
            # æ›´å¼ºçš„æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            predictions = torch.argmax(outputs, dim=1)
            all_predictions.extend(predictions.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Avg Loss': f'{total_loss/(batch_idx+1):.4f}'
            })
        
        avg_loss = total_loss / len(train_loader)
        accuracy = accuracy_score(all_labels, all_predictions)
        
        return avg_loss, accuracy
    
    def validate_epoch(self, val_loader):
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0.0
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            pbar = tqdm(val_loader, desc="éªŒè¯")
            for faces, landmarks, labels in pbar:
                faces = faces.to(self.device)
                landmarks = landmarks.to(self.device)
                labels = labels.to(self.device)
                
                outputs = self.model(faces, landmarks)
                loss = self.criterion(outputs, labels)
                
                total_loss += loss.item()
                predictions = torch.argmax(outputs, dim=1)
                all_predictions.extend(predictions.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                
                pbar.set_postfix({'Loss': f'{loss.item():.4f}'})
        
        avg_loss = total_loss / len(val_loader)
        accuracy = accuracy_score(all_labels, all_predictions)
        
        # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
        precision, recall, f1, _ = precision_recall_fscore_support(
            all_labels, all_predictions, average='weighted'
        )
        
        return avg_loss, accuracy, precision, recall, f1, all_labels, all_predictions
    
    def save_model(self, epoch, val_acc, is_best=False):
        """ä¿å­˜æ¨¡å‹ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼Œè°ƒç”¨save_checkpointï¼‰"""
        self.save_checkpoint(epoch, val_acc, is_best)

    def load_checkpoint(self, checkpoint_path=None):
        """åŠ è½½æ£€æŸ¥ç‚¹è¿›è¡Œæ–­ç‚¹è®­ç»ƒ"""
        if checkpoint_path is None:
            # é»˜è®¤åŠ è½½æœ€æ–°çš„æ£€æŸ¥ç‚¹
            checkpoint_path = os.path.join(MODEL_SAVE_PATH, 'latest_model.pth')

        if not os.path.exists(checkpoint_path):
            self.logger.warning(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            return False

        try:
            self.logger.info(f"åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location=self.device)

            # åŠ è½½æ¨¡å‹çŠ¶æ€
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

            # åŠ è½½å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'scheduler_state_dict' in checkpoint:
                self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

            # åŠ è½½è®­ç»ƒè®°å½•
            self.start_epoch = checkpoint['epoch'] + 1
            self.best_val_acc = checkpoint.get('val_acc', 0.0)
            self.train_losses = checkpoint.get('train_losses', [])
            self.val_losses = checkpoint.get('val_losses', [])
            self.train_accuracies = checkpoint.get('train_accuracies', [])
            self.val_accuracies = checkpoint.get('val_accuracies', [])

            self.logger.info(f"æˆåŠŸåŠ è½½æ£€æŸ¥ç‚¹ï¼Œä»ç¬¬ {self.start_epoch} è½®å¼€å§‹è®­ç»ƒ")
            self.logger.info(f"å½“å‰æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.4f}")
            return True

        except Exception as e:
            self.logger.error(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return False

    def save_checkpoint(self, epoch, val_acc, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆæ”¹è¿›ç‰ˆä¿å­˜æ–¹æ³•ï¼‰"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_acc': val_acc,
            'best_val_acc': self.best_val_acc,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_accuracies': self.train_accuracies,
            'val_accuracies': self.val_accuracies,
            'timestamp': datetime.now().isoformat(),
            'config': {
                'learning_rate': LEARNING_RATE,
                'batch_size': BATCH_SIZE,
                'sequence_length': SEQUENCE_LENGTH,
                'num_epochs': NUM_EPOCHS
            }
        }

        # ä¿å­˜æœ€æ–°æ¨¡å‹
        latest_path = os.path.join(MODEL_SAVE_PATH, 'latest_model.pth')
        torch.save(checkpoint, latest_path)
        self.logger.info(f"ä¿å­˜æ£€æŸ¥ç‚¹: {latest_path}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = os.path.join(MODEL_SAVE_PATH, 'best_model.pth')
            torch.save(checkpoint, best_path)
            self.best_model_path = best_path
            self.logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")

        # å®šæœŸä¿å­˜å¸¦æ—¶é—´æˆ³çš„å¤‡ä»½
        if epoch % 10 == 0:  # æ¯10è½®ä¿å­˜ä¸€æ¬¡å¤‡ä»½
            backup_path = os.path.join(MODEL_SAVE_PATH, f'checkpoint_epoch_{epoch}.pth')
            torch.save(checkpoint, backup_path)
            self.logger.info(f"ä¿å­˜å¤‡ä»½æ£€æŸ¥ç‚¹: {backup_path}")
    
    def plot_training_curves(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # æŸå¤±æ›²çº¿
        ax1.plot(self.train_losses, label='è®­ç»ƒæŸå¤±')
        ax1.plot(self.val_losses, label='éªŒè¯æŸå¤±')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
        ax1.legend()
        ax1.grid(True)
        
        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(self.train_accuracies, label='è®­ç»ƒå‡†ç¡®ç‡')
        ax2.plot(self.val_accuracies, label='éªŒè¯å‡†ç¡®ç‡')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.set_title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡')
        ax2.legend()
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig(os.path.join(OUTPUT_DIR, 'training_curves.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_confusion_matrix(self, y_true, y_pred, epoch):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        cm = confusion_matrix(y_true, y_pred)
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=['æ­£å¸¸/è¯´è¯', 'æ‰“å“ˆæ¬ '],
                   yticklabels=['æ­£å¸¸/è¯´è¯', 'æ‰“å“ˆæ¬ '])
        plt.title(f'æ··æ·†çŸ©é˜µ - Epoch {epoch}')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.savefig(os.path.join(OUTPUT_DIR, f'confusion_matrix_epoch_{epoch}.png'),
                   dpi=300, bbox_inches='tight')
        plt.close()

    def train(self, train_loader, val_loader, num_epochs=None):
        if num_epochs is None:
            num_epochs = DEBUG_EPOCHS if DEBUG_MODE else NUM_EPOCHS

        # å¦‚æœæ˜¯æ–­ç‚¹è®­ç»ƒï¼Œéœ€è¦é‡æ–°åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.start_epoch > 0:
            self.logger.info(f"æ–­ç‚¹è®­ç»ƒï¼šä»ç¬¬ {self.start_epoch} è½®å¼€å§‹ï¼Œæ€»å…± {num_epochs} è½®")
            # é‡æ–°åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œè€ƒè™‘å·²ç»è®­ç»ƒçš„è½®æ¬¡
            remaining_epochs = num_epochs - self.start_epoch
            if remaining_epochs <= 0:
                self.logger.warning("å·²è¾¾åˆ°æˆ–è¶…è¿‡ç›®æ ‡è®­ç»ƒè½®æ¬¡")
                return

            self.scheduler = optim.lr_scheduler.OneCycleLR(
                self.optimizer,
                max_lr=LEARNING_RATE * 3,
                epochs=remaining_epochs,
                steps_per_epoch=len(train_loader),
                pct_start=0.3,
                anneal_strategy='cos'
            )
        else:
            # æ­£å¸¸è®­ç»ƒï¼Œæ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨çš„steps_per_epoch
            self.scheduler = optim.lr_scheduler.OneCycleLR(
                self.optimizer,
                max_lr=LEARNING_RATE * 3,
                epochs=num_epochs,
                steps_per_epoch=len(train_loader),
                pct_start=0.3,
                anneal_strategy='cos'
            )
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        if self.start_epoch > 0:
            self.logger.info(f"ç»§ç»­è®­ç»ƒ... ä»ç¬¬ {self.start_epoch} è½®å¼€å§‹")
        else:
            self.logger.info("å¼€å§‹è®­ç»ƒ...")
        start_time = time.time()

        for epoch in range(self.start_epoch, num_epochs):
            epoch_start_time = time.time()

            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_loader)

            # éªŒè¯
            val_loss, val_acc, val_precision, val_recall, val_f1, y_true, y_pred = self.validate_epoch(val_loader)

            # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆOneCycleLRæ¯ä¸ªepochè°ƒç”¨ä¸€æ¬¡ï¼‰
            self.scheduler.step()

            # è®°å½•
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.train_accuracies.append(train_acc)
            self.val_accuracies.append(val_acc)

            # TensorBoardè®°å½•
            self.writer.add_scalar('Loss/Train', train_loss, epoch)
            self.writer.add_scalar('Loss/Val', val_loss, epoch)
            self.writer.add_scalar('Accuracy/Train', train_acc, epoch)
            self.writer.add_scalar('Accuracy/Val', val_acc, epoch)
            self.writer.add_scalar('Precision/Val', val_precision, epoch)
            self.writer.add_scalar('Recall/Val', val_recall, epoch)
            self.writer.add_scalar('F1/Val', val_f1, epoch)
            self.writer.add_scalar('Learning_Rate', self.optimizer.param_groups[0]['lr'], epoch)

            # ä¿å­˜æ¨¡å‹
            is_best = val_acc > self.best_val_acc
            if is_best:
                self.best_val_acc = val_acc

            self.save_model(epoch, val_acc, is_best)

            # æ¯10ä¸ªepochç»˜åˆ¶æ··æ·†çŸ©é˜µ
            if (epoch + 1) % 10 == 0:
                self.plot_confusion_matrix(y_true, y_pred, epoch + 1)

            # æ‰“å°è¿›åº¦
            epoch_time = time.time() - epoch_start_time
            self.logger.info(
                f"Epoch [{epoch+1}/{num_epochs}] - "
                f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, "
                f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, "
                f"Val F1: {val_f1:.4f}, Time: {epoch_time:.2f}s"
            )

        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        self.logger.info(f"è®­ç»ƒå®Œæˆ! æ€»æ—¶é—´: {total_time/3600:.2f}å°æ—¶")
        self.logger.info(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.4f}")

        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_training_curves()

        # å…³é—­TensorBoard
        self.writer.close()

        return self.best_model_path

def main(resume_training=False):
    """ä¸»å‡½æ•°"""
    if resume_training:
        print("ğŸ”„ ç»§ç»­è®­ç»ƒç–²åŠ³æ£€æµ‹æ¨¡å‹")
    else:
        print("ğŸš€ å¼€å§‹è®­ç»ƒç–²åŠ³æ£€æµ‹æ¨¡å‹")
    print("="*50)

    # æŒ‰ä¼˜å…ˆçº§æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
    data_files = [
        os.path.join(PROCESSED_DATA_PATH, "balanced_debug_samples.pkl"),  # å¹³è¡¡è°ƒè¯•æ•°æ®
        os.path.join(PROCESSED_DATA_PATH, "debug_samples.pkl"),           # è°ƒè¯•æ•°æ®
        os.path.join(PROCESSED_DATA_PATH, "processed_samples.pkl")        # æ­£å¼æ•°æ®
    ]

    data_path = None
    for file_path in data_files:
        if os.path.exists(file_path):
            data_path = file_path
            print(f"æ‰¾åˆ°æ•°æ®æ–‡ä»¶: {data_path}")
            break

    if data_path is None:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é¢„å¤„ç†æ•°æ®æ–‡ä»¶ï¼")
        print("è¯·å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤ä¹‹ä¸€:")
        print("  python main.py --mode preprocess --debug  # ç”Ÿæˆè°ƒè¯•æ•°æ®")
        print("  python analyze_data.py                     # ç”Ÿæˆå¹³è¡¡æ•°æ®")
        print("  python main.py --mode preprocess           # ç”Ÿæˆå®Œæ•´æ•°æ®")
        return

    print("åŠ è½½é¢„å¤„ç†æ•°æ®...")
    samples = load_processed_data(data_path)

    # åˆ†ææ•°æ®é›†
    analyze_dataset(samples)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    train_loader, val_loader, test_loader = create_data_loaders(samples)

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = YawnDetectionTrainer(resume_from_checkpoint=resume_training)

    # å¼€å§‹è®­ç»ƒ
    best_model_path = trainer.train(train_loader, val_loader)

    print(f"è®­ç»ƒå®Œæˆ! æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨: {best_model_path}")

if __name__ == "__main__":
    main()
