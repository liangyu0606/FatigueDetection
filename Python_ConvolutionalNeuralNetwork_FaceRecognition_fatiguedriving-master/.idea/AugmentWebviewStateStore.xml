<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="AugmentWebviewStateStore">
    <option name="stateMap">
      <map>
        <entry key="CHAT_STATE" value="{&quot;currentConversationId&quot;:&quot;e0d5dab0-ce51-41f1-a48d-4e01b397e9da&quot;,&quot;conversations&quot;:{&quot;f6c5ccba-ce36-41e2-ba3b-37347dadd699&quot;:{&quot;id&quot;:&quot;f6c5ccba-ce36-41e2-ba3b-37347dadd699&quot;,&quot;createdAtIso&quot;:&quot;2025-07-04T07:10:07.725Z&quot;,&quot;lastInteractedAtIso&quot;:&quot;2025-07-04T07:10:07.725Z&quot;,&quot;chatHistory&quot;:[],&quot;feedbackStates&quot;:{},&quot;toolUseStates&quot;:{},&quot;draftExchange&quot;:{&quot;request_message&quot;:&quot;&quot;,&quot;rich_text_json_repr&quot;:{&quot;type&quot;:&quot;doc&quot;,&quot;content&quot;:[{&quot;type&quot;:&quot;paragraph&quot;}]},&quot;mentioned_items&quot;:[],&quot;status&quot;:&quot;draft&quot;},&quot;requestIds&quot;:[],&quot;isPinned&quot;:false,&quot;isShareable&quot;:false,&quot;extraData&quot;:{&quot;hasDirtyEdits&quot;:false},&quot;personaType&quot;:0,&quot;rootTaskUuid&quot;:&quot;608ba648-c9b7-44d0-93b7-a8603d061a87&quot;},&quot;e0d5dab0-ce51-41f1-a48d-4e01b397e9da&quot;:{&quot;id&quot;:&quot;e0d5dab0-ce51-41f1-a48d-4e01b397e9da&quot;,&quot;createdAtIso&quot;:&quot;2025-07-04T07:10:07.828Z&quot;,&quot;lastInteractedAtIso&quot;:&quot;2025-07-04T07:17:21.886Z&quot;,&quot;chatHistory&quot;:[{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;fd1bfaa4-5aaf-4ab5-89a1-8dd464f3d334&quot;,&quot;request_message&quot;:&quot;main.py中检测打哈欠的模型定义是怎么定义的&quot;,&quot;rich_text_json_repr&quot;:{&quot;type&quot;:&quot;doc&quot;,&quot;content&quot;:[{&quot;type&quot;:&quot;paragraph&quot;,&quot;content&quot;:[{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;main.py中检测打哈欠的模型定义是怎么定义的&quot;}]}]},&quot;mentioned_items&quot;:[],&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_01RQTUrKLnrCT4tCagEYcUem&quot;,&quot;tool_name&quot;:&quot;view&quot;,&quot;input_json&quot;:&quot;{\&quot;path\&quot;: \&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot;, \&quot;type\&quot;: \&quot;file\&quot;}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;我来帮您查看main.py文件中打哈欠检测模型的定义。让我先检查这个文件的内容。\n&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:0,&quot;type&quot;:0,&quot;text_node&quot;:{&quot;content&quot;:&quot;main.py中检测打哈欠的模型定义是怎么定义的&quot;}},{&quot;id&quot;:1,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;D:/FatigueDetection&quot;,&quot;folderRoot&quot;:&quot;D:/FatigueDetection&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-04T07:10:46.195Z&quot;,&quot;response_text&quot;:&quot;我来帮您查看main.py文件中打哈欠检测模型的定义。让我先检查这个文件的内容。\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;67ba8c2a-ffdc-47e0-9a76-ac469d8768bc&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_01LQY4oCJQh7aJ3U8V5RcT9L&quot;,&quot;tool_name&quot;:&quot;view&quot;,&quot;input_json&quot;:&quot;{\&quot;path\&quot;: \&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot;, \&quot;type\&quot;: \&quot;file\&quot;, \&quot;search_query_regex\&quot;: \&quot;class YawnCNNLSTM|def __init__|CNN\\\\+LSTM|\\u6253\\u54c8\\u6b20\\u68c0\\u6d4b\&quot;}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;让我继续查看文件的其余部分，特别是关于打哈欠检测模型的部分：\n&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;Here's the result of running `cat -n` on Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py:\n     1\timport datetime\n     2\timport math\n     3\timport os\n     4\timport sys\n     5\timport threading\n     6\timport time\n     7\t\n     8\timport cv2\n     9\timport dlib\n    10\timport numpy as np\n    11\timport pygame\n    12\tfrom PySide6 import QtCore, QtGui, QtWidgets\n    13\tfrom PySide6.QtGui import QImage\n    14\tfrom PySide6.QtCore import Qt\n    15\tfrom imutils import face_utils\n    16\tfrom scipy.spatial import distance as dist\n    17\tfrom camera_config import camera_config\n    18\t\n    19\t# 尝试导入深度学习库，如果失败则使用占位符\n    20\ttry:\n    21\t    # 首先定义TENSORFLOW_AVAILABLE变量\n    22\t    TENSORFLOW_AVAILABLE = False\n    23\t\n    24\t    import tensorflow as tf\n    25\t    from tensorflow.keras.applications import ResNet50\n    26\t    from tensorflow.keras.models import Sequential\n    27\t    from tensorflow.keras.layers import LSTM, Dense, Conv2D, MaxPooling2D, Flatten\n    28\t\n    29\t    # 如果导入成功，设置TENSORFLOW_AVAILABLE为True\n    30\t    TENSORFLOW_AVAILABLE = True\n    31\t\n    32\t    # 只有当TensorFlow可用时才尝试导入Input层\n    33\t    if TENSORFLOW_AVAILABLE:\n    34\t        from tensorflow.keras.layers import Input\n    35\t        pass\n    36\t\n    37\texcept ImportError as e:\n    38\t    print(f\&quot;TensorFlow导入失败: {e}\&quot;)\n    39\t    TENSORFLOW_AVAILABLE = False\n    40\t\n    41\t# 尝试导入PyTorch用于CNN+LSTM打哈欠检测\n    42\ttry:\n    43\t    import torch\n    44\t    import torch.nn as nn\n    45\t    PYTORCH_AVAILABLE = True\n    46\t    print(\&quot;✅ PyTorch可用，将启用CNN+LSTM打哈欠检测\&quot;)\n    47\texcept ImportError as e:\n    48\t    print(f\&quot;PyTorch导入失败: {e}\&quot;)\n    49\t    PYTORCH_AVAILABLE = False\n    50\t\n    51\t\n    52\t# CNN+LSTM模型定义（用于打哈欠检测）\n    53\tif PYTORCH_AVAILABLE:\n    54\t    class YawnCNNLSTM(nn.Module):\n    55\t        \&quot;\&quot;\&quot;专门用于打哈欠检测的CNN+LSTM模型\&quot;\&quot;\&quot;\n    56\t        def __init__(self, input_size, hidden_size, num_layers, output_size):\n    57\t            super(YawnCNNLSTM, self).__init__()\n    58\t\n    59\t            # CNN layers - Extract spatial features\n    60\t            self.cnn = nn.Sequential(\n    61\t                nn.Conv1d(input_size, 64, kernel_size=3, padding=1),\n    62\t                nn.BatchNorm1d(64),\n    63\t                nn.ReLU(),\n    64\t                nn.MaxPool1d(kernel_size=2),\n    65\t                nn.Conv1d(64, 128, kernel_size=3, padding=1),\n    66\t                nn.BatchNorm1d(128),\n    67\t                nn.ReLU(),\n    68\t                nn.MaxPool1d(kernel_size=2),\n    69\t                nn.Conv1d(128, 256, kernel_size=3, padding=1),\n    70\t                nn.BatchNorm1d(256),\n    71\t                nn.ReLU(),\n    72\t                nn.MaxPool1d(kernel_size=2)\n    73\t            )\n    74\t\n    75\t            # CNN output channels is 256\n    76\t            cnn_output_size = 256\n    77\t\n    78\t            # LSTM layers - Process temporal features\n    79\t            self.lstm = nn.LSTM(\n    80\t                input_size=cnn_output_size,\n    81\t                hidden_size=hidden_size,\n    82\t                num_layers=num_layers,\n    83\t                batch_first=True,\n    84\t                dropout=0.2,\n    85\t                bidirectional=True\n    86\t            )\n    87\t\n    88\t            # Fully connected layers - Output classification results\n    89\t            self.fc = nn.Sequential(\n    90\t                nn.Linear(hidden_size * 2, 128),\n    91\t                nn.ReLU(),\n    92\t                nn.Dropout(0.5),\n    93\t                nn.Linear(128, 64),\n    94\t                nn.ReLU(),\n    95\t                nn.Dropout(0.3),\n    96\t                nn.Linear(64, output_size)\n    97\t            )\n    98\t\n    99\t        def forward(self, x):\n   100\t            batch_size, seq_len, features = x.size()\n   101\t\n   102\t            # 重塑输入以通过CNN: (batch_size, features, seq_len)\n   103\t            x = x.permute(0, 2, 1)\n   104\t\n   105\t            # 通过CNN\n   106\t            cnn_out = self.cnn(x)\n   107\t\n   108\t            # 重塑CNN输出以通过LSTM: (batch_size, seq_len, cnn_features)\n   109\t            cnn_out = cnn_out.permute(0, 2, 1)\n   110\t\n   111\t            # 通过LSTM\n   112\t            lstm_out, _ = self.lstm(cnn_out)\n   113\t\n   114\t            # 取最后一个时间步的输出\n   115\t            lstm_out = lstm_out[:, -1, :]\n   116\t\n   117\t            # 通过全连接层\n   118\t            output = self.fc(lstm_out)\n   119\t\n   120\t            return output\n   121\t\n   122\t\n   123\t    class YawnDetector:\n   124\t        \&quot;\&quot;\&quot;专门用于打哈欠检测的类\&quot;\&quot;\&quot;\n   125\t        def __init__(self, model_path=None, seq_length=30, consecutive_frames=15):\n   126\t            self.model = None\n   127\t            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if PYTORCH_AVAILABLE else None\n   128\t            self.seq_length = seq_length\n   129\t            self.features_buffer = []\n   130\t            self.is_available = False\n   131\t\n   132\t            # 添加连续帧判断逻辑（参考real_pljc）\n   133\t            self.consecutive_frames = consecutive_frames\n   134\t            self.fatigue_frames = 0\n   135\t            self.frame_count = 0\n   136\t\n   137\t            if PYTORCH_AVAILABLE and model_path and os.path.exists(model_path):\n   138\t                self.load_model(model_path)\n   139\t\n   140\t        def load_model(self, model_path):\n   141\t            \&quot;\&quot;\&quot;加载训练好的CNN+LSTM模型\&quot;\&quot;\&quot;\n   142\t            try:\n   143\t                # 初始化模型（与训练时的结构保持一致）\n   144\t                self.model = YawnCNNLSTM(\n   145\t                    input_size=138,  # 2 (EAR, MAR) + 68*2 (landmark coordinates)\n   146\t                    hidden_size=64,\n   147\t                    num_layers=1,\n   148\t                    output_size=1\n   149\t                ).to(self.device)\n   150\t\n   151\t                # 加载模型权重\n   152\t                self.model.load_state_dict(torch.load(model_path, map_location=self.device, weights_only=False))\n   153\t                self.model.eval()\n   154\t                self.is_available = True\n   155\t                print(f\&quot;✅ CNN+LSTM打哈欠检测模型加载成功: {model_path}\&quot;)\n   156\t\n   157\t            except Exception as e:\n   158\t                print(f\&quot;❌ CNN+LSTM打哈欠检测模型加载失败: {e}\&quot;)\n   159\t                self.model = None\n   160\t                self.is_available = False\n   161\t\n   162\t        def extract_features(self, landmarks, ear, mar, frame_height=480):\n   163\t            \&quot;\&quot;\&quot;提取特征向量（与real_pljc保持一致）\&quot;\&quot;\&quot;\n   164\t            if landmarks is None:\n   165\t                return None\n   166\t\n   167\t            try:\n   168\t                # 归一化关键点坐标（以鼻尖为基准，使用帧高度归一化）\n   169\t                nose = landmarks[30]  # 鼻尖关键点\n   170\t                normalized_landmarks = (landmarks - nose).flatten() / frame_height  # 使用帧高度归一化\n   171\t\n   172\t                # 组合特征：EAR, MAR + 68个关键点的x,y坐标\n   173\t                features = np.concatenate([[ear, mar], normalized_landmarks])\n   174\t                return features\n   175\t\n   176\t            except Exception as e:\n   177\t                print(f\&quot;特征提取失败: {e}\&quot;)\n   178\t                return None\n   179\t\n   180\t        def update_buffer(self, features):\n   181\t            \&quot;\&quot;\&quot;更新特征缓冲区\&quot;\&quot;\&quot;\n   182\t            if features is not None:\n   183\t                self.features_buffer.append(features)\n   184\t                if len(self.features_buffer) &gt; self.seq_length:\n   185\t                    self.features_buffer.pop(0)\n   186\t\n   187\t        def predict_yawn(self, detection_enabled=True):\n   188\t            \&quot;\&quot;\&quot;预测是否打哈欠（与real_pljc保持一致的逻辑）\&quot;\&quot;\&quot;\n   189\t            if not self.is_available or len(self.features_buffer) &lt; self.seq_length:\n   190\t                return False, 0.0\n   191\t\n   192\t            try:\n   193\t                # 准备输入序列\n   194\t                input_seq = np.array([self.features_buffer])\n   195\t                input_tensor = torch.FloatTensor(input_seq).to(self.device)\n   196\t\n   197\t                # 模型预测\n   198\t                with torch.no_grad():\n   199\t                    logits = self.model(input_tensor).item()\n   200\t                    prediction = torch.sigmoid(torch.tensor(logits)).item()\n   201\t\n   202\t                # 更新疲劳状态（参考real_pljc的连续帧判断逻辑）\n   203\t                self.frame_count += 1\n   204\t                if prediction &gt;= 0.5:  # 单帧预测阈值\n   205\t                    self.fatigue_frames += 1\n   206\t                else:\n   207\t                    self.fatigue_frames = 0\n   208\t\n   209\t                # 判定疲劳需要连续帧数达到阈值\n   210\t                is_fatigued = self.fatigue_frames &gt;= self.consecutive_frames\n   211\t\n   212\t                # 如果检测被禁用（冷却期），重置连续帧计数\n   213\t                if not detection_enabled and is_fatigued:\n   214\t                    self.fatigue_frames = 0\n   215\t                    is_fatigued = False\n   216\t\n   217\t                return is_fatigued, prediction\n   218\t\n   219\t            except Exception as e:\n   220\t                print(f\&quot;CNN+LSTM打哈欠预测失败: {e}\&quot;)\n   221\t                return False, 0.0\n   222\t\n   223\t        def reset_state(self):\n   224\t            \&quot;\&quot;\&quot;重置检测器状态（用于冷却期）\&quot;\&quot;\&quot;\n   225\t            self.fatigue_frames = 0\n   226\t            self.frame_count = 0\n   227\t\n   228\t\n   229\t# 如果PyTorch不可用，定义占位符类\n   230\tif not PYTORCH_AVAILABLE:\n   231\t    class YawnCNNLSTM:\n   232\t        def __init__(self, *args, **kwargs):\n   233\t            pass\n   234\t\n   235\t    class YawnDetector:\n   236\t        def __init__(self, *args, **kwargs):\n   237\t            self.is_available = False\n   238\t\n   239\t\n   240\t# 处理TensorFlow导入失败的情况\n   241\ttry:\n   242\t    # 这个try块是为了兼容原有的except ImportError\n   243\t    pass\n   244\texcept ImportError:\n   245\t    # 如果导入失败，定义基本占位符\n   246\t    class Sequential:\n   247\t        def __init__(self):\n   248\t            pass\n   249\t\n   250\t        def add(self, layer):\n   251\t            pass\n   252\t\n   253\t    class ResNet50:\n   254\t        def __init__(self, *args, **kwargs):\n   255\t            pass\n   256\t\n   257\t    class LSTM:\n   258\t        def __init__(self, *args, **kwargs):\n   259\t            pass\n   260\t\n   261\t    class Dense:\n   262\t        def __init__(self, *args, **kwargs):\n   263\t            pass\n   264\t\n   265\t    class Conv2D:\n   266\t        def __init__(self, *args, **kwargs):\n   267\t            pass\n   268\t\n   269\t    class MaxPooling2D:\n   270\t        def __init__(self, *args, **kwargs):\n   271\t            pass\n   272\t\n   273\t    class Flatten:\n   274\t        def __init__(self, *args, **kwargs):\n   275\t            pass\n   276\t\n   277\t    tf = None\n   278\t    TENSORFLOW_AVAILABLE = False\n   279\t\n   280\timport main_ui\n   281\t\n   282\t\n   283\t# pyside6-uic -o main_ui.py main.ui\n   284\t\n   285\tclass MainUI(QtWidgets.QWidget, main_ui.Ui_Form):\n   286\t    # 信号，在UI线程中，不能在其他线程直接操作UI\n   287\t    thread_signal = QtCore.Signal(dict)\n   288\t\n   289\t    def __init__(self):\n   290\t        super().__init__()\n   291\t        self.setupUi(self)\n   292\t\n   293\t        # 初始化摄像头列表\n   294\t        self.cameras = []\n   295\t        self.init_camera_list()\n   296\t\n   297\t        # 连接信号\n   298\t        self.pushButton.clicked.connect(self.button_clicked)\n   299\t\n   300\t        # 初始化疲劳统计模块\n   301\t        try:\n   302\t            from fatigue_statistics import FatigueStatistics\n   303\t            self.fatigue_stats = FatigueStatistics()\n   304\t            print(\&quot;✅ 疲劳统计模块已初始化\&quot;)\n   305\t        except Exception as e:\n   306\t            print(f\&quot;⚠️ 疲劳统计模块初始化失败: {e}\&quot;)\n   307\t            self.fatigue_stats = None\n   308\t\n   309\t        # 初始化深度学习模型\n   310\t        self.resnet = None\n   311\t        self.cnn_model = None\n   312\t        self.lstm_model = None\n   313\t\n   314\t        # 初始化CNN疲劳检测器\n   315\t        self.cnn_detector = None\n   316\t\n   317\t        # 初始化CNN+LSTM打哈欠检测器\n   318\t        self.yawn_detector = None\n   319\t\n   320\t        if TENSORFLOW_AVAILABLE:\n   321\t            try:\n   322\t                self.init_models()\n   323\t                self.init_cnn_detector()\n   324\t            except Exception as e:\n   325\t                print(f\&quot;模型初始化失败: {e}\&quot;)\n   326\t        else:\n   327\t            print(\&quot;TensorFlow未安装，使用简化功能\&quot;)\n   328\t\n   329\t        # 初始化CNN+LSTM打哈欠检测器\n   330\t        if PYTORCH_AVAILABLE:\n   331\t            try:\n   332\t                self.init_yawn_detector()\n   333\t            except Exception as e:\n   334\t                print(f\&quot;CNN+LSTM打哈欠检测器初始化失败: {e}\&quot;)\n   335\t\n   336\t        # 连接信号\n   337\t        # self.thread_signal.connect(self.thread_singnal_slot)\n   338\t\n   339\t        # 六个功能是否要用\n   340\t        self.fun = [True] * 6\n   341\t\n   342\t        # 兼容新旧界面的复选框名称\n   343\t        try:\n   344\t            # 尝试使用新界面的复选框\n   345\t            if hasattr(self, 'checkBox'):\n   346\t                self.checkBox_11 = self.checkBox\n   347\t                self.checkBox_12 = self.checkBox_2\n   348\t                self.checkBox_21 = self.checkBox_3\n   349\t                self.checkBox_22 = self.checkBox_4\n   350\t                # 为缺失的复选框创建占位符\n   351\t                if not hasattr(self, 'checkBox_31'):\n   352\t                    from PySide6.QtWidgets import QCheckBox\n   353\t                    self.checkBox_31 = QCheckBox()\n   354\t                    self.checkBox_32 = QCheckBox()\n   355\t\n   356\t            self.checkBox_11.setChecked(self.fun[0])\n   357\t            self.checkBox_12.setChecked(self.fun[1])\n   358\t            self.checkBox_21.setChecked(self.fun[2])\n   359\t            self.checkBox_22.setChecked(self.fun[3])\n   360\t            if hasattr(self, 'checkBox_31'):\n   361\t                self.checkBox_31.setChecked(self.fun[4])\n   362\t            if hasattr(self, 'checkBox_32'):\n   363\t                self.checkBox_32.setChecked(self.fun[5])\n   364\t\n   365\t            self.checkBox_11.stateChanged.connect(self.select_changed)\n   366\t            self.checkBox_12.stateChanged.connect(self.select_changed)\n   367\t            self.checkBox_21.stateChanged.connect(self.select_changed)\n   368\t            self.checkBox_22.stateChanged.connect(self.select_changed)\n   369\t            if hasattr(self, 'checkBox_31'):\n   370\t                self.checkBox_31.stateChanged.connect(self.select_changed)\n   371\t            if hasattr(self, 'checkBox_32'):\n   372\t                self.checkBox_32.stateChanged.connect(self.select_changed)\n   373\t        except AttributeError as e:\n   374\t            print(f\&quot;界面组件初始化警告: {e}\&quot;)\n   375\t            # 创建默认的功能状态\n   376\t            pass\n   377\t\n   378\t        # 阈值\n   379\t        self.values = [3,2,3,5,2]\n   380\t\n   381\t        # 兼容新旧界面的spinBox名称\n   382\t        try:\n   383\t            # 为缺失的spinBox创建占位符\n   384\t            if not hasattr(self, 'spinBox_2'):\n   385\t                from PySide6.QtWidgets import QSpinBox\n   386\t                self.spinBox_2 = QSpinBox()\n   387\t                self.spinBox_2.setValue(2)\n   388\t\n   389\t            if hasattr(self, 'spinBox_1'):\n   390\t                self.spinBox_1.setValue(self.values[0])\n   391\t                self.spinBox_1.valueChanged.connect(self.value_changed)\n   392\t            if hasattr(self, 'spinBox_2'):\n   393\t                self.spinBox_2.setValue(self.values[1])\n   394\t                self.spinBox_2.valueChanged.connect(self.value_changed)\n   395\t            if hasattr(self, 'spinBox_3'):\n   396\t                self.spinBox_3.setValue(self.values[2])\n   397\t                self.spinBox_3.valueChanged.connect(self.value_changed)\n   398\t            if hasattr(self, 'spinBox_4'):\n   399\t                self.spinBox_4.setValue(self.values[3])\n   400\t                self.spinBox_4.valueChanged.connect(self.value_changed)\n   401\t            if hasattr(self, 'spinBox_5'):\n   402\t                self.spinBox_5.setValue(self.values[4])\n   403\t                self.spinBox_5.valueChanged.connect(self.value_changed)\n   404\t        except AttributeError as e:\n   405\t            print(f\&quot;SpinBox初始化警告: {e}\&quot;)\n   406\t\n   407\t        self.thread_signal.connect(self.thread_singnal_slot)\n   408\t\n   409\t        # 兼容新旧界面的图片显示标签\n   410\t        if not hasattr(self, 'label_img'):\n   411\t            from PySide6.QtWidgets import QLabel\n   412\t            self.label_img = QLabel()\n   413\t            self.label_img.setScaledContents(True)\n   414\t            # 设置最小尺寸确保有足够空间显示人脸\n   415\t            self.label_img.setMinimumSize(640, 480)\n   416\t            # 如果有视频显示区域，可以将label_img添加到其中\n   417\t            print(\&quot;创建了默认的图片显示标签\&quot;)\n   418\t        else:\n   419\t            self.label_img.setScaledContents(True)\n   420\t            # 确保现有的label_img也有合适的最小尺寸\n   421\t            if self.label_img.minimumSize().width() &lt; 640:\n   422\t                self.label_img.setMinimumSize(640, 480)\n   423\t\n   424\t        if hasattr(self, 'plainTextEdit_tip'):\n   425\t            self.plainTextEdit_tip.appendPlainText('等待开始\\n')\n   426\t        else:\n   427\t            print(\&quot;等待开始\&quot;)\n   428\t\n   429\t\n   430\t        \&quot;\&quot;\&quot;参数\&quot;\&quot;\&quot;\n   431\t        # 默认为摄像头0\n   432\t        self.VIDEO_STREAM = 0\n   433\t        self.CAMERA_STYLE = False  # False未打开摄像头，True摄像头已打开\n   434\t\n   435\t        # 优化后的眨眼检测参数 - 进一步提高敏感度\n   436\t        self.EYE_AR_THRESH = 0.20  # 进一步降低阈值，提高敏感度（原0.22）\n   437\t        self.EYE_AR_CONSEC_FRAMES = 2  # 保持较低的连续帧要求\n   438\t        self.EYE_AR_UPPER_THRESH = 0.40  # 适当提高上限，避免过滤正常眨眼\n   439\t\n   440\t        # 优化后的打哈欠检测参数 - 进一步提高敏感度\n   441\t        self.MAR_THRESH = 0.40  # 进一步降低阈值，提高敏感度（原0.45）\n   442\t        self.MAR_DURATION_THRESH = 0.6  # 进一步降低哈欠持续时间阈值（秒）\n   443\t        self.MOUTH_AR_CONSEC_FRAMES = 2  # 减少连续帧要求\n   444\t\n   445\t        # 优化后的瞌睡点头检测参数\n   446\t        self.HAR_THRESH_LOW = 15.0  # 轻微点头角度阈值（度）\n   447\t        self.HAR_THRESH_HIGH = 25.0  # 明显点头角度阈值（度）\n   448\t        self.NOD_AR_CONSEC_FRAMES = 4  # 增加连续帧要求，减少误检\n   449\t\n   450\t        # 其他检测参数\n   451\t        self.AR_CONSEC_FRAMES_check = 3\n   452\t        self.OUT_AR_CONSEC_FRAMES_check = 5\n   453\t\n   454\t        \&quot;\&quot;\&quot;计数\&quot;\&quot;\&quot;\n   455\t        # 初始化帧计数器和眨眼总数\n   456\t        self.COUNTER = 0\n   457\t        self.TOTAL = 0\n   458\t        # 初始化帧计数器和打哈欠总数\n   459\t        self.mCOUNTER = 0\n   460\t        self.mTOTAL = 0\n   461\t        # 初始化帧计数器和点头总数\n   462\t        self.hCOUNTER = 0\n   463\t        self.hTOTAL = 0\n   464\t        # 离职时间长度\n   465\t        self.oCOUNTER = 0\n   466\t\n   467\t        # 新增：改进的检测状态跟踪\n   468\t        self.yawn_start_time = None  # 哈欠开始时间\n   469\t        self.last_ear_values = []  # 最近的EAR值历史\n   470\t        self.last_mar_values = []  # 最近的MAR值历史\n   471\t        self.fatigue_score = 0.0  # 疲劳评分\n   472\t        self.baseline_ear = 0.3  # 基线EAR值（将动态调整）\n   473\t        self.baseline_mar = 0.4  # 基线MAR值（将动态调整）\n   474\t\n   475\t        # 打哈欠冷却机制\n   476\t        self.last_yawn_time = None  # 上次检测到打哈欠的时间\n   477\t        self.yawn_cooldown_seconds = 3.0  # 打哈欠冷却时间（秒）\n   478\t        self.yawn_detection_enabled = True  # 打哈欠检测是否启用\n   479\t\n   480\t        # 自适应阈值调整\n   481\t        self.calibration_frames = 0  # 校准帧数\n   482\t        self.calibration_period = 300  # 校准周期（帧数）\n   483\t        self.adaptive_mode = True  # 是否启用自适应模式\n   484\t\n   485\t        \&quot;\&quot;\&quot;姿态\&quot;\&quot;\&quot;\n   486\t        # 世界坐标系(UVW)：填写3D参考点，该模型参考http://aifi.isr.uc.pt/Downloads/OpenGL/glAnthropometric3DModel.cpp\n   487\t        self.object_pts = np.float32([[6.825897, 6.760612, 4.402142],  #33左眉左上角\n   488\t                                 [1.330353, 7.122144, 6.903745],  #29左眉右角\n   489\t                                 [-1.330353, 7.122144, 6.903745], #34右眉左角\n   490\t                                 [-6.825897, 6.760612, 4.402142], #38右眉右上角\n   491\t                                 [5.311432, 5.485328, 3.987654],  #13左眼左上角\n   492\t                                 [1.789930, 5.393625, 4.413414],  #17左眼右上角\n   493\t                                 [-1.789930, 5.393625, 4.413414], #25右眼左上角\n   494\t                                 [-5.311432, 5.485328, 3.987654], #21右眼右上角\n   495\t                                 [2.005628, 1.409845, 6.165652],  #55鼻子左上角\n   496\t                                 [-2.005628, 1.409845, 6.165652], #49鼻子右上角\n   497\t                                 [2.774015, -2.080775, 5.048531], #43嘴左上角\n   498\t                                 [-2.774015, -2.080775, 5.048531],#39嘴右上角\n   499\t                                 [0.000000, -3.116408, 6.097667], #45嘴中央下角\n   500\t                                 [0.000000, -7.415691, 4.070434]])#6下巴角\n   501\t\n   502\t        # 相机坐标系(XYZ)：添加相机内参\n   503\t        self.K = [6.5308391993466671e+002, 0.0, 3.1950000000000000e+002,\n   504\t                 0.0, 6.5308391993466671e+002, 2.3950000000000000e+002,\n   505\t                 0.0, 0.0, 1.0]# 等价于矩阵[fx, 0, cx; 0, fy, cy; 0, 0, 1]\n   506\t        # 图像中心坐标系(uv)：相机畸变参数[k1, k2, p1, p2, k3]\n   507\t        self.D = [7.0834633684407095e-002, 6.9140193737175351e-002, 0.0, 0.0, -1.3073460323689292e+000]\n   508\t\n   509\t        # 像素坐标系(xy)：填写凸轮的本征和畸变系数\n   510\t        self.cam_matrix = np.array(self.K).reshape(3, 3).astype(np.float32)\n   511\t        self.dist_coeffs = np.array(self.D).reshape(5, 1).astype(np.float32)\n   512\t\n   513\t        # 重新投影3D点的世界坐标轴以验证结果姿势\n   514\t        self.reprojectsrc = np.float32([[10.0, 10.0, 10.0],\n   515\t                                       [10.0, 10.0, -10.0],\n   516\t                                       [10.0, -10.0, -10.0],\n   517\t                                       [10.0, -10.0, 10.0],\n   518\t                                       [-10.0, 10.0, 10.0],\n   519\t                                       [-10.0, 10.0, -10.0],\n   520\t                                       [-10.0, -10.0, -10.0],\n   521\t                                       [-10.0, -10.0, 10.0]])\n   522\t        # 绘制正方体12轴\n   523\t        self.line_pairs = [[0, 1], [1, 2], [2, 3], [3, 0],\n   524\t                          [4, 5], [5, 6], [6, 7], [7, 4],\n   525\t                          [0, 4], [1, 5], [2, 6], [3, 7]]\n   526\t\n   527\t\n   528\t        # 线程\n   529\t        self.thread = None\n   530\t        self.sound_thread = None\n   531\t        self.is_running = True  # 添加运行状态标志\n   532\t\n   533\t    def safe_emit_signal(self, data):\n   534\t        \&quot;\&quot;\&quot;安全地发送信号\&quot;\&quot;\&quot;\n   535\t        try:\n   536\t            if self.is_running and hasattr(self, 'thread_signal'):\n   537\t                # 进一步减少调试输出，只在重要事件时打印\n   538\t                if data['type'] == 'msg' and ('疲劳' in str(data.get('value', '')) or 'CNN检测' in str(data.get('value', ''))):\n   539\t                    print(f\&quot;发送重要信号: {data['type']} - {data.get('value', '')}\&quot;)\n   540\t                elif data['type'] == 'res' and hasattr(self, 'frame_count') and self.frame_count % 300 == 0:  # 每10秒打印一次状态\n   541\t                    print(f\&quot;系统状态正常: FPS={data.get('value', ['', '', '0'])[2]}\&quot;)\n   542\t\n   543\t                self.thread_signal.emit(data)\n   544\t                return True\n   545\t            else:\n   546\t                # 只在第一次出现错误时记录，避免重复日志\n   547\t                if not hasattr(self, '_signal_error_logged'):\n   548\t                    print(f\&quot;检测已停止，停止信号发送\&quot;)\n   549\t                    self._signal_error_logged = True\n   550\t                return False\n   551\t        except RuntimeError as e:\n   552\t            # Qt对象已被销毁的情况\n   553\t            if not hasattr(self, '_runtime_error_logged'):\n   554\t                print(f\&quot;UI已关闭，停止信号发送\&quot;)\n   555\t                self._runtime_error_logged = True\n   556\t            return False\n   557\t        except Exception as e:\n   558\t            if not hasattr(self, '_unknown_error_logged'):\n   559\t                print(f\&quot;信号发送异常: {e}\&quot;)\n   560\t                self._unknown_error_logged = True\n   561\t            return False\n   562\t\n   563\t    def _optimize_camera_brightness(self):\n   564\t        \&quot;\&quot;\&quot;优化摄像头设置 - 使用配置文件参数\&quot;\&quot;\&quot;\n   565\t        if self.cap is None or not self.cap.isOpened():\n   566\t            return\n   567\t\n   568\t        print(\&quot;正在优化摄像头设置...\&quot;)\n   569\t\n   570\t        try:\n   571\t            # 使用配置文件中的摄像头属性\n   572\t            camera_props = camera_config.get_camera_properties()\n   573\t\n   574\t            for prop, value in camera_props.items():\n   575\t                try:\n   576\t                    self.cap.set(prop, value)\n   577\t                except Exception as e:\n   578\t                    # 某些属性可能不被支持，继续设置其他属性\n   579\t                    pass\n   580\t\n   581\t            print(f\&quot;摄像头优化设置完成:\&quot;)\n   582\t            print(f\&quot;  分辨率: {camera_config.CAMERA_WIDTH}x{camera_config.CAMERA_HEIGHT}\&quot;)\n   583\t            print(f\&quot;  FPS: {camera_config.CAMERA_FPS}\&quot;)\n   584\t            print(f\&quot;  亮度: {camera_config.BRIGHTNESS}\&quot;)\n   585\t            print(f\&quot;  对比度: {camera_config.CONTRAST}\&quot;)\n   586\t            print(f\&quot;  增益: {camera_config.GAIN}\&quot;)\n   587\t            print(\&quot;  已针对暗环境和帧率进行优化\&quot;)\n   588\t\n   589\t        except Exception as e:\n   590\t            print(f\&quot;摄像头参数设置失败: {e}\&quot;)\n   591\t            # 即使设置失败也继续，使用默认参数\n   592\t\n   593\t    def _enhance_dark_frame(self, frame):\n   594\t        \&quot;\&quot;\&quot;增强暗图像 - 使用配置文件参数\&quot;\&quot;\&quot;\n   595\t        if frame is None:\n   596\t            return frame\n   597\t\n   598\t        # 检查图像亮度\n   599\t        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n   600\t        mean_brightness = np.mean(gray)\n   601\t\n   602\t        # 如果图像太暗，应用多步骤增强\n   603\t        if mean_brightness &lt; camera_config.DARK_THRESHOLD:\n   604\t            enhanced_frame = frame.copy()\n   605\t\n   606\t            # 步骤1: 基础亮度和对比度调整\n   607\t            enhanced_frame = cv2.convertScaleAbs(\n   608\t                enhanced_frame,\n   609\t                alpha=camera_config.BRIGHTNESS_ALPHA,\n   610\t                beta=camera_config.BRIGHTNESS_BETA\n   611\t            )\n   612\t\n   613\t            # 步骤2: 应用CLAHE（对比度限制自适应直方图均衡化）\n   614\t            clahe = cv2.createCLAHE(\n   615\t                clipLimit=camera_config.CLAHE_CLIP_LIMIT,\n   616\t                tileGridSize=camera_config.CLAHE_TILE_SIZE\n   617\t            )\n   618\t\n   619\t            # 在LAB色彩空间中处理亮度通道\n   620\t            lab = cv2.cvtColor(enhanced_frame, cv2.COLOR_BGR2LAB)\n   621\t            lab[:,:,0] = clahe.apply(lab[:,:,0])\n   622\t            enhanced_frame = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n   623\t\n   624\t            # 步骤3: Gamma校正进一步提亮暗部\n   625\t            gamma = camera_config.GAMMA_CORRECTION\n   626\t            inv_gamma = 1.0 / gamma\n   627\t            table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\&quot;uint8\&quot;)\n   628\t            enhanced_frame = cv2.LUT(enhanced_frame, table)\n   629\t\n   630\t            # 步骤4: 与原图混合，保持自然效果\n   631\t            enhanced_frame = cv2.addWeighted(\n   632\t                frame,\n   633\t                1.0 - camera_config.ENHANCEMENT_WEIGHT,\n   634\t                enhanced_frame,\n   635\t                camera_config.ENHANCEMENT_WEIGHT,\n   636\t                0\n   637\t            )\n   638\t\n   639\t            return enhanced_frame\n   640\t\n   641\t        return frame\n   642\t\n   643\t    def init_camera_list(self):\n   644\t        \&quot;\&quot;\&quot;初始化摄像头列表\&quot;\&quot;\&quot;\n   645\t        self.cameras = []\n   646\t        print(\&quot;正在扫描可用摄像头...\&quot;)\n   647\t\n   648\t        for i in range(5):\n   649\t            print(f\&quot;  测试摄像头索引 {i}...\&quot;)\n   650\t            cap = None\n   651\t            try:\n   652\t                cap = cv2.VideoCapture(i, cv2.CAP_DSHOW)\n   653\t                if cap.isOpened():\n   654\t                    # 测试是否能读取帧\n   655\t                    ret, frame = cap.read()\n   656\t                    if ret and frame is not None and frame.size &gt; 0:\n   657\t                        self.cameras.append(i)\n   658\t                        print(f\&quot;    ✓ 摄像头 {i} 可用\&quot;)\n   659\t                    else:\n   660\t                        print(f\&quot;    ✗ 摄像头 {i} 无法读取帧\&quot;)\n   661\t                else:\n   662\t                    print(f\&quot;    ✗ 摄像头 {i} 无法打开\&quot;)\n   663\t\n   664\t                if cap is not None:\n   665\t                    cap.release()\n   666\t\n   667\t            except Exception as e:\n   668\t                print(f\&quot;    ✗ 摄像头 {i} 测试异常: {e}\&quot;)\n   669\t                if cap is not None:\n   670\t                    try:\n   671\t                        cap.release()\n   672\t                    except:\n   673\t                        pass\n   674\t\n   675\t        print(f\&quot;找到 {len(self.cameras)} 个摄像头设备\&quot;)\n   676\t        if self.cameras:\n   677\t            print(\&quot;可用摄像头:\&quot;, self.cameras)\n   678\t        else:\n   679\t            print(\&quot;未找到摄像头设备\&quot;)\n   680\t\n   681\t    def _init_camera_robust(self, camera_index):\n   682\t        \&quot;\&quot;\&quot;简化的摄像头初始化方法 - 避免闪烁\&quot;\&quot;\&quot;\n   683\t        print(f\&quot;正在初始化摄像头 {camera_index}...\&quot;)\n   684\t\n   685\t        # 优先使用DirectShow后端（Windows最稳定）\n   686\t        try:\n   687\t            print(f\&quot;  使用DirectShow后端...\&quot;)\n   688\t            cap = cv2.VideoCapture(camera_index, cv2.CAP_DSHOW)\n   689\t\n   690\t            if cap.isOpened():\n   691\t                # 立即测试读取\n   692\t                ret, frame = cap.read()\n   693\t                if ret and frame is not None and frame.size &gt; 0:\n   694\t                    print(f\&quot;摄像头 {camera_index} 初始化成功\&quot;)\n   695\t                    return cap\n   696\t                else:\n   697\t                    print(f\&quot;  无法读取帧\&quot;)\n   698\t                    cap.release()\n   699\t            else:\n   700\t                print(f\&quot;  无法打开摄像头\&quot;)\n   701\t                if cap is not None:\n   702\t                    cap.release()\n   703\t\n   704\t        except Exception as e:\n   705\t            print(f\&quot;  DirectShow初始化失败: {e}\&quot;)\n   706\t            if 'cap' in locals() and cap is not None:\n   707\t                try:\n   708\t                    cap.release()\n   709\t                except:\n   710\t                    pass\n   711\t\n   712\t        # 如果DirectShow失败，尝试默认后端\n   713\t        try:\n   714\t            print(f\&quot;  尝试默认后端...\&quot;)\n   715\t            cap = cv2.VideoCapture(camera_index)\n   716\t\n   717\t            if cap.isOpened():\n   718\t                ret, frame = cap.read()\n   719\t                if ret and frame is not None and frame.size &gt; 0:\n   720\t                    print(f\&quot;摄像头 {camera_index} 初始化成功（默认后端）\&quot;)\n   721\t                    return cap\n   722\t                else:\n   723\t                    cap.release()\n   724\t            else:\n   725\t                if cap is not None:\n   726\t                    cap.release()\n   727\t\n   728\t        except Exception as e:\n   729\t            print(f\&quot;  默认后端初始化失败: {e}\&quot;)\n   730\t            if 'cap' in locals() and cap is not None:\n   731\t                try:\n   732\t                    cap.release()\n   733\t                except:\n   734\t                    pass\n   735\t\n   736\t        print(f\&quot;摄像头 {camera_index} 初始化失败\&quot;)\n   737\t        return None\n   738\t\n   739\t    def _check_camera_health(self):\n   740\t        \&quot;\&quot;\&quot;检查摄像头健康状态\&quot;\&quot;\&quot;\n   741\t        if not hasattr(self, 'cap') or self.cap is None:\n   742\t            return False\n   743\t\n   744\t        try:\n   745\t            # 检查摄像头是否仍然打开\n   746\t            if not self.cap.isOpened():\n   747\t                return False\n   748\t\n   749\t            # 尝试获取一些基本属性\n   750\t            width = self.cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n   751\t            height = self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n   752\t\n   753\t            if width &lt;= 0 or height &lt;= 0:\n   754\t                return False\n   755\t\n   756\t            return True\n   757\t\n   758\t        except Exception as e:\n   759\t            print(f\&quot;摄像头健康检查失败: {e}\&quot;)\n   760\t            return False\n   761\t\n   762\t    def _reconnect_camera(self):\n   763\t        \&quot;\&quot;\&quot;重新连接摄像头\&quot;\&quot;\&quot;\n   764\t        print(\&quot;尝试重新连接摄像头...\&quot;)\n   765\t\n   766\t        # 释放当前摄像头\n   767\t        if hasattr(self, 'cap') and self.cap is not None:\n   768\t            try:\n   769\t                self.cap.release()\n   770\t            except:\n   771\t                pass\n   772\t            self.cap = None\n   773\t\n   774\t        # 短暂等待\n   775\t        time.sleep(0.2)\n   776\t\n   777\t        # 尝试重新打开\n   778\t        try:\n   779\t            self.cap = cv2.VideoCapture(self.VIDEO_STREAM, cv2.CAP_DSHOW)\n   780\t            if self.cap.isOpened():\n   781\t                # 重新设置基本参数\n   782\t                self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n   783\t                self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n   784\t                self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n   785\t\n   786\t                # 测试读取\n   787\t                ret, test_frame = self.cap.read()\n   788\t                if ret and test_frame is not None:\n   789\t                    print(\&quot;摄像头重连成功\&quot;)\n   790\t                    return True\n   791\t                else:\n   792\t                    print(\&quot;摄像头重连后无法读取帧\&quot;)\n   793\t                    self.cap.release()\n   794\t                    self.cap = None\n   795\t                    return False\n   796\t            else:\n   797\t                print(\&quot;摄像头重连失败\&quot;)\n   798\t                return False\n   799\t\n   800\t        except Exception as e:\n   801\t            print(f\&quot;摄像头重连异常: {e}\&quot;)\n   802\t            return False\n   803\t\n   804\t\n   805\t\n   806\t    def init_models(self):\n   807\t        \&quot;\&quot;\&quot;初始化深度学习模型\&quot;\&quot;\&quot;\n   808\t        self.resnet = self._init_resnet()\n   809\t        self.cnn_model = self._init_cnn_model()\n   810\t        self.lstm_model = self._init_lstm_model()\n   811\t\n   812\t        if all([self.resnet, self.cnn_model, self.lstm_model]):\n   813\t            print(\&quot;所有模型初始化完成\&quot;)\n   814\t        else:\n   815\t            print(\&quot;部分模型初始化失败\&quot;)\n   816\t\n   817\t    def init_cnn_detector(self):\n   818\t        \&quot;\&quot;\&quot;初始化CNN疲劳检测器\&quot;\&quot;\&quot;\n   819\t        try:\n   820\t            # 尝试导入CNN检测器\n   821\t            from simple_cnn_detector import CNNFatigueDetector\n   822\t\n   823\t            # 检查是否有训练好的模型\n   824\t            model_path = './model/fatigue_model_mobilenet.h5'\n   825\t            if os.path.exists(model_path):\n   826\t                self.cnn_detector = CNNFatigueDetector(model_path)\n   827\t                if self.cnn_detector.is_available():\n   828\t                    print(\&quot;✅ CNN疲劳检测器已加载\&quot;)\n   829\t                else:\n   830\t                    print(\&quot;⚠️ CNN检测器加载失败\&quot;)\n   831\t                    self.cnn_detector = None\n   832\t            else:\n   833\t                print(\&quot;⚠️ 未找到训练好的CNN模型，将创建简化版本\&quot;)\n   834\t                # 创建一个使用预训练ResNet的简化检测器\n   835\t                self.cnn_detector = self._create_simple_detector()\n   836\t\n   837\t        except ImportError as e:\n   838\t            print(f\&quot;CNN检测器导入失败: {e}\&quot;)\n   839\t            self.cnn_detector = None\n   840\t        except Exception as e:\n   841\t            print(f\&quot;CNN检测器初始化失败: {e}\&quot;)\n   842\t            self.cnn_detector = None\n   843\t\n   844\t    def _create_simple_detector(self):\n   845\t        \&quot;\&quot;\&quot;创建简化的疲劳检测器\&quot;\&quot;\&quot;\n   846\t        class SimpleFatigueDetector:\n   847\t            def __init__(self, resnet_model):\n   848\t                self.resnet = resnet_model\n   849\t                self.available = resnet_model is not None\n   850\t\n   851\t            def is_available(self):\n   852\t                return self.available\n   853\t\n   854\t            def predict_fatigue(self, face_image):\n   855\t                \&quot;\&quot;\&quot;使用ResNet特征进行简单的疲劳检测\&quot;\&quot;\&quot;\n   856\t                if not self.available or face_image is None or face_image.size == 0:\n   857\t                    return None\n   858\t\n   859\t                try:\n   860\t                    # 预处理图像\n   861\t                    img = cv2.resize(face_image, (224, 224))\n   862\t                    img = tf.keras.applications.resnet50.preprocess_input(img)\n   863\t\n   864\t                    # 提取特征\n   865\t                    features = self.resnet.predict(np.expand_dims(img, axis=0), verbose=0)\n   866\t\n   867\t                    # 简单的疲劳判断（基于特征的统计特性）\n   868\t                    feature_mean = np.mean(features)\n   869\t                    feature_std = np.std(features)\n   870\t\n   871\t                    # 简化的疲劳判断逻辑\n   872\t                    fatigue_score = feature_mean * feature_std\n   873\t                    confidence = min(abs(fatigue_score) * 100, 1.0)\n   874\t\n   875\t                    is_fatigue = fatigue_score &lt; -0.1  # 阈值可调整\n   876\t\n   877\t                    return {\n   878\t                        'predicted_class': 'drowsy' if is_fatigue else 'alert',\n   879\t                        'confidence': confidence,\n   880\t                        'fatigue_level': '疲劳' if is_fatigue else '正常',\n   881\t                        'fatigue_detected': is_fatigue,\n   882\t                        'feature_mean': feature_mean,\n   883\t                        'feature_std': feature_std\n   884\t                    }\n   885\t\n   886\t                except Exception as e:\n   887\t                    print(f\&quot;简化检测器预测失败: {e}\&quot;)\n   888\t                    return None\n   889\t\n   890\t        return SimpleFatigueDetector(self.resnet)\n   891\t\n   892\t    def init_yawn_detector(self):\n   893\t        \&quot;\&quot;\&quot;初始化CNN+LSTM打哈欠检测器\&quot;\&quot;\&quot;\n   894\t        try:\n   895\t            # 检查是否有训练好的模型 - 现在模型文件在model文件夹中\n   896\t            model_path = './model/best_fatigue_model.pth'\n   897\t            if not os.path.exists(model_path):\n   898\t                # 尝试其他可能的路径\n   899\t                model_path = '../real_pljc/models/best_fatigue_model.pth'\n   900\t            if not os.path.exists(model_path):\n   901\t                # 尝试相对路径\n   902\t                model_path = './real_pljc/models/best_fatigue_model.pth'\n   903\t            if not os.path.exists(model_path):\n   904\t                # 尝试绝对路径\n   905\t                model_path = 'D:/code/PythonProject2/real_pljc/models/best_fatigue_model.pth'\n   906\t\n   907\t            if os.path.exists(model_path):\n   908\t                self.yawn_detector = YawnDetector(model_path)\n   909\t                if self.yawn_detector.is_available:\n   910\t                    print(\&quot;✅ CNN+LSTM打哈欠检测器已加载\&quot;)\n   911\t                else:\n   912\t                    print(\&quot;⚠️ CNN+LSTM打哈欠检测器加载失败\&quot;)\n   913\t                    self.yawn_detector = None\n   914\t            else:\n   915\t                print(\&quot;⚠️ 未找到CNN+LSTM打哈欠检测模型\&quot;)\n   916\t                self.yawn_detector = None\n   917\t\n   918\t        except Exception as e:\n   919\t            print(f\&quot;CNN+LSTM打哈欠检测器初始化失败: {e}\&quot;)\n   920\t            self.yawn_detector = None\n   921\t\n   922\t    def _init_resnet(self):\n   923\t        \&quot;\&quot;\&quot;初始化ResNet50模型\&quot;\&quot;\&quot;\n   924\t        try:\n   925\t            print(\&quot;正在加载ResNet50模型...\&quot;)\n   926\t            resnet = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n   927\t            print(\&quot;ResNet50模型加载成功\&quot;)\n   928\t            return resnet\n   929\t        except Exception as e:\n   930\t            print(f\&quot;ResNet50模型加载失败: {e}\&quot;)\n   931\t            return None\n   932\t\n   933\t    def _init_cnn_model(self):\n   934\t        \&quot;\&quot;\&quot;初始化CNN模型\&quot;\&quot;\&quot;\n   935\t        try:\n   936\t            print(\&quot;正在创建CNN模型...\&quot;)\n   937\t            cnn_model = Sequential()\n   938\t\n   939\t            if TENSORFLOW_AVAILABLE:\n   940\t                cnn_model.add(Input(shape=(64, 64, 3)))\n   941\t\n   942\t            cnn_model.add(Conv2D(32, (3, 3), activation='relu'))\n   943\t            cnn_model.add(MaxPooling2D((2, 2)))\n   944\t            cnn_model.add(Conv2D(64, (3, 3), activation='relu'))\n   945\t            cnn_model.add(MaxPooling2D((2, 2)))\n   946\t            cnn_model.add(Flatten())\n   947\t            cnn_model.add(Dense(128, activation='relu'))\n   948\t            cnn_model.add(Dense(1, activation='sigmoid'))\n   949\t            print(\&quot;CNN模型创建成功\&quot;)\n   950\t            return cnn_model\n   951\t        except Exception as e:\n   952\t            print(f\&quot;CNN模型创建失败: {e}\&quot;)\n   953\t            return None\n   954\t\n   955\t    def _init_lstm_model(self):\n   956\t        \&quot;\&quot;\&quot;初始化LSTM模型\&quot;\&quot;\&quot;\n   957\t        try:\n   958\t            print(\&quot;正在创建LSTM模型...\&quot;)\n   959\t            lstm_model = Sequential()\n   960\t\n   961\t            if TENSORFLOW_AVAILABLE:\n   962\t                lstm_model.add(Input(shape=(10, 2048)))\n   963\t\n   964\t            lstm_model.add(LSTM(64, return_sequences=True))\n   965\t            lstm_model.add(LSTM(32))\n   966\t            lstm_model.add(Dense(1, activation='sigmoid'))\n   967\t            print(\&quot;LSTM模型创建成功\&quot;)\n   968\t            return lstm_model\n   969\t        except Exception as e:\n   970\t            print(f\&quot;LSTM模型创建失败: {e}\&quot;)\n   971\t            return None\n   972\t\n   973\t    def extract_resnet_features(self, face_img):\n   974\t        \&quot;\&quot;\&quot;使用ResNet提取人脸特征\&quot;\&quot;\&quot;\n   975\t        if self.resnet is None or face_img.size == 0:\n   976\t            return None\n   977\t\n   978\t        # 预处理图像\n   979\t        img = cv2.resize(face_img, (224, 224))\n   980\t        img = tf.keras.applications.resnet50.preprocess_input(img)\n   981\t        features = self.resnet.predict(np.expand_dims(img, axis=0))\n   982\t        return features\n   983\t\n   984\t    def detect_cnn_fatigue(self, face_img):\n   985\t        \&quot;\&quot;\&quot;使用CNN进行疲劳检测\&quot;\&quot;\&quot;\n   986\t        if self.cnn_model is None or face_img.size == 0:\n   987\t            return None\n   988\t\n   989\t        # 预处理图像\n   990\t        img = cv2.resize(face_img, (64, 64))\n   991\t        img = img / 255.0  # 归一化\n   992\t        prediction = self.cnn_model.predict(np.expand_dims(img, axis=0))\n   993\t        return '疲劳' if prediction[0][0] &gt; 0.5 else '正常'\n   994\t\n   995\t    def analyze_lstm_fatigue(self, features_sequence):\n   996\t        \&quot;\&quot;\&quot;使用LSTM分析时序特征\&quot;\&quot;\&quot;\n   997\t        if self.lstm_model is None or not features_sequence:\n   998\t            return None\n   999\t\n  1000\t        # 确保序列长度一致\n  1001\t        if len(features_sequence) != 10:\n  1002\t            return None\n  1003\t\n  1004\t        prediction = self.lstm_model.predict(np.expand_dims(features_sequence, axis=0))\n  1005\t        return '深度疲劳' if prediction[0][0] &gt; 0.7 else '轻度疲劳'\n  1006\t\n  1007\t    def calculate_fatigue_score(self, blink_count, yawn_count, nod_count, time_window=60):\n  1008\t        \&quot;\&quot;\&quot;计算综合疲劳评分\&quot;\&quot;\&quot;\n  1009\t        # 权重设置（基于疲劳检测研究）\n  1010\t        blink_weight = 0.3  # 眨眼权重\n  1011\t        yawn_weight = 0.5   # 哈欠权重（更重要）\n  1012\t        nod_weight = 0.4    # 点头权重\n  1013\t\n  1014\t        # 标准化到每分钟的频率\n  1015\t        blink_rate = (blink_count / time_window) * 60\n  1016\t        yawn_rate = (yawn_count / time_window) * 60\n  1017\t        nod_rate = (nod_count / time_window) * 60\n  1018\t\n  1019\t        # 正常基线值（每分钟）\n  1020\t        normal_blink_rate = 15  # 正常眨眼频率\n  1021\t        normal_yawn_rate = 0.5  # 正常哈欠频率\n  1022\t        normal_nod_rate = 1     # 正常点头频率\n  1023\t\n  1024\t        # 计算偏离度\n  1025\t        blink_deviation = max(0, blink_rate - normal_blink_rate) / normal_blink_rate\n  1026\t        yawn_deviation = max(0, yawn_rate - normal_yawn_rate) / normal_yawn_rate\n  1027\t        nod_deviation = max(0, nod_rate - normal_nod_rate) / normal_nod_rate\n  1028\t\n  1029\t        # 综合评分\n  1030\t        fatigue_score = (blink_deviation * blink_weight +\n  1031\t                        yawn_deviation * yawn_weight +\n  1032\t                        nod_deviation * nod_weight)\n  1033\t\n  1034\t        return min(fatigue_score, 1.0)  # 限制在0-1之间\n  1035\t\n  1036\t    def get_fatigue_level(self, score):\n  1037\t        \&quot;\&quot;\&quot;根据评分获取疲劳等级\&quot;\&quot;\&quot;\n  1038\t        if score &lt; 0.2:\n  1039\t            return '正常'\n  1040\t        elif score &lt; 0.4:\n  1041\t            return '轻微疲劳'\n  1042\t        elif score &lt; 0.7:\n  1043\t            return '中度疲劳'\n  1044\t        else:\n  1045\t            return '重度疲劳'\n  1046\t\n  1047\t    def adaptive_threshold_adjustment(self, ear, mar):\n  1048\t        \&quot;\&quot;\&quot;自适应阈值调整\&quot;\&quot;\&quot;\n  1049\t        if not self.adaptive_mode:\n  1050\t            return\n  1051\t\n  1052\t        self.calibration_frames += 1\n  1053\t\n  1054\t        # 在校准期间收集数据\n  1055\t        if self.calibration_frames &lt;= self.calibration_period:\n  1056\t            # 更新基线值\n  1057\t            self.update_baseline_values(ear, mar)\n  1058\t\n  1059\t            # 校准完成后调整阈值\n  1060\t            if self.calibration_frames == self.calibration_period:\n  1061\t                self._adjust_thresholds()\n  1062\t                print(f\&quot;自适应校准完成 - EAR基线: {self.baseline_ear:.3f}, MAR基线: {self.baseline_mar:.3f}\&quot;)\n  1063\t\n  1064\t        # 定期重新校准（每1000帧）\n  1065\t        elif self.calibration_frames % 1000 == 0:\n  1066\t            self._adjust_thresholds()\n  1067\t\n  1068\t    def _adjust_thresholds(self):\n  1069\t        \&quot;\&quot;\&quot;根据基线值调整检测阈值\&quot;\&quot;\&quot;\n  1070\t        if len(self.last_ear_values) &gt;= 10:\n  1071\t            # 动态调整眨眼阈值\n  1072\t            ear_std = np.std(self.last_ear_values)\n  1073\t            self.EYE_AR_THRESH = max(0.2, self.baseline_ear - 2 * ear_std)\n  1074\t            self.EYE_AR_UPPER_THRESH = self.baseline_ear + 2 * ear_std\n  1075\t\n  1076\t        if len(self.last_mar_values) &gt;= 10:\n  1077\t            # 动态调整哈欠阈值\n  1078\t            mar_std = np.std(self.last_mar_values)\n  1079\t            self.MAR_THRESH = max(0.5, self.baseline_mar + 1.5 * mar_std)\n  1080\t\n  1081\t    def get_detection_confidence(self, ear, mar):\n  1082\t        \&quot;\&quot;\&quot;计算检测置信度\&quot;\&quot;\&quot;\n  1083\t        ear_confidence = 1.0\n  1084\t        mar_confidence = 1.0\n  1085\t\n  1086\t        if len(self.last_ear_values) &gt;= 10:\n  1087\t            ear_std = np.std(self.last_ear_values)\n  1088\t            ear_z_score = abs(ear - self.baseline_ear) / (ear_std + 1e-6)\n  1089\t            ear_confidence = min(1.0, ear_z_score / 3.0)  # 3-sigma规则\n  1090\t\n  1091\t        if len(self.last_mar_values) &gt;= 10:\n  1092\t            mar_std = np.std(self.last_mar_values)\n  1093\t            mar_z_score = abs(mar - self.baseline_mar) / (mar_std + 1e-6)\n  1094\t            mar_confidence = min(1.0, mar_z_score / 3.0)\n  1095\t\n  1096\t        return (ear_confidence + mar_confidence) / 2.0\n  1097\t\n  1098\t    def get_head_pose(self,shape):# 头部姿态估计\n  1099\t        # （像素坐标集合）填写2D参考点，注释遵循https://ibug.doc.ic.ac.uk/resources/300-W/\n  1100\t        # 17左眉左上角/21左眉右角/22右眉左上角/26右眉右上角/36左眼左上角/39左眼右上角/42右眼左上角/\n  1101\t        # 45右眼右上角/31鼻子左上角/35鼻子右上角/48左上角/54嘴右上角/57嘴中央下角/8下巴角\n  1102\t        image_pts = np.float32([shape[17], shape[21], shape[22], shape[26], shape[36],\n  1103\t                                shape[39], shape[42], shape[45], shape[31], shape[35],\n  1104\t                                shape[48], shape[54], shape[57], shape[8]])\n  1105\t        # solvePnP计算姿势——求解旋转和平移矩阵：\n  1106\t        # rotation_vec表示旋转矩阵，translation_vec表示平移矩阵，cam_matrix与K矩阵对应，dist_coeffs与D矩阵对应。\n  1107\t        _, rotation_vec, translation_vec = cv2.solvePnP(self.object_pts, image_pts, self.cam_matrix, self.dist_coeffs)\n  1108\t        # projectPoints重新投影误差：原2d点和重投影2d点的距离（输入3d点、相机内参、相机畸变、r、t，输出重投影2d点）\n  1109\t        reprojectdst, _ = cv2.projectPoints(self.reprojectsrc, rotation_vec, translation_vec, self.cam_matrix,self.dist_coeffs)\n  1110\t        reprojectdst = tuple(map(tuple, reprojectdst.reshape(8, 2)))# 以8行2列显示\n  1111\t\n  1112\t        # 计算欧拉角calc euler angle\n  1113\t        # 参考https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#decomposeprojectionmatrix\n  1114\t        rotation_mat, _ = cv2.Rodrigues(rotation_vec) #罗德里格斯公式（将旋转矩阵转换为旋转向量）\n  1115\t        pose_mat = cv2.hconcat((rotation_mat, translation_vec)) # 水平拼接，vconcat垂直拼接\n  1116\t        # decomposeProjectionMatrix将投影矩阵分解为旋转矩阵和相机矩阵\n  1117\t        _, _, _, _, _, _, euler_angle = cv2.decomposeProjectionMatrix(pose_mat)\n  1118\t\n  1119\t        # 确保从数组中提取标量值\n  1120\t        pitch = math.radians(euler_angle[0].item())\n  1121\t        yaw = math.radians(euler_angle[1].item())\n  1122\t        roll = math.radians(euler_angle[2].item())\n  1123\t\n  1124\t        pitch = math.degrees(math.asin(math.sin(pitch)))\n  1125\t        roll = -math.degrees(math.asin(math.sin(roll)))\n  1126\t        yaw = math.degrees(math.asin(math.sin(yaw)))\n  1127\t        #print('pitch:{}, yaw:{}, roll:{}'.format(pitch, yaw, roll))\n  1128\t\n  1129\t        return reprojectdst, euler_angle# 投影误差，欧拉角\n  1130\t    def eye_aspect_ratio(self,eye):\n  1131\t        # 垂直眼标志（X，Y）坐标\n  1132\t        A = dist.euclidean(eye[1], eye[5])# 计算两个集合之间的欧式距离\n  1133\t        B = dist.euclidean(eye[2], eye[4])\n  1134\t        # 计算水平之间的欧几里得距离\n  1135\t        # 水平眼标志（X，Y）坐标\n  1136\t        C = dist.euclidean(eye[0], eye[3])\n  1137\t        # 眼睛长宽比的计算\n  1138\t        ear = (A + B) / (2.0 * C)\n  1139\t        # 返回眼睛的长宽比\n  1140\t        return ear\n  1141\t\n  1142\t    def mouth_aspect_ratio(self,mouth):# 嘴部\n  1143\t        A = np.linalg.norm(mouth[2] - mouth[9])  # 51, 59\n  1144\t        B = np.linalg.norm(mouth[4] - mouth[7])  # 53, 57\n  1145\t        C = np.linalg.norm(mouth[0] - mouth[6])  # 49, 55\n  1146\t        mar = (A + B) / (2.0 * C)\n  1147\t        return mar\n  1148\t\n  1149\t    def update_baseline_values(self, ear, mar):\n  1150\t        \&quot;\&quot;\&quot;动态更新基线值\&quot;\&quot;\&quot;\n  1151\t        # 保持最近50个值的历史\n  1152\t        self.last_ear_values.append(ear)\n  1153\t        self.last_mar_values.append(mar)\n  1154\t\n  1155\t        if len(self.last_ear_values) &gt; 50:\n  1156\t            self.last_ear_values.pop(0)\n  1157\t        if len(self.last_mar_values) &gt; 50:\n  1158\t            self.last_mar_values.pop(0)\n  1159\t\n  1160\t        # 更新基线值（使用中位数，更稳定）\n  1161\t        if len(self.last_ear_values) &gt;= 10:\n  1162\t            self.baseline_ear = np.median(self.last_ear_values)\n  1163\t        if len(self.last_mar_values) &gt;= 10:\n  1164\t            self.baseline_mar = np.median(self.last_mar_values)\n  1165\t\n  1166\t    def is_valid_blink(self, ear):\n  1167\t        \&quot;\&quot;\&quot;改进的眨眼检测 - 简化版本\&quot;\&quot;\&quot;\n  1168\t        # 使用固定阈值，更容易触发\n  1169\t        thresh = self.EYE_AR_THRESH\n  1170\t\n  1171\t        # 检查是否在合理范围内（过滤异常值）\n  1172\t        if ear &gt; self.EYE_AR_UPPER_THRESH or ear &lt; 0.1:  # 添加下限检查\n  1173\t            return False\n  1174\t\n  1175\t        is_blink = ear &lt; thresh\n  1176\t\n  1177\t        # 添加调试信息\n  1178\t        if not hasattr(self, '_blink_detail_counter'):\n  1179\t            self._blink_detail_counter = 0\n  1180\t        self._blink_detail_counter += 1\n  1181\t\n  1182\t        if self._blink_detail_counter % 60 == 0:  # 每60帧打印一次详细信息\n  1183\t            print(f\&quot; 眨眼检测详情 - EAR: {ear:.3f}, 阈值: {thresh:.3f}, 是否眨眼: {is_blink}\&quot;)\n  1184\t\n  1185\t        return is_blink\n  1186\t\n  1187\t    def is_valid_yawn(self, mar, current_time):\n  1188\t        \&quot;\&quot;\&quot;改进的打哈欠检测 - 更宽松的条件\&quot;\&quot;\&quot;\n  1189\t        # 使用简化的阈值检测\n  1190\t        thresh = self.MAR_THRESH\n  1191\t\n  1192\t        is_mouth_open = mar &gt; thresh\n  1193\t\n  1194\t        if is_mouth_open:\n  1195\t            if self.yawn_start_time is None:\n  1196\t                self.yawn_start_time = current_time\n  1197\t                print(f\&quot; 开始检测哈欠，MAR: {mar:.3f}, 阈值: {thresh:.3f}\&quot;)\n  1198\t            return False  # 还在张嘴过程中\n  1199\t        else:\n  1200\t            if self.yawn_start_time is not None:\n  1201\t                # 检查持续时间\n  1202\t                duration = (current_time - self.yawn_start_time).total_seconds()\n  1203\t                self.yawn_start_time = None\n  1204\t\n  1205\t                print(f\&quot; 哈欠持续时间: {duration:.2f}秒\&quot;)\n  1206\t                # 进一步放宽哈欠持续时间要求：0.3-3.0秒\n  1207\t                is_valid = 0.3 &lt;= duration &lt;= 3.0\n  1208\t                if is_valid:\n  1209\t                    print(f\&quot;✅ 有效哈欠，持续时间: {duration:.2f}秒\&quot;)\n  1210\t                else:\n  1211\t                    print(f\&quot;❌ 无效哈欠，持续时间: {duration:.2f}秒（要求0.3-3.0秒）\&quot;)\n  1212\t                return is_valid\n  1213\t            return False\n  1214\t\n  1215\t\n  1216\t    def _learning_face(self):\n  1217\t        \&quot;\&quot;\&quot;dlib的初始化调用 - 增强版本\&quot;\&quot;\&quot;\n  1218\t        try:\n  1219\t            # 检查模型文件是否存在\n  1220\t            model_path = \&quot;./model/shape_predictor_68_face_landmarks.dat\&quot;\n  1221\t            if not os.path.exists(model_path):\n  1222\t                data = {\n  1223\t                    'type':'msg',\n  1224\t                    'value':u\&quot;❌ 模型文件不存在，请下载 shape_predictor_68_face_landmarks.dat\\n\&quot;\n  1225\t                }\n  1226\t                self.safe_emit_signal(data)\n  1227\t                return\n  1228\t\n  1229\t            # 初始化检测器\n  1230\t            print(\&quot; 正在初始化dlib人脸检测器...\&quot;)\n  1231\t            self.detector = dlib.get_frontal_face_detector()\n  1232\t            print(\&quot; 正在加载特征点预测器...\&quot;)\n  1233\t            self.predictor = dlib.shape_predictor(model_path)\n  1234\t\n  1235\t            # 验证模型加载\n  1236\t            if self.detector is None or self.predictor is None:\n  1237\t                print(\&quot;❌ 检测器或预测器为None\&quot;)\n  1238\t                data = {\n  1239\t                    'type':'msg',\n  1240\t                    'value':u\&quot;❌ 人脸检测模型加载失败\\n\&quot;\n  1241\t                }\n  1242\t                self.safe_emit_signal(data)\n  1243\t                return\n  1244\t\n  1245\t            print(\&quot;✅ 检测器初始化成功\&quot;)\n  1246\t            print(f\&quot;   检测器类型: {type(self.detector)}\&quot;)\n  1247\t            print(f\&quot;   预测器类型: {type(self.predictor)}\&quot;)\n  1248\t\n  1249\t            data = {\n  1250\t                'type':'msg',\n  1251\t                'value':u\&quot;✅ 人脸检测模型加载成功!!!\\n\&quot;\n  1252\t            }\n  1253\t            self.safe_emit_signal(data)\n  1254\t\n  1255\t        except Exception as e:\n  1256\t            data = {\n  1257\t                'type':'msg',\n  1258\t                'value':f\&quot;❌ 模型初始化失败: {e}\\n\&quot;\n  1259\t            }\n  1260\t            self.safe_emit_signal(data)\n  1261\t            return\n  1262\t\n  1263\t        (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\&quot;left_eye\&quot;]\n  1264\t        (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\&quot;right_eye\&quot;]\n  1265\t        (mStart, mEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\&quot;mouth\&quot;]\n  1266\t\n  1267\t        self.cap = None\n  1268\t\n  1269\t        # 简化的摄像头初始化\n  1270\t        print(f\&quot;尝试打开摄像头，索引: {self.VIDEO_STREAM}\&quot;)\n  1271\t\n  1272\t        # 直接尝试打开摄像头，不过度复杂化\n  1273\t        success = False\n  1274\t        for camera_index in [self.VIDEO_STREAM, 0, 1]:  # 尝试当前索引、0、1\n  1275\t            try:\n  1276\t                print(f\&quot;  尝试摄像头索引 {camera_index}...\&quot;)\n  1277\t                self.cap = cv2.VideoCapture(camera_index, cv2.CAP_DSHOW)\n  1278\t\n  1279\t                if self.cap.isOpened():\n  1280\t                    # 基本设置\n  1281\t                    self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n  1282\t                    self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n  1283\t                    self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n  1284\t\n  1285\t                    # 测试读取\n  1286\t                    ret, test_frame = self.cap.read()\n  1287\t                    if ret and test_frame is not None:\n  1288\t                        print(f\&quot;  ✓ 摄像头 {camera_index} 工作正常\&quot;)\n  1289\t                        self.VIDEO_STREAM = camera_index\n  1290\t                        self.CAMERA_STYLE = True\n  1291\t                        success = True\n  1292\t                        data['value'] = f\&quot;打开摄像头成功(索引{camera_index})!!!\&quot;\n  1293\t                        break\n  1294\t                    else:\n  1295\t                        print(f\&quot;  ✗ 摄像头 {camera_index} 无法读取\&quot;)\n  1296\t                        self.cap.release()\n  1297\t                        self.cap = None\n  1298\t                else:\n  1299\t                    print(f\&quot;  ✗ 摄像头 {camera_index} 无法打开\&quot;)\n  1300\t                    if self.cap is not None:\n  1301\t                        self.cap.release()\n  1302\t                        self.cap = None\n  1303\t\n  1304\t            except Exception as e:\n  1305\t                print(f\&quot;  ✗ 摄像头 {camera_index} 异常: {e}\&quot;)\n  1306\t                if hasattr(self, 'cap') and self.cap is not None:\n  1307\t                    try:\n  1308\t                        self.cap.release()\n  1309\t                    except:\n  1310\t                        pass\n  1311\t                    self.cap = None\n  1312\t\n  1313\t        if not success:\n  1314\t            data['value'] = u\&quot;摄像头打开失败!!!\&quot;\n  1315\t            print(\&quot;未找到可用的摄像头设备\&quot;)\n  1316\t        else:\n  1317\t            # 简化的参数设置\n  1318\t            self._optimize_camera_brightness()\n  1319\t        self.safe_emit_signal(data)\n  1320\t\n  1321\t        # 打印最终摄像头状态\n  1322\t        if hasattr(self, 'cap') and self.cap is not None and self.cap.isOpened():\n  1323\t            width = self.cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n  1324\t            height = self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n  1325\t            fps = self.cap.get(cv2.CAP_PROP_FPS)\n  1326\t            backend = self.cap.getBackendName()\n  1327\t            print(f\&quot;摄像头状态: 已打开\&quot;)\n  1328\t            print(f\&quot;  分辨率: {width}x{height}\&quot;)\n  1329\t            print(f\&quot;  设置FPS: {fps}\&quot;)\n  1330\t            print(f\&quot;  后端: {backend}\&quot;)\n  1331\t\n  1332\t            # 如果FPS为0，尝试手动设置\n  1333\t            if fps == 0:\n  1334\t                print(\&quot;检测到FPS为0，尝试手动设置...\&quot;)\n  1335\t                self.cap.set(cv2.CAP_PROP_FPS, 30)\n  1336\t                new_fps = self.cap.get(cv2.CAP_PROP_FPS)\n  1337\t                print(f\&quot;  重新设置后FPS: {new_fps}\&quot;)\n  1338\t        else:\n  1339\t            print(\&quot;未找到可用的视频源\&quot;)\n  1340\t\n  1341\t        # 初始化FPS计算变量\n  1342\t        self.frame_count = 0\n  1343\t        self.fps_start_time = time.time()\n  1344\t        self.actual_fps = 0\n  1345\t\n  1346\t        # 初始化时间变量\n  1347\t        t_time = datetime.datetime.now()\n  1348\t        e_time = datetime.datetime.now()\n  1349\t        h_time = datetime.datetime.now()\n  1350\t\n  1351\t        # 成功打开视频，循环读取视频流\n  1352\t        print(\&quot;开始视频流处理...\&quot;)\n  1353\t\n  1354\t        # 初始化性能监控变量\n  1355\t        frame_count = 0\n  1356\t        error_count = 0\n  1357\t        last_status_time = time.time()\n  1358\t\n  1359\t        while self.is_running:\n  1360\t            try:\n  1361\t                # 在循环开始时立即检查停止标志\n  1362\t                if not self.is_running:\n  1363\t                    break\n  1364\t\n  1365\t                start_time = datetime.datetime.now()\n  1366\t                res = ['-' for _ in range(9)]\n  1367\t\n  1368\t                # 初始化CNN检测结果变量，确保在整个循环中都有定义\n  1369\t                cnn_result = None\n  1370\t\n  1371\t                # 简单的摄像头状态检查\n  1372\t                if not hasattr(self, 'cap') or self.cap is None:\n  1373\t                    print(\&quot;摄像头未初始化\&quot;)\n  1374\t                    break\n  1375\t\n  1376\t                # 简化的摄像头检查 - 只在必要时检查\n  1377\t                if not self.cap.isOpened():\n  1378\t                    print(\&quot;摄像头连接丢失，尝试重新连接...\&quot;)\n  1379\t                    if not self._reconnect_camera():\n  1380\t                        print(\&quot;摄像头重连失败，停止检测\&quot;)\n  1381\t                        break\n  1382\t                    continue\n  1383\t\n  1384\t                # 优化的帧读取 - 减少卡顿\n  1385\t                flag, im_rd = self.cap.read()\n  1386\t\n  1387\t                if not flag or im_rd is None or im_rd.size == 0:\n  1388\t                    error_count += 1\n  1389\t                    # 减少错误报告频率\n  1390\t                    if error_count % 30 == 0:\n  1391\t                        print(f\&quot;读取帧失败，错误计数: {error_count}\&quot;)\n  1392\t\n  1393\t                    # 如果连续错误太多，尝试重连\n  1394\t                    if error_count &gt; 100:\n  1395\t                        print(\&quot;错误过多，尝试重连摄像头...\&quot;)\n  1396\t                        if not self._reconnect_camera():\n  1397\t                            break\n  1398\t                        error_count = 0\n  1399\t\n  1400\t                    # 使用配置文件中的延迟时间\n  1401\t                    time.sleep(camera_config.MAIN_LOOP_DELAY)\n  1402\t                    continue\n  1403\t\n  1404\t                # 成功读取帧，重置错误计数\n  1405\t                if error_count &gt; 0:\n  1406\t                    error_count = 0\n  1407\t\n  1408\t                frame_count += 1\n  1409\t\n  1410\t                # 跳帧处理 - 使用配置文件参数\n  1411\t                skip_detection = (frame_count % camera_config.FRAME_SKIP_DETECTION != 0)\n  1412\t\n  1413\t                # 每10秒报告一次状态，减少输出频率\n  1414\t                current_time_float = time.time()\n  1415\t                if current_time_float - last_status_time &gt;= 10.0:\n  1416\t                    fps = frame_count / (current_time_float - last_status_time)\n  1417\t                    print(f\&quot;处理状态: {frame_count} 帧, FPS: {fps:.1f}, 错误: {error_count}\&quot;)\n  1418\t                    frame_count = 0\n  1419\t                    last_status_time = current_time_float\n  1420\t\n  1421\t                # 改进的图像处理 - 增强人脸检测\n  1422\t                try:\n  1423\t                    # 验证图像格式和尺寸\n  1424\t                    if len(im_rd.shape) != 3 or im_rd.shape[2] != 3:\n  1425\t                        print(f\&quot;图像格式异常: {im_rd.shape}\&quot;)\n  1426\t                        continue\n  1427\t\n  1428\t                    height, width = im_rd.shape[:2]\n  1429\t                    if height &lt; 100 or width &lt; 100:\n  1430\t                        print(f\&quot;图像尺寸过小: {width}x{height}\&quot;)\n  1431\t                        continue\n  1432\t\n  1433\t                    # 应用暗图像增强 - 解决摄像头太暗的问题\n  1434\t                    im_rd = self._enhance_dark_frame(im_rd)\n  1435\t\n  1436\t                    # 转换为灰度图像\n  1437\t                    img_gray = cv2.cvtColor(im_rd, cv2.COLOR_BGR2GRAY)\n  1438\t\n  1439\t                    # 验证灰度图像\n  1440\t                    if img_gray is None or img_gray.size == 0:\n  1441\t                        print(\&quot;灰度转换失败\&quot;)\n  1442\t                        continue\n  1443\t\n  1444\t                    # 增强图像质量以提高人脸检测率\n  1445\t                    mean_brightness = np.mean(img_gray)\n  1446\t\n  1447\t                    # 创建增强版本的图像\n  1448\t                    enhanced_gray = img_gray.copy()\n  1449\t\n  1450\t                    # 增强的亮度调整 - 针对暗环境优化\n  1451\t                    if mean_brightness &lt; 120:  # 提高阈值，更积极地增强暗图像\n  1452\t                        # 图像太暗，大幅增加亮度和对比度\n  1453\t                        enhanced_gray = cv2.convertScaleAbs(enhanced_gray, alpha=1.6, beta=40)\n  1454\t\n  1455\t                        # 使用CLAHE进行局部对比度增强\n  1456\t                        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n  1457\t                        enhanced_gray = clahe.apply(enhanced_gray)\n  1458\t\n  1459\t                        #&lt;response clipped&gt;&lt;NOTE&gt;To save on context only part of this file has been shown to you.&lt;/NOTE&gt;\nTotal lines in file: 2432\n&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_01RQTUrKLnrCT4tCagEYcUem&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;D:/FatigueDetection&quot;,&quot;folderRoot&quot;:&quot;D:/FatigueDetection&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-04T07:10:52.411Z&quot;,&quot;response_text&quot;:&quot;让我继续查看文件的其余部分，特别是关于打哈欠检测模型的部分：\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;ab16ec88-288b-4277-a058-d33621d1f7bc&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_014KkNVqKW6bXo5zogVj5Wsd&quot;,&quot;tool_name&quot;:&quot;view&quot;,&quot;input_json&quot;:&quot;{\&quot;path\&quot;: \&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot;, \&quot;type\&quot;: \&quot;file\&quot;, \&quot;view_range\&quot;: [54, 121]}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;现在让我查看模型的具体结构定义：\n&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;Regex search results for pattern: class YawnCNNLSTM|def __init__|CNN\\+LSTM|打哈欠检测 in Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\nFound 55 matching lines:\n\n     36\t\n     37\texcept ImportError as e:\n     38\t    print(f\&quot;TensorFlow导入失败: {e}\&quot;)\n     39\t    TENSORFLOW_AVAILABLE = False\n     40\t\n&gt;    41\t# 尝试导入PyTorch用于CNN+LSTM打哈欠检测\n     42\ttry:\n     43\t    import torch\n     44\t    import torch.nn as nn\n     45\t    PYTORCH_AVAILABLE = True\n     46\t    print(\&quot;✅ PyTorch可用，将启用CNN+LSTM打哈欠检测\&quot;)\n     47\texcept ImportError as e:\n     48\t    print(f\&quot;PyTorch导入失败: {e}\&quot;)\n     49\t    PYTORCH_AVAILABLE = False\n     50\t\n     51\t\n&gt;    52\t# CNN+LSTM模型定义（用于打哈欠检测）\n     53\tif PYTORCH_AVAILABLE:\n     54\t    class YawnCNNLSTM(nn.Module):\n     55\t        \&quot;\&quot;\&quot;专门用于打哈欠检测的CNN+LSTM模型\&quot;\&quot;\&quot;\n     56\t        def __init__(self, input_size, hidden_size, num_layers, output_size):\n     57\t            super(YawnCNNLSTM, self).__init__()\n     58\t\n     59\t            # CNN layers - Extract spatial features\n     60\t            self.cnn = nn.Sequential(\n     61\t                nn.Conv1d(input_size, 64, kernel_size=3, padding=1),\n...\n    119\t\n    120\t            return output\n    121\t\n    122\t\n    123\t    class YawnDetector:\n&gt;   124\t        \&quot;\&quot;\&quot;专门用于打哈欠检测的类\&quot;\&quot;\&quot;\n    125\t        def __init__(self, model_path=None, seq_length=30, consecutive_frames=15):\n    126\t            self.model = None\n    127\t            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if PYTORCH_AVAILABLE else None\n    128\t            self.seq_length = seq_length\n    129\t            self.features_buffer = []\n    130\t            self.is_available = False\n...\n    136\t\n    137\t            if PYTORCH_AVAILABLE and model_path and os.path.exists(model_path):\n    138\t                self.load_model(model_path)\n    139\t\n    140\t        def load_model(self, model_path):\n&gt;   141\t            \&quot;\&quot;\&quot;加载训练好的CNN+LSTM模型\&quot;\&quot;\&quot;\n    142\t            try:\n    143\t                # 初始化模型（与训练时的结构保持一致）\n    144\t                self.model = YawnCNNLSTM(\n    145\t                    input_size=138,  # 2 (EAR, MAR) + 68*2 (landmark coordinates)\n    146\t                    hidden_size=64,\n...\n    150\t\n    151\t                # 加载模型权重\n    152\t                self.model.load_state_dict(torch.load(model_path, map_location=self.device, weights_only=False))\n    153\t                self.model.eval()\n    154\t                self.is_available = True\n&gt;   155\t                print(f\&quot;✅ CNN+LSTM打哈欠检测模型加载成功: {model_path}\&quot;)\n    156\t\n    157\t            except Exception as e:\n    158\t                print(f\&quot;❌ CNN+LSTM打哈欠检测模型加载失败: {e}\&quot;)\n    159\t                self.model = None\n    160\t                self.is_available = False\n    161\t\n    162\t        def extract_features(self, landmarks, ear, mar, frame_height=480):\n    163\t            \&quot;\&quot;\&quot;提取特征向量（与real_pljc保持一致）\&quot;\&quot;\&quot;\n...\n    215\t                    is_fatigued = False\n    216\t\n    217\t                return is_fatigued, prediction\n    218\t\n    219\t            except Exception as e:\n&gt;   220\t                print(f\&quot;CNN+LSTM打哈欠预测失败: {e}\&quot;)\n    221\t                return False, 0.0\n    222\t\n    223\t        def reset_state(self):\n    224\t            \&quot;\&quot;\&quot;重置检测器状态（用于冷却期）\&quot;\&quot;\&quot;\n    225\t            self.fatigue_frames = 0\n    226\t            self.frame_count = 0\n    227\t\n    228\t\n    229\t# 如果PyTorch不可用，定义占位符类\n    230\tif not PYTORCH_AVAILABLE:\n&gt;   231\t    class YawnCNNLSTM:\n    232\t        def __init__(self, *args, **kwargs):\n    233\t            pass\n    234\t\n    235\t    class YawnDetector:\n    236\t        def __init__(self, *args, **kwargs):\n    237\t            self.is_available = False\n    238\t\n    239\t\n    240\t# 处理TensorFlow导入失败的情况\n    241\ttry:\n    242\t    # 这个try块是为了兼容原有的except ImportError\n    243\t    pass\n    244\texcept ImportError:\n    245\t    # 如果导入失败，定义基本占位符\n    246\t    class Sequential:\n&gt;   247\t        def __init__(self):\n    248\t            pass\n    249\t\n    250\t        def add(self, layer):\n    251\t            pass\n    252\t\n    253\t    class ResNet50:\n&gt;   254\t        def __init__(self, *args, **kwargs):\n    255\t            pass\n    256\t\n    257\t    class LSTM:\n    258\t        def __init__(self, *args, **kwargs):\n    259\t            pass\n    260\t\n    261\t    class Dense:\n    262\t        def __init__(self, *args, **kwargs):\n    263\t            pass\n    264\t\n    265\t    class Conv2D:\n    266\t        def __init__(self, *args, **kwargs):\n    267\t            pass\n    268\t\n    269\t    class MaxPooling2D:\n    270\t        def __init__(self, *args, **kwargs):\n    271\t            pass\n    272\t\n    273\t    class Flatten:\n    274\t        def __init__(self, *args, **kwargs):\n    275\t            pass\n    276\t\n    277\t    tf = None\n    278\t    TENSORFLOW_AVAILABLE = False\n    279\t\n...\n    284\t\n    285\tclass MainUI(QtWidgets.QWidget, main_ui.Ui_Form):\n    286\t    # 信号，在UI线程中，不能在其他线程直接操作UI\n    287\t    thread_signal = QtCore.Signal(dict)\n    288\t\n&gt;   289\t    def __init__(self):\n    290\t        super().__init__()\n    291\t        self.setupUi(self)\n    292\t\n    293\t        # 初始化摄像头列表\n    294\t        self.cameras = []\n...\n    312\t        self.lstm_model = None\n    313\t\n    314\t        # 初始化CNN疲劳检测器\n    315\t        self.cnn_detector = None\n    316\t\n&gt;   317\t        # 初始化CNN+LSTM打哈欠检测器\n    318\t        self.yawn_detector = None\n    319\t\n    320\t        if TENSORFLOW_AVAILABLE:\n    321\t            try:\n    322\t                self.init_models()\n...\n    324\t            except Exception as e:\n    325\t                print(f\&quot;模型初始化失败: {e}\&quot;)\n    326\t        else:\n    327\t            print(\&quot;TensorFlow未安装，使用简化功能\&quot;)\n    328\t\n&gt;   329\t        # 初始化CNN+LSTM打哈欠检测器\n    330\t        if PYTORCH_AVAILABLE:\n    331\t            try:\n    332\t                self.init_yawn_detector()\n    333\t            except Exception as e:\n    334\t                print(f\&quot;CNN+LSTM打哈欠检测器初始化失败: {e}\&quot;)\n    335\t\n    336\t        # 连接信号\n    337\t        # self.thread_signal.connect(self.thread_singnal_slot)\n    338\t\n    339\t        # 六个功能是否要用\n...\n    435\t        # 优化后的眨眼检测参数 - 进一步提高敏感度\n    436\t        self.EYE_AR_THRESH = 0.20  # 进一步降低阈值，提高敏感度（原0.22）\n    437\t        self.EYE_AR_CONSEC_FRAMES = 2  # 保持较低的连续帧要求\n    438\t        self.EYE_AR_UPPER_THRESH = 0.40  # 适当提高上限，避免过滤正常眨眼\n    439\t\n&gt;   440\t        # 优化后的打哈欠检测参数 - 进一步提高敏感度\n    441\t        self.MAR_THRESH = 0.40  # 进一步降低阈值，提高敏感度（原0.45）\n    442\t        self.MAR_DURATION_THRESH = 0.6  # 进一步降低哈欠持续时间阈值（秒）\n    443\t        self.MOUTH_AR_CONSEC_FRAMES = 2  # 减少连续帧要求\n    444\t\n    445\t        # 优化后的瞌睡点头检测参数\n...\n    473\t        self.baseline_mar = 0.4  # 基线MAR值（将动态调整）\n    474\t\n    475\t        # 打哈欠冷却机制\n    476\t        self.last_yawn_time = None  # 上次检测到打哈欠的时间\n    477\t        self.yawn_cooldown_seconds = 3.0  # 打哈欠冷却时间（秒）\n&gt;   478\t        self.yawn_detection_enabled = True  # 打哈欠检测是否启用\n    479\t\n    480\t        # 自适应阈值调整\n    481\t        self.calibration_frames = 0  # 校准帧数\n    482\t        self.calibration_period = 300  # 校准周期（帧数）\n    483\t        self.adaptive_mode = True  # 是否启用自适应模式\n...\n    842\t            self.cnn_detector = None\n    843\t\n    844\t    def _create_simple_detector(self):\n    845\t        \&quot;\&quot;\&quot;创建简化的疲劳检测器\&quot;\&quot;\&quot;\n    846\t        class SimpleFatigueDetector:\n&gt;   847\t            def __init__(self, resnet_model):\n    848\t                self.resnet = resnet_model\n    849\t                self.available = resnet_model is not None\n    850\t\n    851\t            def is_available(self):\n    852\t                return self.available\n...\n    888\t                    return None\n    889\t\n    890\t        return SimpleFatigueDetector(self.resnet)\n    891\t\n    892\t    def init_yawn_detector(self):\n&gt;   893\t        \&quot;\&quot;\&quot;初始化CNN+LSTM打哈欠检测器\&quot;\&quot;\&quot;\n    894\t        try:\n    895\t            # 检查是否有训练好的模型 - 现在模型文件在model文件夹中\n    896\t            model_path = './model/best_fatigue_model.pth'\n    897\t            if not os.path.exists(model_path):\n    898\t                # 尝试其他可能的路径\n...\n    905\t                model_path = 'D:/code/PythonProject2/real_pljc/models/best_fatigue_model.pth'\n    906\t\n    907\t            if os.path.exists(model_path):\n    908\t                self.yawn_detector = YawnDetector(model_path)\n    909\t                if self.yawn_detector.is_available:\n&gt;   910\t                    print(\&quot;✅ CNN+LSTM打哈欠检测器已加载\&quot;)\n    911\t                else:\n    912\t                    print(\&quot;⚠️ CNN+LSTM打哈欠检测器加载失败\&quot;)\n    913\t                    self.yawn_detector = None\n    914\t            else:\n    915\t                print(\&quot;⚠️ 未找到CNN+LSTM打哈欠检测模型\&quot;)\n    916\t                self.yawn_detector = None\n    917\t\n    918\t        except Exception as e:\n    919\t            print(f\&quot;CNN+LSTM打哈欠检测器初始化失败: {e}\&quot;)\n    920\t            self.yawn_detector = None\n    921\t\n    922\t    def _init_resnet(self):\n    923\t        \&quot;\&quot;\&quot;初始化ResNet50模型\&quot;\&quot;\&quot;\n    924\t        try:\n...\n   1183\t            print(f\&quot; 眨眼检测详情 - EAR: {ear:.3f}, 阈值: {thresh:.3f}, 是否眨眼: {is_blink}\&quot;)\n   1184\t\n   1185\t        return is_blink\n   1186\t\n   1187\t    def is_valid_yawn(self, mar, current_time):\n&gt;  1188\t        \&quot;\&quot;\&quot;改进的打哈欠检测 - 更宽松的条件\&quot;\&quot;\&quot;\n   1189\t        # 使用简化的阈值检测\n   1190\t        thresh = self.MAR_THRESH\n   1191\t\n   1192\t        is_mouth_open = mar &gt; thresh\n   1193\t\n...\n   1592\t                            except Exception as e:\n   1593\t                                if frame_count % 60 == 0:  # 减少错误信息频率\n   1594\t                                    print(f\&quot;CNN检测失败: {e}\&quot;)\n   1595\t                                cnn_result = None  # 确保在异常情况下重置为None\n   1596\t\n&gt;  1597\t                        # 预先计算EAR和MAR，因为CNN+LSTM模型需要这些值\n   1598\t                        # 提取左眼和右眼坐标\n   1599\t                        leftEye = shape[lStart:lEnd]\n   1600\t                        rightEye = shape[rStart:rEnd]\n   1601\t                        # 构造函数计算左右眼的EAR值，使用平均值作为最终的EAR\n   1602\t                        leftEAR = self.eye_aspect_ratio(leftEye)\n...\n   1609\t\n   1610\t                        # 获取当前时间（确保在所有地方都可用）\n   1611\t                        current_time = datetime.datetime.now()\n   1612\t\n   1613\t                        \&quot;\&quot;\&quot;\n&gt;  1614\t                        打哈欠 - 集成CNN+LSTM和启发式检测（带冷却机制）\n   1615\t                        \&quot;\&quot;\&quot;\n   1616\t                        if self.fun[1]:\n   1617\t                            # 使用cv2.convexHull获得凸包位置，使用drawContours画出轮廓位置进行画图操作\n   1618\t                            mouthHull = cv2.convexHull(mouth)\n   1619\t                            cv2.drawContours(im_rd, [mouthHull], -1, (0, 255, 0), 1)\n...\n   1628\t                                    cv2.putText(im_rd, f\&quot;Yawn Cooldown: {cooldown_remaining:.1f}s\&quot;, (10, 160),\n   1629\t                                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n   1630\t                                else:\n   1631\t                                    self.yawn_detection_enabled = True\n   1632\t\n&gt;  1633\t                            # CNN+LSTM打哈欠检测（只在启用时进行）\n   1634\t                            cnn_lstm_yawn_detected = False\n   1635\t                            cnn_lstm_confidence = 0.0\n   1636\t\n   1637\t                            if self.yawn_detector and self.yawn_detector.is_available:\n   1638\t                                try:\n...\n   1640\t                                    frame_height = im_rd.shape[0]\n   1641\t                                    features = self.yawn_detector.extract_features(shape, ear, mar, frame_height)\n   1642\t                                    if features is not None:\n   1643\t                                        self.yawn_detector.update_buffer(features)\n   1644\t\n&gt;  1645\t                                        # 进行CNN+LSTM预测（传递检测启用状态）\n   1646\t                                        cnn_lstm_yawn_detected, cnn_lstm_confidence = self.yawn_detector.predict_yawn(self.yawn_detection_enabled)\n   1647\t\n   1648\t                                        # 在界面上显示CNN+LSTM检测结果\n   1649\t                                        if frame_count % 10 == 0:  # 每10帧显示一次\n   1650\t                                            status = \&quot;ENABLED\&quot; if self.yawn_detection_enabled else \&quot;COOLDOWN\&quot;\n   1651\t                                            cnn_lstm_text = f\&quot;CNN+LSTM: {cnn_lstm_confidence:.2f} (连续:{self.yawn_detector.fatigue_frames}) [{status}]\&quot;\n   1652\t                                            color = (0, 0, 255) if cnn_lstm_yawn_detected else (0, 255, 0)\n   1653\t                                            cv2.putText(im_rd, cnn_lstm_text, (10, 140), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n   1654\t\n   1655\t                                except Exception as e:\n   1656\t                                    if frame_count % 60 == 0:  # 减少错误信息频率\n&gt;  1657\t                                        print(f\&quot;CNN+LSTM打哈欠检测失败: {e}\&quot;)\n   1658\t\n   1659\t                            # 只使用CNN+LSTM检测打哈欠（移除启发式检测）\n   1660\t                            yawn_detected = False\n   1661\t                            detection_method = \&quot;\&quot;\n   1662\t\n   1663\t                            # 注意：cnn_lstm_yawn_detected已经包含了连续帧判断\n   1664\t                            if cnn_lstm_yawn_detected:\n&gt;  1665\t                                # CNN+LSTM检测到打哈欠（已经通过连续帧验证）\n   1666\t                                yawn_detected = True\n   1667\t                                detection_method = \&quot;CNN+LSTM\&quot;\n   1668\t                                print(f\&quot; CNN+LSTM检测到打哈欠: 置信度={cnn_lstm_confidence:.3f}, 连续帧={self.yawn_detector.fatigue_frames}\&quot;)\n   1669\t\n   1670\t                            # 如果CNN+LSTM不可用，显示提示信息\n   1671\t                            if not (self.yawn_detector and self.yawn_detector.is_available):\n   1672\t                                # 在界面上显示CNN+LSTM不可用的提示\n   1673\t                                cv2.putText(im_rd, \&quot;CNN+LSTM Yawn Detection: UNAVAILABLE\&quot;, (10, 140),\n   1674\t                                          cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n   1675\t                                if self._yawn_debug_counter % 300 == 0:  # 每300帧提示一次\n   1676\t                                    print(\&quot;⚠️ CNN+LSTM打哈欠检测不可用，请检查模型文件和PyTorch安装\&quot;)\n   1677\t\n   1678\t                            # 添加哈欠检测调试信息\n   1679\t                            if not hasattr(self, '_yawn_debug_counter'):\n   1680\t                                self._yawn_debug_counter = 0\n   1681\t                            self._yawn_debug_counter += 1\n   1682\t                            if self._yawn_debug_counter % 60 == 0:  # 每60帧打印一次\n   1683\t                                if self.yawn_detector and self.yawn_detector.is_available:\n&gt;  1684\t                                    print(f\&quot; 哈欠检测 - MAR: {mar:.3f}, CNN+LSTM: {cnn_lstm_confidence:.3f}, 连续帧: {self.yawn_detector.fatigue_frames}\&quot;)\n   1685\t                                else:\n   1686\t                                    print(f\&quot; 哈欠检测 - CNN+LSTM不可用，跳过检测\&quot;)\n   1687\t\n   1688\t                            # 只在检测启用且检测到打哈欠时才计数和记录\n   1689\t                            if yawn_detected and self.yawn_detection_enabled:\n   1690\t                                self.mTOTAL += 1\n   1691\t                                print(f\&quot; 检测到哈欠！方法: {detection_method}, 总计: {self.mTOTAL}, MAR: {mar:.3f}, CNN+LSTM置信度: {cnn_lstm_confidence:.3f}\&quot;)\n   1692\t                                self.safe_emit_signal({'type':'msg','value':time.strftime('%Y-%m-%d %H:%M ', time.localtime()) + f\&quot;打哈欠({detection_method})\&quot;})\n   1693\t                                res[4] = '哈欠'\n   1694\t\n   1695\t                                # 设置冷却时间\n   1696\t                                self.last_yawn_time = current_time\n   1697\t                                self.yawn_detection_enabled = False\n&gt;  1698\t                                print(f\&quot; 打哈欠检测进入冷却期 {self.yawn_cooldown_seconds} 秒\&quot;)\n   1699\t\n   1700\t                                # 记录哈欠事件到统计数据库\n   1701\t                                if self.fatigue_stats:\n   1702\t                                    try:\n   1703\t                                        from fatigue_statistics import FatigueEvent\n&gt;  1704\t                                        # 使用CNN+LSTM的置信度\n   1705\t                                        confidence = cnn_lstm_confidence\n   1706\t                                        event = FatigueEvent(\n   1707\t                                            timestamp=current_time,\n   1708\t                                            event_type='yawn',\n   1709\t                                            value=1.0,\n\nTotal matches: 55\nTotal lines in file: 2432&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_01LQY4oCJQh7aJ3U8V5RcT9L&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;D:/FatigueDetection&quot;,&quot;folderRoot&quot;:&quot;D:/FatigueDetection&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-04T07:11:34.833Z&quot;,&quot;response_text&quot;:&quot;现在让我查看模型的具体结构定义：\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;d3f64a10-0bce-41ec-8b24-fe0e60d0962e&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_019NN3GXj1PLQ8BypEs3jW9g&quot;,&quot;tool_name&quot;:&quot;view&quot;,&quot;input_json&quot;:&quot;{\&quot;path\&quot;: \&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot;, \&quot;type\&quot;: \&quot;file\&quot;, \&quot;view_range\&quot;: [123, 227]}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;现在让我查看YawnDetector类的详细定义：\n&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;Here's the result of running `cat -n` on Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py:\n    54\t    class YawnCNNLSTM(nn.Module):\n    55\t        \&quot;\&quot;\&quot;专门用于打哈欠检测的CNN+LSTM模型\&quot;\&quot;\&quot;\n    56\t        def __init__(self, input_size, hidden_size, num_layers, output_size):\n    57\t            super(YawnCNNLSTM, self).__init__()\n    58\t\n    59\t            # CNN layers - Extract spatial features\n    60\t            self.cnn = nn.Sequential(\n    61\t                nn.Conv1d(input_size, 64, kernel_size=3, padding=1),\n    62\t                nn.BatchNorm1d(64),\n    63\t                nn.ReLU(),\n    64\t                nn.MaxPool1d(kernel_size=2),\n    65\t                nn.Conv1d(64, 128, kernel_size=3, padding=1),\n    66\t                nn.BatchNorm1d(128),\n    67\t                nn.ReLU(),\n    68\t                nn.MaxPool1d(kernel_size=2),\n    69\t                nn.Conv1d(128, 256, kernel_size=3, padding=1),\n    70\t                nn.BatchNorm1d(256),\n    71\t                nn.ReLU(),\n    72\t                nn.MaxPool1d(kernel_size=2)\n    73\t            )\n    74\t\n    75\t            # CNN output channels is 256\n    76\t            cnn_output_size = 256\n    77\t\n    78\t            # LSTM layers - Process temporal features\n    79\t            self.lstm = nn.LSTM(\n    80\t                input_size=cnn_output_size,\n    81\t                hidden_size=hidden_size,\n    82\t                num_layers=num_layers,\n    83\t                batch_first=True,\n    84\t                dropout=0.2,\n    85\t                bidirectional=True\n    86\t            )\n    87\t\n    88\t            # Fully connected layers - Output classification results\n    89\t            self.fc = nn.Sequential(\n    90\t                nn.Linear(hidden_size * 2, 128),\n    91\t                nn.ReLU(),\n    92\t                nn.Dropout(0.5),\n    93\t                nn.Linear(128, 64),\n    94\t                nn.ReLU(),\n    95\t                nn.Dropout(0.3),\n    96\t                nn.Linear(64, output_size)\n    97\t            )\n    98\t\n    99\t        def forward(self, x):\n   100\t            batch_size, seq_len, features = x.size()\n   101\t\n   102\t            # 重塑输入以通过CNN: (batch_size, features, seq_len)\n   103\t            x = x.permute(0, 2, 1)\n   104\t\n   105\t            # 通过CNN\n   106\t            cnn_out = self.cnn(x)\n   107\t\n   108\t            # 重塑CNN输出以通过LSTM: (batch_size, seq_len, cnn_features)\n   109\t            cnn_out = cnn_out.permute(0, 2, 1)\n   110\t\n   111\t            # 通过LSTM\n   112\t            lstm_out, _ = self.lstm(cnn_out)\n   113\t\n   114\t            # 取最后一个时间步的输出\n   115\t            lstm_out = lstm_out[:, -1, :]\n   116\t\n   117\t            # 通过全连接层\n   118\t            output = self.fc(lstm_out)\n   119\t\n   120\t            return output\n   121\t\nTotal lines in file: 2432\n&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_014KkNVqKW6bXo5zogVj5Wsd&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;D:/FatigueDetection&quot;,&quot;folderRoot&quot;:&quot;D:/FatigueDetection&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-04T07:12:19.505Z&quot;,&quot;response_text&quot;:&quot;现在让我查看YawnDetector类的详细定义：\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;3eefd5ac-4c81-47f5-9dc5-9a88188d08b0&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;根据我对main.py文件的分析，我可以为您详细解释打哈欠检测模型的定义：\n\n## 打哈欠检测模型定义\n\n在main.py中，打哈欠检测使用了一个基于PyTorch的**CNN+LSTM混合模型**，具体定义如下：\n\n### 1. 模型架构 - YawnCNNLSTM类\n\n&lt;augment_code_snippet path=\&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot; mode=\&quot;EXCERPT\&quot;&gt;\n```python\nclass YawnCNNLSTM(nn.Module):\n    \&quot;\&quot;\&quot;专门用于打哈欠检测的CNN+LSTM模型\&quot;\&quot;\&quot;\n    def __init__(self, input_size, hidden_size, num_layers, output_size):\n        super(YawnCNNLSTM, self).__init__()\n\n        # CNN layers - Extract spatial features\n        self.cnn = nn.Sequential(\n            nn.Conv1d(input_size, 64, kernel_size=3, padding=1),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2),\n            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2),\n            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2)\n        )\n```\n&lt;/augment_code_snippet&gt;\n\n### 2. 模型组件说明\n\n**CNN部分（空间特征提取）：**\n- 使用1D卷积层处理时序特征\n- 3层卷积结构：64→128→256通道\n- 每层包含：Conv1d + BatchNorm + ReLU + MaxPool\n- 用于提取空间特征模式\n\n**LSTM部分（时序特征处理）：**\n&lt;augment_code_snippet path=\&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot; mode=\&quot;EXCERPT\&quot;&gt;\n```python\n# LSTM layers - Process temporal features\nself.lstm = nn.LSTM(\n    input_size=cnn_output_size,\n    hidden_size=hidden_size,\n    num_layers=num_layers,\n    batch_first=True,\n    dropout=0.2,\n    bidirectional=True\n)\n```\n&lt;/augment_code_snippet&gt;\n\n**全连接层（分类输出）：**\n&lt;augment_code_snippet path=\&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot; mode=\&quot;EXCERPT\&quot;&gt;\n```python\n# Fully connected layers - Output classification results\nself.fc = nn.Sequential(\n    nn.Linear(hidden_size * 2, 128),\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(128, 64),\n    nn.ReLU(),\n    nn.Dropout(0.3),\n    nn.Linear(64, output_size)\n)\n```\n&lt;/augment_code_snippet&gt;\n\n### 3. 模型参数配置\n\n&lt;augment_code_snippet path=\&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot; mode=\&quot;EXCERPT\&quot;&gt;\n```python\nself.model = YawnCNNLSTM(\n    input_size=138,  # 2 (EAR, MAR) + 68*2 (landmark coordinates)\n    hidden_size=64,\n    num_layers=1,\n    output_size=1\n).to(self.device)\n```\n&lt;/augment_code_snippet&gt;\n\n### 4. 特征提取\n\n模型使用138维特征向量：\n- **2个生理指标**：EAR（眼部长宽比）+ MAR（嘴部长宽比）\n- **136个坐标特征**：68个面部关键点的x,y坐标（68×2=136）\n\n&lt;augment_code_snippet path=\&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot; mode=\&quot;EXCERPT\&quot;&gt;\n```python\ndef extract_features(self, landmarks, ear, mar, frame_height=480):\n    # 归一化关键点坐标（以鼻尖为基准，使用帧高度归一化）\n    nose = landmarks[30]  # 鼻尖关键点\n    normalized_landmarks = (landmarks - nose).flatten() / frame_height\n    \n    # 组合特征：EAR, MAR + 68个关键点的x,y坐标\n    features = np.concatenate([[ear, mar], normalized_landmarks])\n    return features\n```\n&lt;/augment_code_snippet&gt;\n\n### 5. 检测逻辑\n\n模型采用**时序窗口 + 连续帧判断**的检测策略：\n- **时序窗口**：30帧的特征序列\n- **连续帧阈值**：需要连续15帧预测为疲劳才判定为打哈欠\n- **预测阈值**：单帧预测概率≥0.5认为是疲劳状态\n\n### 6. 模型优势\n\n1. **时空特征融合**：CNN提取空间特征，LSTM处理时序依赖\n2. **多模态输入**：结合生理指标和几何特征\n3. **鲁棒性强**：使用连续帧判断减少误检\n4. **实时性好**：轻量级网络结构，支持实时检测\n\n这个模型设计充分考虑了打哈欠行为的时序特性和空间特征，通过CNN+LSTM的组合实现了准确的疲劳检测。\n&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;根据我对main.py文件的分析，我可以为您详细解释打哈欠检测模型的定义：\n\n## 打哈欠检测模型定义\n\n在main.py中，打哈欠检测使用了一个基于PyTorch的**CNN+LSTM混合模型**，具体定义如下：\n\n### 1. 模型架构 - YawnCNNLSTM类\n\n````python path=Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py mode=EXCERPT\nclass YawnCNNLSTM(nn.Module):\n    \&quot;\&quot;\&quot;专门用于打哈欠检测的CNN+LSTM模型\&quot;\&quot;\&quot;\n    def __init__(self, input_size, hidden_size, num_layers, output_size):\n        super(YawnCNNLSTM, self).__init__()\n\n        # CNN layers - Extract spatial features\n        self.cnn = nn.Sequential(\n            nn.Conv1d(input_size, 64, kernel_size=3, padding=1),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2),\n            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2),\n            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2)\n        )\n````\n\n### 2. 模型组件说明\n\n**CNN部分（空间特征提取）：**\n- 使用1D卷积层处理时序特征\n- 3层卷积结构：64→128→256通道\n- 每层包含：Conv1d + BatchNorm + ReLU + MaxPool\n- 用于提取空间特征模式\n\n**LSTM部分（时序特征处理）：**\n````python path=Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py mode=EXCERPT\n# LSTM layers - Process temporal features\nself.lstm = nn.LSTM(\n    input_size=cnn_output_size,\n    hidden_size=hidden_size,\n    num_layers=num_layers,\n    batch_first=True,\n    dropout=0.2,\n    bidirectional=True\n)\n````\n\n**全连接层（分类输出）：**\n````python path=Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py mode=EXCERPT\n# Fully connected layers - Output classification results\nself.fc = nn.Sequential(\n    nn.Linear(hidden_size * 2, 128),\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(128, 64),\n    nn.ReLU(),\n    nn.Dropout(0.3),\n    nn.Linear(64, output_size)\n)\n````\n\n### 3. 模型参数配置\n\n````python path=Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py mode=EXCERPT\nself.model = YawnCNNLSTM(\n    input_size=138,  # 2 (EAR, MAR) + 68*2 (landmark coordinates)\n    hidden_size=64,\n    num_layers=1,\n    output_size=1\n).to(self.device)\n````\n\n### 4. 特征提取\n\n模型使用138维特征向量：\n- **2个生理指标**：EAR（眼部长宽比）+ MAR（嘴部长宽比）\n- **136个坐标特征**：68个面部关键点的x,y坐标（68×2=136）\n\n````python path=Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py mode=EXCERPT\ndef extract_features(self, landmarks, ear, mar, frame_height=480):\n    # 归一化关键点坐标（以鼻尖为基准，使用帧高度归一化）\n    nose = landmarks[30]  # 鼻尖关键点\n    normalized_landmarks = (landmarks - nose).flatten() / frame_height\n    \n    # 组合特征：EAR, MAR + 68个关键点的x,y坐标\n    features = np.concatenate([[ear, mar], normalized_landmarks])\n    return features\n````\n\n### 5. 检测逻辑\n\n模型采用**时序窗口 + 连续帧判断**的检测策略：\n- **时序窗口**：30帧的特征序列\n- **连续帧阈值**：需要连续15帧预测为疲劳才判定为打哈欠\n- **预测阈值**：单帧预测概率≥0.5认为是疲劳状态\n\n### 6. 模型优势\n\n1. **时空特征融合**：CNN提取空间特征，LSTM处理时序依赖\n2. **多模态输入**：结合生理指标和几何特征\n3. **鲁棒性强**：使用连续帧判断减少误检\n4. **实时性好**：轻量级网络结构，支持实时检测\n\n这个模型设计充分考虑了打哈欠行为的时序特性和空间特征，通过CNN+LSTM的组合实现了准确的疲劳检测。\n&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;Here's the result of running `cat -n` on Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py:\n   123\t    class YawnDetector:\n   124\t        \&quot;\&quot;\&quot;专门用于打哈欠检测的类\&quot;\&quot;\&quot;\n   125\t        def __init__(self, model_path=None, seq_length=30, consecutive_frames=15):\n   126\t            self.model = None\n   127\t            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if PYTORCH_AVAILABLE else None\n   128\t            self.seq_length = seq_length\n   129\t            self.features_buffer = []\n   130\t            self.is_available = False\n   131\t\n   132\t            # 添加连续帧判断逻辑（参考real_pljc）\n   133\t            self.consecutive_frames = consecutive_frames\n   134\t            self.fatigue_frames = 0\n   135\t            self.frame_count = 0\n   136\t\n   137\t            if PYTORCH_AVAILABLE and model_path and os.path.exists(model_path):\n   138\t                self.load_model(model_path)\n   139\t\n   140\t        def load_model(self, model_path):\n   141\t            \&quot;\&quot;\&quot;加载训练好的CNN+LSTM模型\&quot;\&quot;\&quot;\n   142\t            try:\n   143\t                # 初始化模型（与训练时的结构保持一致）\n   144\t                self.model = YawnCNNLSTM(\n   145\t                    input_size=138,  # 2 (EAR, MAR) + 68*2 (landmark coordinates)\n   146\t                    hidden_size=64,\n   147\t                    num_layers=1,\n   148\t                    output_size=1\n   149\t                ).to(self.device)\n   150\t\n   151\t                # 加载模型权重\n   152\t                self.model.load_state_dict(torch.load(model_path, map_location=self.device, weights_only=False))\n   153\t                self.model.eval()\n   154\t                self.is_available = True\n   155\t                print(f\&quot;✅ CNN+LSTM打哈欠检测模型加载成功: {model_path}\&quot;)\n   156\t\n   157\t            except Exception as e:\n   158\t                print(f\&quot;❌ CNN+LSTM打哈欠检测模型加载失败: {e}\&quot;)\n   159\t                self.model = None\n   160\t                self.is_available = False\n   161\t\n   162\t        def extract_features(self, landmarks, ear, mar, frame_height=480):\n   163\t            \&quot;\&quot;\&quot;提取特征向量（与real_pljc保持一致）\&quot;\&quot;\&quot;\n   164\t            if landmarks is None:\n   165\t                return None\n   166\t\n   167\t            try:\n   168\t                # 归一化关键点坐标（以鼻尖为基准，使用帧高度归一化）\n   169\t                nose = landmarks[30]  # 鼻尖关键点\n   170\t                normalized_landmarks = (landmarks - nose).flatten() / frame_height  # 使用帧高度归一化\n   171\t\n   172\t                # 组合特征：EAR, MAR + 68个关键点的x,y坐标\n   173\t                features = np.concatenate([[ear, mar], normalized_landmarks])\n   174\t                return features\n   175\t\n   176\t            except Exception as e:\n   177\t                print(f\&quot;特征提取失败: {e}\&quot;)\n   178\t                return None\n   179\t\n   180\t        def update_buffer(self, features):\n   181\t            \&quot;\&quot;\&quot;更新特征缓冲区\&quot;\&quot;\&quot;\n   182\t            if features is not None:\n   183\t                self.features_buffer.append(features)\n   184\t                if len(self.features_buffer) &gt; self.seq_length:\n   185\t                    self.features_buffer.pop(0)\n   186\t\n   187\t        def predict_yawn(self, detection_enabled=True):\n   188\t            \&quot;\&quot;\&quot;预测是否打哈欠（与real_pljc保持一致的逻辑）\&quot;\&quot;\&quot;\n   189\t            if not self.is_available or len(self.features_buffer) &lt; self.seq_length:\n   190\t                return False, 0.0\n   191\t\n   192\t            try:\n   193\t                # 准备输入序列\n   194\t                input_seq = np.array([self.features_buffer])\n   195\t                input_tensor = torch.FloatTensor(input_seq).to(self.device)\n   196\t\n   197\t                # 模型预测\n   198\t                with torch.no_grad():\n   199\t                    logits = self.model(input_tensor).item()\n   200\t                    prediction = torch.sigmoid(torch.tensor(logits)).item()\n   201\t\n   202\t                # 更新疲劳状态（参考real_pljc的连续帧判断逻辑）\n   203\t                self.frame_count += 1\n   204\t                if prediction &gt;= 0.5:  # 单帧预测阈值\n   205\t                    self.fatigue_frames += 1\n   206\t                else:\n   207\t                    self.fatigue_frames = 0\n   208\t\n   209\t                # 判定疲劳需要连续帧数达到阈值\n   210\t                is_fatigued = self.fatigue_frames &gt;= self.consecutive_frames\n   211\t\n   212\t                # 如果检测被禁用（冷却期），重置连续帧计数\n   213\t                if not detection_enabled and is_fatigued:\n   214\t                    self.fatigue_frames = 0\n   215\t                    is_fatigued = False\n   216\t\n   217\t                return is_fatigued, prediction\n   218\t\n   219\t            except Exception as e:\n   220\t                print(f\&quot;CNN+LSTM打哈欠预测失败: {e}\&quot;)\n   221\t                return False, 0.0\n   222\t\n   223\t        def reset_state(self):\n   224\t            \&quot;\&quot;\&quot;重置检测器状态（用于冷却期）\&quot;\&quot;\&quot;\n   225\t            self.fatigue_frames = 0\n   226\t            self.frame_count = 0\n   227\t\nTotal lines in file: 2432\n&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_019NN3GXj1PLQ8BypEs3jW9g&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;D:/FatigueDetection&quot;,&quot;folderRoot&quot;:&quot;D:/FatigueDetection&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-04T07:12:25.240Z&quot;,&quot;response_text&quot;:&quot;根据我对main.py文件的分析，我可以为您详细解释打哈欠检测模型的定义：\n\n## 打哈欠检测模型定义\n\n在main.py中，打哈欠检测使用了一个基于PyTorch的**CNN+LSTM混合模型**，具体定义如下：\n\n### 1. 模型架构 - YawnCNNLSTM类\n\n````python path=Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py mode=EXCERPT\nclass YawnCNNLSTM(nn.Module):\n    \&quot;\&quot;\&quot;专门用于打哈欠检测的CNN+LSTM模型\&quot;\&quot;\&quot;\n    def __init__(self, input_size, hidden_size, num_layers, output_size):\n        super(YawnCNNLSTM, self).__init__()\n\n        # CNN layers - Extract spatial features\n        self.cnn = nn.Sequential(\n            nn.Conv1d(input_size, 64, kernel_size=3, padding=1),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2),\n            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2),\n            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2)\n        )\n````\n\n### 2. 模型组件说明\n\n**CNN部分（空间特征提取）：**\n- 使用1D卷积层处理时序特征\n- 3层卷积结构：64→128→256通道\n- 每层包含：Conv1d + BatchNorm + ReLU + MaxPool\n- 用于提取空间特征模式\n\n**LSTM部分（时序特征处理）：**\n````python path=Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py mode=EXCERPT\n# LSTM layers - Process temporal features\nself.lstm = nn.LSTM(\n    input_size=cnn_output_size,\n    hidden_size=hidden_size,\n    num_layers=num_layers,\n    batch_first=True,\n    dropout=0.2,\n    bidirectional=True\n)\n````\n\n**全连接层（分类输出）：**\n````python path=Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py mode=EXCERPT\n# Fully connected layers - Output classification results\nself.fc = nn.Sequential(\n    nn.Linear(hidden_size * 2, 128),\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(128, 64),\n    nn.ReLU(),\n    nn.Dropout(0.3),\n    nn.Linear(64, output_size)\n)\n````\n\n### 3. 模型参数配置\n\n````python path=Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py mode=EXCERPT\nself.model = YawnCNNLSTM(\n    input_size=138,  # 2 (EAR, MAR) + 68*2 (landmark coordinates)\n    hidden_size=64,\n    num_layers=1,\n    output_size=1\n).to(self.device)\n````\n\n### 4. 特征提取\n\n模型使用138维特征向量：\n- **2个生理指标**：EAR（眼部长宽比）+ MAR（嘴部长宽比）\n- **136个坐标特征**：68个面部关键点的x,y坐标（68×2=136）\n\n````python path=Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py mode=EXCERPT\ndef extract_features(self, landmarks, ear, mar, frame_height=480):\n    # 归一化关键点坐标（以鼻尖为基准，使用帧高度归一化）\n    nose = landmarks[30]  # 鼻尖关键点\n    normalized_landmarks = (landmarks - nose).flatten() / frame_height\n    \n    # 组合特征：EAR, MAR + 68个关键点的x,y坐标\n    features = np.concatenate([[ear, mar], normalized_landmarks])\n    return features\n````\n\n### 5. 检测逻辑\n\n模型采用**时序窗口 + 连续帧判断**的检测策略：\n- **时序窗口**：30帧的特征序列\n- **连续帧阈值**：需要连续15帧预测为疲劳才判定为打哈欠\n- **预测阈值**：单帧预测概率≥0.5认为是疲劳状态\n\n### 6. 模型优势\n\n1. **时空特征融合**：CNN提取空间特征，LSTM处理时序依赖\n2. **多模态输入**：结合生理指标和几何特征\n3. **鲁棒性强**：使用连续帧判断减少误检\n4. **实时性好**：轻量级网络结构，支持实时检测\n\n这个模型设计充分考虑了打哈欠行为的时序特性和空间特征，通过CNN+LSTM的组合实现了准确的疲劳检测。\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;e58c8682-c5e7-4a7d-a0b4-e603c4451c64&quot;,&quot;request_message&quot;:&quot;这个程序是怎么把从摄像头获取到的视频信息加入模型并进行预测的，请给出完整的流程&quot;,&quot;rich_text_json_repr&quot;:{&quot;type&quot;:&quot;doc&quot;,&quot;content&quot;:[{&quot;type&quot;:&quot;paragraph&quot;,&quot;content&quot;:[{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;这个程序是怎么把从摄像头获取到的视频信息加入模型并进行预测的，请给出完整的流程&quot;}]}]},&quot;mentioned_items&quot;:[],&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_01Wg2YwP3S84X44Hj1MVjcVc&quot;,&quot;tool_name&quot;:&quot;codebase-retrieval&quot;,&quot;input_json&quot;:&quot;{\&quot;information_request\&quot;: \&quot;\\u89c6\\u9891\\u6d41\\u5904\\u7406\\u548c\\u6a21\\u578b\\u9884\\u6d4b\\u7684\\u5b8c\\u6574\\u6d41\\u7a0b\\uff0c\\u5305\\u62ec\\u6444\\u50cf\\u5934\\u521d\\u59cb\\u5316\\u3001\\u5e27\\u8bfb\\u53d6\\u3001\\u4eba\\u8138\\u68c0\\u6d4b\\u3001\\u7279\\u5f81\\u63d0\\u53d6\\u3001\\u6a21\\u578b\\u9884\\u6d4b\\u7b49\\u6b65\\u9aa4\&quot;}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;我来为您详细分析这个程序是如何将摄像头获取的视频信息加入模型并进行预测的完整流程。\n&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:0,&quot;type&quot;:0,&quot;text_node&quot;:{&quot;content&quot;:&quot;这个程序是怎么把从摄像头获取到的视频信息加入模型并进行预测的，请给出完整的流程&quot;}},{&quot;id&quot;:1,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;D:/FatigueDetection&quot;,&quot;folderRoot&quot;:&quot;D:/FatigueDetection&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-04T07:15:54.532Z&quot;,&quot;response_text&quot;:&quot;我来为您详细分析这个程序是如何将摄像头获取的视频信息加入模型并进行预测的完整流程。\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;8942d239-c747-4a02-be4f-859d68159c94&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_01FU8ztzTwKkejJrorkDB4fK&quot;,&quot;tool_name&quot;:&quot;view&quot;,&quot;input_json&quot;:&quot;{\&quot;path\&quot;: \&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot;, \&quot;type\&quot;: \&quot;file\&quot;, \&quot;view_range\&quot;: [1383, 1450]}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;现在让我查看主循环中的具体处理流程：\n&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;The following code sections were retrieved:\nPath: Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\n...\n   161\t\n   162\t        def extract_features(self, landmarks, ear, mar, frame_height=480):\n   163\t            \&quot;\&quot;\&quot;提取特征向量（与real_pljc保持一致）\&quot;\&quot;\&quot;\n   164\t            if landmarks is None:\n   165\t                return None\n   166\t\n   167\t            try:\n   168\t                # 归一化关键点坐标（以鼻尖为基准，使用帧高度归一化）\n   169\t                nose = landmarks[30]  # 鼻尖关键点\n   170\t                normalized_landmarks = (landmarks - nose).flatten() / frame_height  # 使用帧高度归一化\n   171\t\n   172\t                # 组合特征：EAR, MAR + 68个关键点的x,y坐标\n   173\t                features = np.concatenate([[ear, mar], normalized_landmarks])\n   174\t                return features\n   175\t\n   176\t            except Exception as e:\n   177\t                print(f\&quot;特征提取失败: {e}\&quot;)\n   178\t                return None\n   179\t\n   180\t        def update_buffer(self, features):\n   181\t            \&quot;\&quot;\&quot;更新特征缓冲区\&quot;\&quot;\&quot;\n   182\t            if features is not None:\n   183\t                self.features_buffer.append(features)\n   184\t                if len(self.features_buffer) &gt; self.seq_length:\n   185\t                    self.features_buffer.pop(0)\n...\n   680\t\n   681\t    def _init_camera_robust(self, camera_index):\n   682\t        \&quot;\&quot;\&quot;简化的摄像头初始化方法 - 避免闪烁\&quot;\&quot;\&quot;\n   683\t        print(f\&quot;正在初始化摄像头 {camera_index}...\&quot;)\n   684\t\n   685\t        # 优先使用DirectShow后端（Windows最稳定）\n   686\t        try:\n   687\t            print(f\&quot;  使用DirectShow后端...\&quot;)\n   688\t            cap = cv2.VideoCapture(camera_index, cv2.CAP_DSHOW)\n   689\t\n   690\t            if cap.isOpened():\n   691\t                # 立即测试读取\n   692\t                ret, frame = cap.read()\n   693\t                if ret and frame is not None and frame.size &gt; 0:\n   694\t                    print(f\&quot;摄像头 {camera_index} 初始化成功\&quot;)\n   695\t                    return cap\n   696\t                else:\n   697\t                    print(f\&quot;  无法读取帧\&quot;)\n   698\t                    cap.release()\n   699\t            else:\n   700\t                print(f\&quot;  无法打开摄像头\&quot;)\n   701\t                if cap is not None:\n   702\t                    cap.release()\n   703\t\n   704\t        except Exception as e:\n   705\t            print(f\&quot;  DirectShow初始化失败: {e}\&quot;)\n   706\t            if 'cap' in locals() and cap is not None:\n   707\t                try:\n   708\t                    cap.release()\n   709\t                except:\n   710\t                    pass\n...\n  1244\t\n  1245\t            print(\&quot;✅ 检测器初始化成功\&quot;)\n  1246\t            print(f\&quot;   检测器类型: {type(self.detector)}\&quot;)\n  1247\t            print(f\&quot;   预测器类型: {type(self.predictor)}\&quot;)\n  1248\t\n  1249\t            data = {\n  1250\t                'type':'msg',\n  1251\t                'value':u\&quot;✅ 人脸检测模型加载成功!!!\\n\&quot;\n  1252\t            }\n  1253\t            self.safe_emit_signal(data)\n  1254\t\n  1255\t        except Exception as e:\n  1256\t            data = {\n  1257\t                'type':'msg',\n  1258\t                'value':f\&quot;❌ 模型初始化失败: {e}\\n\&quot;\n  1259\t            }\n  1260\t            self.safe_emit_signal(data)\n  1261\t            return\n  1262\t\n  1263\t        (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\&quot;left_eye\&quot;]\n  1264\t        (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\&quot;right_eye\&quot;]\n  1265\t        (mStart, mEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\&quot;mouth\&quot;]\n  1266\t\n  1267\t        self.cap = None\n  1268\t\n  1269\t        # 简化的摄像头初始化\n  1270\t        print(f\&quot;尝试打开摄像头，索引: {self.VIDEO_STREAM}\&quot;)\n  1271\t\n  1272\t        # 直接尝试打开摄像头，不过度复杂化\n  1273\t        success = False\n  1274\t        for camera_index in [self.VIDEO_STREAM, 0, 1]:  # 尝试当前索引、0、1\n  1275\t            try:\n  1276\t                print(f\&quot;  尝试摄像头索引 {camera_index}...\&quot;)\n  1277\t                self.cap = cv2.VideoCapture(camera_index, cv2.CAP_DSHOW)\n  1278\t\n  1279\t                if self.cap.isOpened():\n  1280\t                    # 基本设置\n  1281\t                    self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n  1282\t                    self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n  1283\t                    self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n  1284\t\n  1285\t                    # 测试读取\n  1286\t                    ret, test_frame = self.cap.read()\n  1287\t                    if ret and test_frame is not None:\n  1288\t                        print(f\&quot;  ✓ 摄像头 {camera_index} 工作正常\&quot;)\n  1289\t                        self.VIDEO_STREAM = camera_index\n  1290\t                        self.CAMERA_STYLE = True\n  1291\t                        success = True\n  1292\t                        data['value'] = f\&quot;打开摄像头成功(索引{camera_index})!!!\&quot;\n  1293\t                        break\n  1294\t                    else:\n  1295\t                        print(f\&quot;  ✗ 摄像头 {camera_index} 无法读取\&quot;)\n  1296\t                        self.cap.release()\n  1297\t                        self.cap = None\n  1298\t                else:\n  1299\t                    print(f\&quot;  ✗ 摄像头 {camera_index} 无法打开\&quot;)\n  1300\t                    if self.cap is not None:\n  1301\t                        self.cap.release()\n  1302\t                        self.cap = None\n  1303\t\n  1304\t            except Exception as e:\n  1305\t                print(f\&quot;  ✗ 摄像头 {camera_index} 异常: {e}\&quot;)\n  1306\t                if hasattr(self, 'cap') and self.cap is not None:\n  1307\t                    try:\n  1308\t                        self.cap.release()\n  1309\t                    except:\n  1310\t                        pass\n  1311\t                    self.cap = None\n  1312\t\n  1313\t        if not success:\n  1314\t            data['value'] = u\&quot;摄像头打开失败!!!\&quot;\n  1315\t            print(\&quot;未找到可用的摄像头设备\&quot;)\n  1316\t        else:\n  1317\t            # 简化的参数设置\n  1318\t            self._optimize_camera_brightness()\n  1319\t        self.safe_emit_signal(data)\n  1320\t\n  1321\t        # 打印最终摄像头状态\n  1322\t        if hasattr(self, 'cap') and self.cap is not None and self.cap.isOpened():\n  1323\t            width = self.cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n  1324\t            height = self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n  1325\t            fps = self.cap.get(cv2.CAP_PROP_FPS)\n  1326\t            backend = self.cap.getBackendName()\n  1327\t            print(f\&quot;摄像头状态: 已打开\&quot;)\n  1328\t            print(f\&quot;  分辨率: {width}x{height}\&quot;)\n  1329\t            print(f\&quot;  设置FPS: {fps}\&quot;)\n  1330\t            print(f\&quot;  后端: {backend}\&quot;)\n  1331\t\n  1332\t            # 如果FPS为0，尝试手动设置\n  1333\t            if fps == 0:\n  1334\t                print(\&quot;检测到FPS为0，尝试手动设置...\&quot;)\n  1335\t                self.cap.set(cv2.CAP_PROP_FPS, 30)\n  1336\t                new_fps = self.cap.get(cv2.CAP_PROP_FPS)\n  1337\t                print(f\&quot;  重新设置后FPS: {new_fps}\&quot;)\n  1338\t        else:\n  1339\t            print(\&quot;未找到可用的视频源\&quot;)\n  1340\t\n  1341\t        # 初始化FPS计算变量\n  1342\t        self.frame_count = 0\n  1343\t        self.fps_start_time = time.time()\n  1344\t        self.actual_fps = 0\n  1345\t\n  1346\t        # 初始化时间变量\n  1347\t        t_time = datetime.datetime.now()\n  1348\t        e_time = datetime.datetime.now()\n  1349\t        h_time = datetime.datetime.now()\n  1350\t\n  1351\t        # 成功打开视频，循环读取视频流\n  1352\t        print(\&quot;开始视频流处理...\&quot;)\n  1353\t\n  1354\t        # 初始化性能监控变量\n  1355\t        frame_count = 0\n  1356\t        error_count = 0\n  1357\t        last_status_time = time.time()\n  1358\t\n  1359\t        while self.is_running:\n  1360\t            try:\n  1361\t                # 在循环开始时立即检查停止标志\n  1362\t                if not self.is_running:\n  1363\t                    break\n  1364\t\n  1365\t                start_time = datetime.datetime.now()\n  1366\t                res = ['-' for _ in range(9)]\n  1367\t\n  1368\t                # 初始化CNN检测结果变量，确保在整个循环中都有定义\n  1369\t                cnn_result = None\n  1370\t\n  1371\t                # 简单的摄像头状态检查\n  1372\t                if not hasattr(self, 'cap') or self.cap is None:\n  1373\t                    print(\&quot;摄像头未初始化\&quot;)\n  1374\t                    break\n  1375\t\n  1376\t                # 简化的摄像头检查 - 只在必要时检查\n  1377\t                if not self.cap.isOpened():\n  1378\t                    print(\&quot;摄像头连接丢失，尝试重新连接...\&quot;)\n  1379\t                    if not self._reconnect_camera():\n  1380\t                        print(\&quot;摄像头重连失败，停止检测\&quot;)\n  1381\t                        break\n  1382\t                    continue\n...\n  1478\t\n  1479\t                # 使用跳帧策略：只在特定帧进行人脸检测，其他帧使用缓存结果\n  1480\t                if not skip_detection:\n  1481\t                    try:\n  1482\t                        # 检查检测器是否已初始化\n  1483\t                        if not hasattr(self, 'detector') or self.detector is None:\n  1484\t                            if frame_count % 60 == 0:  # 减少错误信息频率\n  1485\t                                print(\&quot;❌ 人脸检测器未初始化\&quot;)\n  1486\t                            faces = []\n  1487\t                        else:\n  1488\t                            # 多策略人脸检测，提高检测成功率\n  1489\t                            faces = []\n  1490\t\n  1491\t                            # 策略1: 使用增强后的图像，不同上采样级别\n  1492\t                            for upsample in [0, 1]:  # 先尝试0（更快），再尝试1\n  1493\t                                if len(faces) == 0:\n  1494\t                                    try:\n  1495\t                                        faces = self.detector(enhanced_gray, upsample)\n  1496\t                                        if len(faces) &gt; 0:\n  1497\t                                            break\n  1498\t                                    except:\n  1499\t                                        continue\n  1500\t\n  1501\t                            # 策略2: 如果增强图像失败，尝试原始灰度图像\n  1502\t                            if len(faces) == 0:\n  1503\t                                try:\n  1504\t                                    faces = self.detector(img_gray, 0)\n  1505\t                                except:\n  1506\t                                    pass\n  1507\t\n  1508\t                            # 策略3: 尝试直方图均衡化\n  1509\t                            if len(faces) == 0:\n  1510\t                                try:\n  1511\t                                    equalized = cv2.equalizeHist(img_gray)\n  1512\t                                    faces = self.detector(equalized, 0)\n  1513\t                                except:\n  1514\t                                    pass\n...\n  1553\t\n  1554\t                    except Exception as e:\n  1555\t                        if frame_count % 60 == 0:  # 减少错误信息频率\n  1556\t                            print(f\&quot;人脸检测失败: {e}\&quot;)\n  1557\t                        faces = []\n  1558\t                else:\n  1559\t                    # 跳帧时使用上次的检测结果\n  1560\t                    if hasattr(self, 'last_faces'):\n  1561\t                        faces = self.last_faces\n  1562\t                    else:\n  1563\t                        faces = []\n  1564\t                # 如果检测到人脸\n  1565\t                if (len(faces) != 0):\n  1566\t                    res[0] = '识别到人脸'\n  1567\t                    # enumerate方法同时返回数据对象的索引和数据，k为索引，d为faces中的对象\n  1568\t                    for _, d in enumerate(faces):\n  1569\t                        # 用红色矩形框出人脸\n  1570\t                        cv2.rectangle(im_rd, (d.left(), d.top()), (d.right(), d.bottom()), (0, 0, 255), 1)\n  1571\t                        # 使用预测器得到68点数据的坐标\n  1572\t                        shape = self.predictor(im_rd, d)\n  1573\t                        # 圆圈显示每个特征点\n  1574\t                        for i in range(68):\n  1575\t                            cv2.circle(im_rd, (shape.part(i).x, shape.part(i).y), 2, (0, 255, 0), -1, 8)\n  1576\t                        # 将脸部特征信息转换为数组array的格式\n  1577\t                        shape = face_utils.shape_to_np(shape)\n  1578\t\n  1579\t                        # 提取人脸区域用于CNN分析\n  1580\t                        face_img = im_rd[d.top():d.bottom(), d.left():d.right()]\n...\n  1636\t\n  1637\t                            if self.yawn_detector and self.yawn_detector.is_available:\n  1638\t                                try:\n  1639\t                                    # 提取特征并更新缓冲区（传递帧高度用于归一化）\n  1640\t                                    frame_height = im_rd.shape[0]\n  1641\t                                    features = self.yawn_detector.extract_features(shape, ear, mar, frame_height)\n  1642\t                                    if features is not None:\n  1643\t                                        self.yawn_detector.update_buffer(features)\n  1644\t\n  1645\t                                        # 进行CNN+LSTM预测（传递检测启用状态）\n  1646\t                                        cnn_lstm_yawn_detected, cnn_lstm_confidence = self.yawn_detector.predict_yawn(self.yawn_detection_enabled)\n...\n  2018\t\n  2019\t                # 优化的图像显示处理 - 减少卡顿\n  2020\t                try:\n  2021\t                    # 验证图像有效性\n  2022\t                    if im_rd is None or im_rd.size == 0:\n  2023\t                        if frame_count % 60 == 0:  # 减少错误信息频率\n  2024\t                            print(\&quot;图像数据无效，跳过显示\&quot;)\n  2025\t                        continue\n  2026\t\n  2027\t                    height, width = im_rd.shape[:2]\n  2028\t\n  2029\t                    # 确保图像有效\n  2030\t                    if height &gt; 0 and width &gt; 0:\n  2031\t                        # 转换颜色空间从BGR到RGB\n  2032\t                        RGBImg = cv2.cvtColor(im_rd, cv2.COLOR_BGR2RGB)\n  2033\t\n  2034\t                        # 验证转换后的图像\n  2035\t                        if RGBImg is None or RGBImg.size == 0:\n  2036\t                            if frame_count % 60 == 0:  # 减少错误信息频率\n  2037\t                                print(\&quot;RGB转换失败\&quot;)\n  2038\t                            continue\n  2039\t\n  2040\t                        # 减少调试信息的频率，避免控制台刷屏\n  2041\t                        if self.frame_count % 300 == 0:  # 每300帧打印一次（约每10秒）\n  2042\t                            print(f\&quot;发送图像数据: {width}x{height}, 人脸数: {len(faces)}\&quot;)\n  2043\t\n  2044\t                        # 发送图像数据到UI线程 - 确保视频流畅显示\n  2045\t                        # 每帧都发送图像数据以保证视频连续性\n  2046\t                        data = {'type':'img','value':RGBImg}\n  2047\t                        self.safe_emit_signal(data)\n  2048\t                    else:\n  2049\t                        if frame_count % 60 == 0:  # 减少错误信息频率\n  2050\t                            print(\&quot;图像尺寸无效，跳过显示\&quot;)\n...\nPath: Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/fatigue_gui.py\n...\n   339\t\n   340\t        faces = np.array(list(self.face_buffer))\n   341\t        landmarks = np.array(list(self.landmark_buffer))\n   342\t\n   343\t        faces_tensor = torch.from_numpy(faces).float().unsqueeze(0)\n   344\t        landmarks_tensor = torch.from_numpy(landmarks).float().unsqueeze(0)\n   345\t\n   346\t        faces_tensor = faces_tensor.permute(0, 1, 4, 2, 3)\n   347\t        landmarks_tensor = landmarks_tensor.reshape(1, SEQUENCE_LENGTH, -1)\n   348\t        faces_tensor = faces_tensor / 255.0\n   349\t\n   350\t        faces_tensor = faces_tensor.to(self.device)\n   351\t        landmarks_tensor = landmarks_tensor.to(self.device)\n   352\t\n   353\t        with torch.no_grad():\n   354\t            outputs = self.model(faces_tensor, landmarks_tensor)\n   355\t            probabilities = torch.softmax(outputs, dim=1)\n   356\t            yawn_prob = probabilities[0, 1].item()\n   357\t            prediction = 1 if yawn_prob &gt; self.yawn_threshold.get() else 0\n   358\t\n   359\t        return yawn_prob, prediction\n...\nPath: Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/data_preprocessing.py\n...\n    71\t        \n    72\t        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    73\t        fps = cap.get(cv2.CAP_PROP_FPS)\n    74\t        \n    75\t        self.logger.info(f\&quot;处理视频: {video_path}, 总帧数: {total_frames}, FPS: {fps}\&quot;)\n    76\t        \n    77\t        # 提取所有帧的人脸和特征点\n    78\t        faces = []\n    79\t        landmarks = []\n    80\t        frame_indices = []\n    81\t        \n    82\t        frame_idx = 0\n    83\t\n    84\t        # 调试模式下限制处理的帧数\n    85\t        if DEBUG_MODE:\n    86\t            total_frames = min(total_frames, DEBUG_MAX_FRAMES)\n    87\t            self.logger.info(f\&quot;调试模式：限制处理帧数为 {total_frames}\&quot;)\n    88\t\n    89\t        pbar = tqdm(total=total_frames, desc=f\&quot;提取特征\&quot;)\n    90\t\n    91\t        while True:\n    92\t            ret, frame = cap.read()\n    93\t            if not ret:\n    94\t                break\n    95\t\n    96\t            # 调试模式下的帧数限制\n    97\t            if DEBUG_MODE and frame_idx &gt;= DEBUG_MAX_FRAMES:\n    98\t                break\n    99\t            \n   100\t            # 提取人脸和特征点\n   101\t            face_img, face_landmarks = extract_face_landmarks(frame, self.detector, self.predictor)\n...\nPath: Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/simple_cnn_detector.py\n...\n   125\t        \n   126\t        try:\n   127\t            # 预测\n   128\t            predictions = self.model.predict(processed_image, verbose=0)\n   129\t            predicted_class_idx = np.argmax(predictions[0])\n   130\t            confidence = float(predictions[0][predicted_class_idx])\n   131\t            \n   132\t            # 获取类别名称\n   133\t            class_name = None\n   134\t            for name, idx in self.class_indices.items():\n   135\t                if idx == predicted_class_idx:\n   136\t                    class_name = name\n   137\t                    break\n   138\t            \n   139\t            if class_name is None:\n   140\t                class_name = f\&quot;class_{predicted_class_idx}\&quot;\n   141\t            \n   142\t            # 添加到历史记录\n   143\t            self.prediction_history.append({\n   144\t                'class': class_name,\n   145\t                'confidence': confidence\n   146\t            })\n   147\t            \n   148\t            # 保持历史记录长度\n   149\t            if len(self.prediction_history) &gt; self.history_length:\n   150\t                self.prediction_history.pop(0)\n   151\t            \n   152\t            # 计算平滑后的结果\n   153\t            smoothed_result = self._smooth_predictions()\n   154\t            \n   155\t            result = {\n   156\t                'predicted_class': class_name,\n   157\t                'confidence': confidence,\n   158\t                'smoothed_class': smoothed_result['class'],\n   159\t                'smoothed_confidence': smoothed_result['confidence'],\n   160\t                'fatigue_level': self._map_to_fatigue_level(smoothed_result['class']),\n   161\t                'all_predictions': predictions[0].tolist()\n   162\t            }\n   163\t            \n   164\t            return result\n   165\t            \n   166\t        except Exception as e:\n   167\t            print(f\&quot;疲劳预测失败: {e}\&quot;)\n   168\t            return None\n...\n   217\t    \n   218\t    def get_fatigue_analysis(self, face_image: np.ndarray) -&gt; Optional[Dict]:\n   219\t        \&quot;\&quot;\&quot;\n   220\t        获取疲劳分析结果\n   221\t        \n   222\t        Args:\n   223\t            face_image: 人脸图像\n   224\t            \n   225\t        Returns:\n   226\t            疲劳分析结果\n   227\t        \&quot;\&quot;\&quot;\n   228\t        prediction_result = self.predict_fatigue(face_image)\n   229\t        \n   230\t        if prediction_result is None:\n   231\t            return None\n   232\t        \n   233\t        # 构建分析结果\n   234\t        analysis = {\n   235\t            'fatigue_detected': prediction_result['smoothed_class'] != 'alert',\n   236\t            'fatigue_level': prediction_result['fatigue_level'],\n   237\t            'confidence': prediction_result['smoothed_confidence'],\n   238\t            'raw_prediction': prediction_result['predicted_class'],\n   239\t            'recommendation': self._generate_recommendation(prediction_result['fatigue_level'])\n   240\t        }\n   241\t        \n   242\t        return analysis\n...\n&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_01Wg2YwP3S84X44Hj1MVjcVc&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;D:/FatigueDetection&quot;,&quot;folderRoot&quot;:&quot;D:/FatigueDetection&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-04T07:16:06.379Z&quot;,&quot;response_text&quot;:&quot;现在让我查看主循环中的具体处理流程：\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;1fde2629-4000-4e12-bf9e-fa0c857db00f&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_01CMsNEpUWJsn7KEHpLquH5J&quot;,&quot;tool_name&quot;:&quot;view&quot;,&quot;input_json&quot;:&quot;{\&quot;path\&quot;: \&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot;, \&quot;type\&quot;: \&quot;file\&quot;, \&quot;view_range\&quot;: [1565, 1650]}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;让我继续查看人脸检测和特征提取的部分：\n&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;Here's the result of running `cat -n` on Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py:\n  1383\t\n  1384\t                # 优化的帧读取 - 减少卡顿\n  1385\t                flag, im_rd = self.cap.read()\n  1386\t\n  1387\t                if not flag or im_rd is None or im_rd.size == 0:\n  1388\t                    error_count += 1\n  1389\t                    # 减少错误报告频率\n  1390\t                    if error_count % 30 == 0:\n  1391\t                        print(f\&quot;读取帧失败，错误计数: {error_count}\&quot;)\n  1392\t\n  1393\t                    # 如果连续错误太多，尝试重连\n  1394\t                    if error_count &gt; 100:\n  1395\t                        print(\&quot;错误过多，尝试重连摄像头...\&quot;)\n  1396\t                        if not self._reconnect_camera():\n  1397\t                            break\n  1398\t                        error_count = 0\n  1399\t\n  1400\t                    # 使用配置文件中的延迟时间\n  1401\t                    time.sleep(camera_config.MAIN_LOOP_DELAY)\n  1402\t                    continue\n  1403\t\n  1404\t                # 成功读取帧，重置错误计数\n  1405\t                if error_count &gt; 0:\n  1406\t                    error_count = 0\n  1407\t\n  1408\t                frame_count += 1\n  1409\t\n  1410\t                # 跳帧处理 - 使用配置文件参数\n  1411\t                skip_detection = (frame_count % camera_config.FRAME_SKIP_DETECTION != 0)\n  1412\t\n  1413\t                # 每10秒报告一次状态，减少输出频率\n  1414\t                current_time_float = time.time()\n  1415\t                if current_time_float - last_status_time &gt;= 10.0:\n  1416\t                    fps = frame_count / (current_time_float - last_status_time)\n  1417\t                    print(f\&quot;处理状态: {frame_count} 帧, FPS: {fps:.1f}, 错误: {error_count}\&quot;)\n  1418\t                    frame_count = 0\n  1419\t                    last_status_time = current_time_float\n  1420\t\n  1421\t                # 改进的图像处理 - 增强人脸检测\n  1422\t                try:\n  1423\t                    # 验证图像格式和尺寸\n  1424\t                    if len(im_rd.shape) != 3 or im_rd.shape[2] != 3:\n  1425\t                        print(f\&quot;图像格式异常: {im_rd.shape}\&quot;)\n  1426\t                        continue\n  1427\t\n  1428\t                    height, width = im_rd.shape[:2]\n  1429\t                    if height &lt; 100 or width &lt; 100:\n  1430\t                        print(f\&quot;图像尺寸过小: {width}x{height}\&quot;)\n  1431\t                        continue\n  1432\t\n  1433\t                    # 应用暗图像增强 - 解决摄像头太暗的问题\n  1434\t                    im_rd = self._enhance_dark_frame(im_rd)\n  1435\t\n  1436\t                    # 转换为灰度图像\n  1437\t                    img_gray = cv2.cvtColor(im_rd, cv2.COLOR_BGR2GRAY)\n  1438\t\n  1439\t                    # 验证灰度图像\n  1440\t                    if img_gray is None or img_gray.size == 0:\n  1441\t                        print(\&quot;灰度转换失败\&quot;)\n  1442\t                        continue\n  1443\t\n  1444\t                    # 增强图像质量以提高人脸检测率\n  1445\t                    mean_brightness = np.mean(img_gray)\n  1446\t\n  1447\t                    # 创建增强版本的图像\n  1448\t                    enhanced_gray = img_gray.copy()\n  1449\t\n  1450\t                    # 增强的亮度调整 - 针对暗环境优化\nTotal lines in file: 2432\n&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_01FU8ztzTwKkejJrorkDB4fK&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;D:/FatigueDetection&quot;,&quot;folderRoot&quot;:&quot;D:/FatigueDetection&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-04T07:17:08.405Z&quot;,&quot;response_text&quot;:&quot;让我继续查看人脸检测和特征提取的部分：\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;c1ec9173-4140-4043-805a-85890e042c64&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_01D44TKNjazPjYXfoKMZiCwU&quot;,&quot;tool_name&quot;:&quot;view&quot;,&quot;input_json&quot;:&quot;{\&quot;path\&quot;: \&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot;, \&quot;type\&quot;: \&quot;file\&quot;, \&quot;view_range\&quot;: [187, 221]}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;现在让我查看模型预测的具体实现：\n&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;Here's the result of running `cat -n` on Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py:\n  1565\t                if (len(faces) != 0):\n  1566\t                    res[0] = '识别到人脸'\n  1567\t                    # enumerate方法同时返回数据对象的索引和数据，k为索引，d为faces中的对象\n  1568\t                    for _, d in enumerate(faces):\n  1569\t                        # 用红色矩形框出人脸\n  1570\t                        cv2.rectangle(im_rd, (d.left(), d.top()), (d.right(), d.bottom()), (0, 0, 255), 1)\n  1571\t                        # 使用预测器得到68点数据的坐标\n  1572\t                        shape = self.predictor(im_rd, d)\n  1573\t                        # 圆圈显示每个特征点\n  1574\t                        for i in range(68):\n  1575\t                            cv2.circle(im_rd, (shape.part(i).x, shape.part(i).y), 2, (0, 255, 0), -1, 8)\n  1576\t                        # 将脸部特征信息转换为数组array的格式\n  1577\t                        shape = face_utils.shape_to_np(shape)\n  1578\t\n  1579\t                        # 提取人脸区域用于CNN分析\n  1580\t                        face_img = im_rd[d.top():d.bottom(), d.left():d.right()]\n  1581\t\n  1582\t                        # 使用CNN进行疲劳检测 - 降低调用频率以提高性能\n  1583\t                        # 只在每10帧调用一次CNN检测，减少计算负担\n  1584\t                        if (self.cnn_detector and self.cnn_detector.is_available() and\n  1585\t                            frame_count % 10 == 0):\n  1586\t                            try:\n  1587\t                                cnn_result = self.cnn_detector.predict_fatigue(face_img)\n  1588\t                                if cnn_result:\n  1589\t                                    # 在界面上显示CNN检测结果\n  1590\t                                    cnn_text = f\&quot;CNN: {cnn_result['fatigue_level']} ({cnn_result['confidence']:.2f})\&quot;\n  1591\t                                    cv2.putText(im_rd, cnn_text, (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n  1592\t                            except Exception as e:\n  1593\t                                if frame_count % 60 == 0:  # 减少错误信息频率\n  1594\t                                    print(f\&quot;CNN检测失败: {e}\&quot;)\n  1595\t                                cnn_result = None  # 确保在异常情况下重置为None\n  1596\t\n  1597\t                        # 预先计算EAR和MAR，因为CNN+LSTM模型需要这些值\n  1598\t                        # 提取左眼和右眼坐标\n  1599\t                        leftEye = shape[lStart:lEnd]\n  1600\t                        rightEye = shape[rStart:rEnd]\n  1601\t                        # 构造函数计算左右眼的EAR值，使用平均值作为最终的EAR\n  1602\t                        leftEAR = self.eye_aspect_ratio(leftEye)\n  1603\t                        rightEAR = self.eye_aspect_ratio(rightEye)\n  1604\t                        ear = (leftEAR + rightEAR) / 2.0\n  1605\t\n  1606\t                        # 嘴巴坐标和MAR计算\n  1607\t                        mouth = shape[mStart:mEnd]\n  1608\t                        mar = self.mouth_aspect_ratio(mouth)\n  1609\t\n  1610\t                        # 获取当前时间（确保在所有地方都可用）\n  1611\t                        current_time = datetime.datetime.now()\n  1612\t\n  1613\t                        \&quot;\&quot;\&quot;\n  1614\t                        打哈欠 - 集成CNN+LSTM和启发式检测（带冷却机制）\n  1615\t                        \&quot;\&quot;\&quot;\n  1616\t                        if self.fun[1]:\n  1617\t                            # 使用cv2.convexHull获得凸包位置，使用drawContours画出轮廓位置进行画图操作\n  1618\t                            mouthHull = cv2.convexHull(mouth)\n  1619\t                            cv2.drawContours(im_rd, [mouthHull], -1, (0, 255, 0), 1)\n  1620\t\n  1621\t                            # 检查打哈欠冷却状态\n  1622\t                            if self.last_yawn_time is not None:\n  1623\t                                time_since_last_yawn = (current_time - self.last_yawn_time).total_seconds()\n  1624\t                                if time_since_last_yawn &lt; self.yawn_cooldown_seconds:\n  1625\t                                    self.yawn_detection_enabled = False\n  1626\t                                    # 显示冷却状态\n  1627\t                                    cooldown_remaining = self.yawn_cooldown_seconds - time_since_last_yawn\n  1628\t                                    cv2.putText(im_rd, f\&quot;Yawn Cooldown: {cooldown_remaining:.1f}s\&quot;, (10, 160),\n  1629\t                                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n  1630\t                                else:\n  1631\t                                    self.yawn_detection_enabled = True\n  1632\t\n  1633\t                            # CNN+LSTM打哈欠检测（只在启用时进行）\n  1634\t                            cnn_lstm_yawn_detected = False\n  1635\t                            cnn_lstm_confidence = 0.0\n  1636\t\n  1637\t                            if self.yawn_detector and self.yawn_detector.is_available:\n  1638\t                                try:\n  1639\t                                    # 提取特征并更新缓冲区（传递帧高度用于归一化）\n  1640\t                                    frame_height = im_rd.shape[0]\n  1641\t                                    features = self.yawn_detector.extract_features(shape, ear, mar, frame_height)\n  1642\t                                    if features is not None:\n  1643\t                                        self.yawn_detector.update_buffer(features)\n  1644\t\n  1645\t                                        # 进行CNN+LSTM预测（传递检测启用状态）\n  1646\t                                        cnn_lstm_yawn_detected, cnn_lstm_confidence = self.yawn_detector.predict_yawn(self.yawn_detection_enabled)\n  1647\t\n  1648\t                                        # 在界面上显示CNN+LSTM检测结果\n  1649\t                                        if frame_count % 10 == 0:  # 每10帧显示一次\n  1650\t                                            status = \&quot;ENABLED\&quot; if self.yawn_detection_enabled else \&quot;COOLDOWN\&quot;\nTotal lines in file: 2432\n&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_01CMsNEpUWJsn7KEHpLquH5J&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;D:/FatigueDetection&quot;,&quot;folderRoot&quot;:&quot;D:/FatigueDetection&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-04T07:17:14.557Z&quot;,&quot;response_text&quot;:&quot;现在让我查看模型预测的具体实现：\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;84368d6b-4290-43fe-86cc-0205ebd68423&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;基于我对代码的详细分析，我来为您展示这个程序从摄像头获取视频信息到模型预测的**完整流程**：\n\n##  视频流处理与模型预测完整流程\n\n### 1. 系统初始化阶段\n\n#### 1.1 模型初始化\n&lt;augment_code_snippet path=\&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot; mode=\&quot;EXCERPT\&quot;&gt;\n```python\n# 初始化CNN+LSTM打哈欠检测器\nif PYTORCH_AVAILABLE:\n    try:\n        self.init_yawn_detector()\n    except Exception as e:\n        print(f\&quot;CNN+LSTM打哈欠检测器初始化失败: {e}\&quot;)\n```\n&lt;/augment_code_snippet&gt;\n\n#### 1.2 摄像头初始化\n&lt;augment_code_snippet path=\&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot; mode=\&quot;EXCERPT\&quot;&gt;\n```python\n# 摄像头初始化\nself.cap = cv2.VideoCapture(camera_index, cv2.CAP_DSHOW)\nself.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\nself.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\nself.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n```\n&lt;/augment_code_snippet&gt;\n\n#### 1.3 人脸检测器初始化\n&lt;augment_code_snippet path=\&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot; mode=\&quot;EXCERPT\&quot;&gt;\n```python\n# dlib人脸检测器和特征点预测器\nself.detector = dlib.get_frontal_face_detector()\nself.predictor = dlib.shape_predictor(\&quot;./model/shape_predictor_68_face_landmarks.dat\&quot;)\n```\n&lt;/augment_code_snippet&gt;\n\n### 2. 主循环处理流程\n\n#### 2.1 帧读取与预处理\n&lt;augment_code_snippet path=\&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot; mode=\&quot;EXCERPT\&quot;&gt;\n```python\nwhile self.is_running:\n    # 读取摄像头帧\n    flag, im_rd = self.cap.read()\n    \n    # 图像增强处理（针对暗环境）\n    im_rd = self._enhance_dark_frame(im_rd)\n    \n    # 转换为灰度图像\n    img_gray = cv2.cvtColor(im_rd, cv2.COLOR_BGR2GRAY)\n    \n    # 图像质量增强\n    enhanced_gray = self._enhance_image_quality(img_gray)\n```\n&lt;/augment_code_snippet&gt;\n\n#### 2.2 人脸检测\n&lt;augment_code_snippet path=\&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot; mode=\&quot;EXCERPT\&quot;&gt;\n```python\n# 多策略人脸检测\nfaces = []\n# 策略1: 使用增强图像\nfor upsample in [0, 1]:\n    if len(faces) == 0:\n        faces = self.detector(enhanced_gray, upsample)\n        if len(faces) &gt; 0:\n            break\n\n# 策略2: 使用原始灰度图像\nif len(faces) == 0:\n    faces = self.detector(img_gray, 0)\n```\n&lt;/augment_code_snippet&gt;\n\n#### 2.3 面部特征点提取\n&lt;augment_code_snippet path=\&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot; mode=\&quot;EXCERPT\&quot;&gt;\n```python\nif len(faces) != 0:\n    for _, d in enumerate(faces):\n        # 提取68个面部特征点\n        shape = self.predictor(im_rd, d)\n        shape = face_utils.shape_to_np(shape)\n        \n        # 提取眼部和嘴部区域\n        leftEye = shape[lStart:lEnd]\n        rightEye = shape[rStart:rEnd]\n        mouth = shape[mStart:mEnd]\n```\n&lt;/augment_code_snippet&gt;\n\n### 3. 特征计算与提取\n\n#### 3.1 生理指标计算\n&lt;augment_code_snippet path=\&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot; mode=\&quot;EXCERPT\&quot;&gt;\n```python\n# 计算眼部长宽比(EAR)\nleftEAR = self.eye_aspect_ratio(leftEye)\nrightEAR = self.eye_aspect_ratio(rightEye)\near = (leftEAR + rightEAR) / 2.0\n\n# 计算嘴部长宽比(MAR)\nmar = self.mouth_aspect_ratio(mouth)\n```\n&lt;/augment_code_snippet&gt;\n\n#### 3.2 模型特征提取\n&lt;augment_code_snippet path=\&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot; mode=\&quot;EXCERPT\&quot;&gt;\n```python\n# 提取CNN+LSTM模型所需的特征\nframe_height = im_rd.shape[0]\nfeatures = self.yawn_detector.extract_features(shape, ear, mar, frame_height)\n\ndef extract_features(self, landmarks, ear, mar, frame_height=480):\n    # 归一化关键点坐标（以鼻尖为基准）\n    nose = landmarks[30]  # 鼻尖关键点\n    normalized_landmarks = (landmarks - nose).flatten() / frame_height\n    \n    # 组合特征：EAR, MAR + 68个关键点的x,y坐标 = 138维特征\n    features = np.concatenate([[ear, mar], normalized_landmarks])\n    return features\n```\n&lt;/augment_code_snippet&gt;\n\n### 4. 时序缓冲区管理\n\n#### 4.1 特征缓冲区更新\n&lt;augment_code_snippet path=\&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot; mode=\&quot;EXCERPT\&quot;&gt;\n```python\n# 更新特征缓冲区（维持30帧的时序窗口）\nif features is not None:\n    self.yawn_detector.update_buffer(features)\n\ndef update_buffer(self, features):\n    if features is not None:\n        self.features_buffer.append(features)\n        if len(self.features_buffer) &gt; self.seq_length:  # seq_length=30\n            self.features_buffer.pop(0)\n```\n&lt;/augment_code_snippet&gt;\n\n### 5. 模型预测阶段\n\n#### 5.1 数据准备与预测\n&lt;augment_code_snippet path=\&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot; mode=\&quot;EXCERPT\&quot;&gt;\n```python\n# CNN+LSTM模型预测\ncnn_lstm_yawn_detected, cnn_lstm_confidence = self.yawn_detector.predict_yawn(self.yawn_detection_enabled)\n\ndef predict_yawn(self, detection_enabled=True):\n    if len(self.features_buffer) &lt; self.seq_length:  # 需要30帧数据\n        return False, 0.0\n    \n    # 准备输入序列 (1, 30, 138)\n    input_seq = np.array([self.features_buffer])\n    input_tensor = torch.FloatTensor(input_seq).to(self.device)\n    \n    # 模型前向推理\n    with torch.no_grad():\n        logits = self.model(input_tensor).item()\n        prediction = torch.sigmoid(torch.tensor(logits)).item()\n```\n&lt;/augment_code_snippet&gt;\n\n#### 5.2 模型内部处理流程\n&lt;augment_code_snippet path=\&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot; mode=\&quot;EXCERPT\&quot;&gt;\n```python\ndef forward(self, x):  # x: (batch_size, seq_len=30, features=138)\n    # 1. CNN特征提取\n    x = x.permute(0, 2, 1)  # (batch_size, features=138, seq_len=30)\n    cnn_out = self.cnn(x)   # (batch_size, 256, seq_len/8)\n    \n    # 2. LSTM时序处理\n    cnn_out = cnn_out.permute(0, 2, 1)  # (batch_size, seq_len/8, 256)\n    lstm_out, _ = self.lstm(cnn_out)    # (batch_size, seq_len/8, 128)\n    \n    # 3. 取最后时间步输出\n    lstm_out = lstm_out[:, -1, :]       # (batch_size, 128)\n    \n    # 4. 全连接分类\n    output = self.fc(lstm_out)          # (batch_size, 1)\n    return output\n```\n&lt;/augment_code_snippet&gt;\n\n### 6. 连续帧判断与决策\n\n#### 6.1 连续帧逻辑\n&lt;augment_code_snippet path=\&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot; mode=\&quot;EXCERPT\&quot;&gt;\n```python\n# 连续帧判断逻辑\nself.frame_count += 1\nif prediction &gt;= 0.5:  # 单帧预测阈值\n    self.fatigue_frames += 1\nelse:\n    self.fatigue_frames = 0\n\n# 需要连续15帧都预测为疲劳才判定为打哈欠\nis_fatigued = self.fatigue_frames &gt;= self.consecutive_frames  # consecutive_frames=15\n```\n&lt;/augment_code_snippet&gt;\n\n### 7. 结果输出与冷却机制\n\n#### 7.1 检测结果处理\n&lt;augment_code_snippet path=\&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot; mode=\&quot;EXCERPT\&quot;&gt;\n```python\n# 打哈欠检测结果处理\nif cnn_lstm_yawn_detected:\n    yawn_detected = True\n    detection_method = \&quot;CNN+LSTM\&quot;\n    self.mTOTAL += 1  # 哈欠计数\n    \n    # 设置冷却时间（3秒内不再检测）\n    self.last_yawn_time = current_time\n    self.yawn_detection_enabled = False\n    \n    # 发送警报信号\n    self.safe_emit_signal({'type':'msg','value': f\&quot;打哈欠({detection_method})\&quot;})\n```\n&lt;/augment_code_snippet&gt;\n\n### 8. 可视化显示\n\n#### 8.1 实时显示\n&lt;augment_code_snippet path=\&quot;Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\&quot; mode=\&quot;EXCERPT\&quot;&gt;\n```python\n# 在视频帧上显示检测结果\ncv2.putText(im_rd, f\&quot;CNN+LSTM: {cnn_lstm_confidence:.2f}\&quot;, (10, 140), \n           cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n\n# 转换并发送到UI显示\nRGBImg = cv2.cvtColor(im_rd, cv2.COLOR_BGR2RGB)\ndata = {'type':'img','value':RGBImg}\nself.safe_emit_signal(data)\n```\n&lt;/augment_code_snippet&gt;\n\n##  完整流程图\n\n```mermaid\ngraph TD\n    A[摄像头初始化] --&gt; B[读取视频帧]\n    B --&gt; C[图像预处理与增强]\n    C --&gt; D[人脸检测]\n    D --&gt; E{检测到人脸?}\n    E --&gt;|否| B\n    E --&gt;|是| F[提取68个面部特征点]\n    F --&gt; G[计算EAR和MAR]\n    G --&gt; H[提取138维特征向量]\n    H --&gt; I[更新30帧时序缓冲区]\n    I --&gt; J{缓冲区满?}\n    J --&gt;|否| B\n    J --&gt;|是| K[CNN+LSTM模型预测]\n    K --&gt; L[Sigmoid激活得到概率]\n    L --&gt; M{概率≥0.5?}\n    M --&gt;|是| N[疲劳帧计数+1]\n    M --&gt;|否| O[重置疲劳帧计数]\n    N --&gt; P{连续疲劳帧≥15?}\n    O --&gt; B\n    P --&gt;|否| B\n    P --&gt;|是| Q{检测启用?}\n    Q --&gt;|否| B\n    Q --&gt;|是| R[触发打哈欠警报]\n    R --&gt; S[设置3秒冷却期]\n    S --&gt; T[显示检测结果]\n    T --&gt; B\n```\n\n##  关键技术要点\n\n1. **多模态特征融合**：EAR + MAR + 68个面部关键点坐标\n2. **时序建模**：30帧滑动窗口捕获打哈欠的时序特性\n3. **CNN+LSTM架构**：CNN提取空间特征，LSTM处理时序依赖\n4. **连续帧判断**：需要连续15帧预测为疲劳才触发警报\n5. **冷却机制**：3秒冷却期避免重复检测\n6. **实时性优化**：跳帧处理、异步显示等提高性能\n\n这个完整流程确保了从原始视频流到最终疲劳检测结果的准确性和实时性。\n&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;unseen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;Here's the result of running `cat -n` on Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py:\n   187\t        def predict_yawn(self, detection_enabled=True):\n   188\t            \&quot;\&quot;\&quot;预测是否打哈欠\&quot;\&quot;\&quot;\n   189\t            if not self.is_available or len(self.features_buffer) &lt; self.seq_length:\n   190\t                return False, 0.0\n   191\t\n   192\t            try:\n   193\t                # 准备输入序列\n   194\t                input_seq = np.array([self.features_buffer])\n   195\t                input_tensor = torch.FloatTensor(input_seq).to(self.device)\n   196\t\n   197\t                # 模型预测\n   198\t                with torch.no_grad():\n   199\t                    logits = self.model(input_tensor).item()\n   200\t                    prediction = torch.sigmoid(torch.tensor(logits)).item()\n   201\t\n   202\t                # 更新疲劳状态（参考real_pljc的连续帧判断逻辑）\n   203\t                self.frame_count += 1\n   204\t                if prediction &gt;= 0.5:  # 单帧预测阈值\n   205\t                    self.fatigue_frames += 1\n   206\t                else:\n   207\t                    self.fatigue_frames = 0\n   208\t\n   209\t                # 判定疲劳需要连续帧数达到阈值\n   210\t                is_fatigued = self.fatigue_frames &gt;= self.consecutive_frames\n   211\t\n   212\t                # 如果检测被禁用（冷却期），重置连续帧计数\n   213\t                if not detection_enabled and is_fatigued:\n   214\t                    self.fatigue_frames = 0\n   215\t                    is_fatigued = False\n   216\t\n   217\t                return is_fatigued, prediction\n   218\t\n   219\t            except Exception as e:\n   220\t                print(f\&quot;CNN+LSTM打哈欠预测失败: {e}\&quot;)\n   221\t                return False, 0.0\nTotal lines in file: 2432\n&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_01D44TKNjazPjYXfoKMZiCwU&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;D:/FatigueDetection&quot;,&quot;folderRoot&quot;:&quot;D:/FatigueDetection&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-04T07:17:21.886Z&quot;,&quot;response_text&quot;:&quot;基于我对代码的详细分析，我来为您展示这个程序从摄像头获取视频信息到模型预测的**完整流程**：\n\n##  视频流处理与模型预测完整流程\n\n### 1. 系统初始化阶段\n\n#### 1.1 模型初始化\n````python path=Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py mode=EXCERPT\n# 初始化CNN+LSTM打哈欠检测器\nif PYTORCH_AVAILABLE:\n    try:\n        self.init_yawn_detector()\n    except Exception as e:\n        print(f\&quot;CNN+LSTM打哈欠检测器初始化失败: {e}\&quot;)\n````\n\n#### 1.2 摄像头初始化\n````python path=Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py mode=EXCERPT\n# 摄像头初始化\nself.cap = cv2.VideoCapture(camera_index, cv2.CAP_DSHOW)\nself.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\nself.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\nself.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n````\n\n#### 1.3 人脸检测器初始化\n````python path=Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py mode=EXCERPT\n# dlib人脸检测器和特征点预测器\nself.detector = dlib.get_frontal_face_detector()\nself.predictor = dlib.shape_predictor(\&quot;./model/shape_predictor_68_face_landmarks.dat\&quot;)\n````\n\n### 2. 主循环处理流程\n\n#### 2.1 帧读取与预处理\n````python path=Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py mode=EXCERPT\nwhile self.is_running:\n    # 读取摄像头帧\n    flag, im_rd = self.cap.read()\n    \n    # 图像增强处理（针对暗环境）\n    im_rd = self._enhance_dark_frame(im_rd)\n    \n    # 转换为灰度图像\n    img_gray = cv2.cvtColor(im_rd, cv2.COLOR_BGR2GRAY)\n    \n    # 图像质量增强\n    enhanced_gray = self._enhance_image_quality(img_gray)\n````\n\n#### 2.2 人脸检测\n````python path=Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py mode=EXCERPT\n# 多策略人脸检测\nfaces = []\n# 策略1: 使用增强图像\nfor upsample in [0, 1]:\n    if len(faces) == 0:\n        faces = self.detector(enhanced_gray, upsample)\n        if len(faces) &gt; 0:\n            break\n\n# 策略2: 使用原始灰度图像\nif len(faces) == 0:\n    faces = self.detector(img_gray, 0)\n````\n\n#### 2.3 面部特征点提取\n````python path=Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py mode=EXCERPT\nif len(faces) != 0:\n    for _, d in enumerate(faces):\n        # 提取68个面部特征点\n        shape = self.predictor(im_rd, d)\n        shape = face_utils.shape_to_np(shape)\n        \n        # 提取眼部和嘴部区域\n        leftEye = shape[lStart:lEnd]\n        rightEye = shape[rStart:rEnd]\n        mouth = shape[mStart:mEnd]\n````\n\n### 3. 特征计算与提取\n\n#### 3.1 生理指标计算\n````python path=Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py mode=EXCERPT\n# 计算眼部长宽比(EAR)\nleftEAR = self.eye_aspect_ratio(leftEye)\nrightEAR = self.eye_aspect_ratio(rightEye)\near = (leftEAR + rightEAR) / 2.0\n\n# 计算嘴部长宽比(MAR)\nmar = self.mouth_aspect_ratio(mouth)\n````\n\n#### 3.2 模型特征提取\n````python path=Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py mode=EXCERPT\n# 提取CNN+LSTM模型所需的特征\nframe_height = im_rd.shape[0]\nfeatures = self.yawn_detector.extract_features(shape, ear, mar, frame_height)\n\ndef extract_features(self, landmarks, ear, mar, frame_height=480):\n    # 归一化关键点坐标（以鼻尖为基准）\n    nose = landmarks[30]  # 鼻尖关键点\n    normalized_landmarks = (landmarks - nose).flatten() / frame_height\n    \n    # 组合特征：EAR, MAR + 68个关键点的x,y坐标 = 138维特征\n    features = np.concatenate([[ear, mar], normalized_landmarks])\n    return features\n````\n\n### 4. 时序缓冲区管理\n\n#### 4.1 特征缓冲区更新\n````python path=Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py mode=EXCERPT\n# 更新特征缓冲区（维持30帧的时序窗口）\nif features is not None:\n    self.yawn_detector.update_buffer(features)\n\ndef update_buffer(self, features):\n    if features is not None:\n        self.features_buffer.append(features)\n        if len(self.features_buffer) &gt; self.seq_length:  # seq_length=30\n            self.features_buffer.pop(0)\n````\n\n### 5. 模型预测阶段\n\n#### 5.1 数据准备与预测\n````python path=Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py mode=EXCERPT\n# CNN+LSTM模型预测\ncnn_lstm_yawn_detected, cnn_lstm_confidence = self.yawn_detector.predict_yawn(self.yawn_detection_enabled)\n\ndef predict_yawn(self, detection_enabled=True):\n    if len(self.features_buffer) &lt; self.seq_length:  # 需要30帧数据\n        return False, 0.0\n    \n    # 准备输入序列 (1, 30, 138)\n    input_seq = np.array([self.features_buffer])\n    input_tensor = torch.FloatTensor(input_seq).to(self.device)\n    \n    # 模型前向推理\n    with torch.no_grad():\n        logits = self.model(input_tensor).item()\n        prediction = torch.sigmoid(torch.tensor(logits)).item()\n````\n\n#### 5.2 模型内部处理流程\n````python path=Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py mode=EXCERPT\ndef forward(self, x):  # x: (batch_size, seq_len=30, features=138)\n    # 1. CNN特征提取\n    x = x.permute(0, 2, 1)  # (batch_size, features=138, seq_len=30)\n    cnn_out = self.cnn(x)   # (batch_size, 256, seq_len/8)\n    \n    # 2. LSTM时序处理\n    cnn_out = cnn_out.permute(0, 2, 1)  # (batch_size, seq_len/8, 256)\n    lstm_out, _ = self.lstm(cnn_out)    # (batch_size, seq_len/8, 128)\n    \n    # 3. 取最后时间步输出\n    lstm_out = lstm_out[:, -1, :]       # (batch_size, 128)\n    \n    # 4. 全连接分类\n    output = self.fc(lstm_out)          # (batch_size, 1)\n    return output\n````\n\n### 6. 连续帧判断与决策\n\n#### 6.1 连续帧逻辑\n````python path=Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py mode=EXCERPT\n# 连续帧判断逻辑\nself.frame_count += 1\nif prediction &gt;= 0.5:  # 单帧预测阈值\n    self.fatigue_frames += 1\nelse:\n    self.fatigue_frames = 0\n\n# 需要连续15帧都预测为疲劳才判定为打哈欠\nis_fatigued = self.fatigue_frames &gt;= self.consecutive_frames  # consecutive_frames=15\n````\n\n### 7. 结果输出与冷却机制\n\n#### 7.1 检测结果处理\n````python path=Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py mode=EXCERPT\n# 打哈欠检测结果处理\nif cnn_lstm_yawn_detected:\n    yawn_detected = True\n    detection_method = \&quot;CNN+LSTM\&quot;\n    self.mTOTAL += 1  # 哈欠计数\n    \n    # 设置冷却时间（3秒内不再检测）\n    self.last_yawn_time = current_time\n    self.yawn_detection_enabled = False\n    \n    # 发送警报信号\n    self.safe_emit_signal({'type':'msg','value': f\&quot;打哈欠({detection_method})\&quot;})\n````\n\n### 8. 可视化显示\n\n#### 8.1 实时显示\n````python path=Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py mode=EXCERPT\n# 在视频帧上显示检测结果\ncv2.putText(im_rd, f\&quot;CNN+LSTM: {cnn_lstm_confidence:.2f}\&quot;, (10, 140), \n           cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n\n# 转换并发送到UI显示\nRGBImg = cv2.cvtColor(im_rd, cv2.COLOR_BGR2RGB)\ndata = {'type':'img','value':RGBImg}\nself.safe_emit_signal(data)\n````\n\n##  完整流程图\n\n```mermaid\ngraph TD\n    A[摄像头初始化] --&gt; B[读取视频帧]\n    B --&gt; C[图像预处理与增强]\n    C --&gt; D[人脸检测]\n    D --&gt; E{检测到人脸?}\n    E --&gt;|否| B\n    E --&gt;|是| F[提取68个面部特征点]\n    F --&gt; G[计算EAR和MAR]\n    G --&gt; H[提取138维特征向量]\n    H --&gt; I[更新30帧时序缓冲区]\n    I --&gt; J{缓冲区满?}\n    J --&gt;|否| B\n    J --&gt;|是| K[CNN+LSTM模型预测]\n    K --&gt; L[Sigmoid激活得到概率]\n    L --&gt; M{概率≥0.5?}\n    M --&gt;|是| N[疲劳帧计数+1]\n    M --&gt;|否| O[重置疲劳帧计数]\n    N --&gt; P{连续疲劳帧≥15?}\n    O --&gt; B\n    P --&gt;|否| B\n    P --&gt;|是| Q{检测启用?}\n    Q --&gt;|否| B\n    Q --&gt;|是| R[触发打哈欠警报]\n    R --&gt; S[设置3秒冷却期]\n    S --&gt; T[显示检测结果]\n    T --&gt; B\n```\n\n##  关键技术要点\n\n1. **多模态特征融合**：EAR + MAR + 68个面部关键点坐标\n2. **时序建模**：30帧滑动窗口捕获打哈欠的时序特性\n3. **CNN+LSTM架构**：CNN提取空间特征，LSTM处理时序依赖\n4. **连续帧判断**：需要连续15帧预测为疲劳才触发警报\n5. **冷却机制**：3秒冷却期避免重复检测\n6. **实时性优化**：跳帧处理、异步显示等提高性能\n\n这个完整流程确保了从原始视频流到最终疲劳检测结果的准确性和实时性。\n&quot;,&quot;workspace_file_chunks&quot;:[]}],&quot;feedbackStates&quot;:{&quot;temp-fe-d8ef1297-06f4-450a-8f0d-1589b28fbe8e&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-7ae211ae-885d-4f48-ab4a-fa87a63e308c&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-4b795fdf-3c0b-4e30-8164-93749a0a3e02&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-9d75c674-64fb-4c60-8874-b73a577b3bd2&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-b2231783-3fea-401d-b130-0d15ab56157e&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-43b1c64d-2472-4922-a08a-4df4da2c2861&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-bbd6c567-deb7-4941-80dd-17e145f264fb&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-989b9da3-2eb8-4d6a-99df-c56a30382bf0&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-9bd12ade-0020-4c86-bd3a-3caf00fd0d93&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-f20a3b74-812a-4e2c-89e0-2156bd479853&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;}},&quot;toolUseStates&quot;:{&quot;fd1bfaa4-5aaf-4ab5-89a1-8dd464f3d334;toolu_01RQTUrKLnrCT4tCagEYcUem&quot;:{&quot;requestId&quot;:&quot;fd1bfaa4-5aaf-4ab5-89a1-8dd464f3d334&quot;,&quot;toolUseId&quot;:&quot;toolu_01RQTUrKLnrCT4tCagEYcUem&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;Here's the result of running `cat -n` on Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py:\n     1\timport datetime\n     2\timport math\n     3\timport os\n     4\timport sys\n     5\timport threading\n     6\timport time\n     7\t\n     8\timport cv2\n     9\timport dlib\n    10\timport numpy as np\n    11\timport pygame\n    12\tfrom PySide6 import QtCore, QtGui, QtWidgets\n    13\tfrom PySide6.QtGui import QImage\n    14\tfrom PySide6.QtCore import Qt\n    15\tfrom imutils import face_utils\n    16\tfrom scipy.spatial import distance as dist\n    17\tfrom camera_config import camera_config\n    18\t\n    19\t# 尝试导入深度学习库，如果失败则使用占位符\n    20\ttry:\n    21\t    # 首先定义TENSORFLOW_AVAILABLE变量\n    22\t    TENSORFLOW_AVAILABLE = False\n    23\t\n    24\t    import tensorflow as tf\n    25\t    from tensorflow.keras.applications import ResNet50\n    26\t    from tensorflow.keras.models import Sequential\n    27\t    from tensorflow.keras.layers import LSTM, Dense, Conv2D, MaxPooling2D, Flatten\n    28\t\n    29\t    # 如果导入成功，设置TENSORFLOW_AVAILABLE为True\n    30\t    TENSORFLOW_AVAILABLE = True\n    31\t\n    32\t    # 只有当TensorFlow可用时才尝试导入Input层\n    33\t    if TENSORFLOW_AVAILABLE:\n    34\t        from tensorflow.keras.layers import Input\n    35\t        pass\n    36\t\n    37\texcept ImportError as e:\n    38\t    print(f\&quot;TensorFlow导入失败: {e}\&quot;)\n    39\t    TENSORFLOW_AVAILABLE = False\n    40\t\n    41\t# 尝试导入PyTorch用于CNN+LSTM打哈欠检测\n    42\ttry:\n    43\t    import torch\n    44\t    import torch.nn as nn\n    45\t    PYTORCH_AVAILABLE = True\n    46\t    print(\&quot;✅ PyTorch可用，将启用CNN+LSTM打哈欠检测\&quot;)\n    47\texcept ImportError as e:\n    48\t    print(f\&quot;PyTorch导入失败: {e}\&quot;)\n    49\t    PYTORCH_AVAILABLE = False\n    50\t\n    51\t\n    52\t# CNN+LSTM模型定义（用于打哈欠检测）\n    53\tif PYTORCH_AVAILABLE:\n    54\t    class YawnCNNLSTM(nn.Module):\n    55\t        \&quot;\&quot;\&quot;专门用于打哈欠检测的CNN+LSTM模型\&quot;\&quot;\&quot;\n    56\t        def __init__(self, input_size, hidden_size, num_layers, output_size):\n    57\t            super(YawnCNNLSTM, self).__init__()\n    58\t\n    59\t            # CNN layers - Extract spatial features\n    60\t            self.cnn = nn.Sequential(\n    61\t                nn.Conv1d(input_size, 64, kernel_size=3, padding=1),\n    62\t                nn.BatchNorm1d(64),\n    63\t                nn.ReLU(),\n    64\t                nn.MaxPool1d(kernel_size=2),\n    65\t                nn.Conv1d(64, 128, kernel_size=3, padding=1),\n    66\t                nn.BatchNorm1d(128),\n    67\t                nn.ReLU(),\n    68\t                nn.MaxPool1d(kernel_size=2),\n    69\t                nn.Conv1d(128, 256, kernel_size=3, padding=1),\n    70\t                nn.BatchNorm1d(256),\n    71\t                nn.ReLU(),\n    72\t                nn.MaxPool1d(kernel_size=2)\n    73\t            )\n    74\t\n    75\t            # CNN output channels is 256\n    76\t            cnn_output_size = 256\n    77\t\n    78\t            # LSTM layers - Process temporal features\n    79\t            self.lstm = nn.LSTM(\n    80\t                input_size=cnn_output_size,\n    81\t                hidden_size=hidden_size,\n    82\t                num_layers=num_layers,\n    83\t                batch_first=True,\n    84\t                dropout=0.2,\n    85\t                bidirectional=True\n    86\t            )\n    87\t\n    88\t            # Fully connected layers - Output classification results\n    89\t            self.fc = nn.Sequential(\n    90\t                nn.Linear(hidden_size * 2, 128),\n    91\t                nn.ReLU(),\n    92\t                nn.Dropout(0.5),\n    93\t                nn.Linear(128, 64),\n    94\t                nn.ReLU(),\n    95\t                nn.Dropout(0.3),\n    96\t                nn.Linear(64, output_size)\n    97\t            )\n    98\t\n    99\t        def forward(self, x):\n   100\t            batch_size, seq_len, features = x.size()\n   101\t\n   102\t            # 重塑输入以通过CNN: (batch_size, features, seq_len)\n   103\t            x = x.permute(0, 2, 1)\n   104\t\n   105\t            # 通过CNN\n   106\t            cnn_out = self.cnn(x)\n   107\t\n   108\t            # 重塑CNN输出以通过LSTM: (batch_size, seq_len, cnn_features)\n   109\t            cnn_out = cnn_out.permute(0, 2, 1)\n   110\t\n   111\t            # 通过LSTM\n   112\t            lstm_out, _ = self.lstm(cnn_out)\n   113\t\n   114\t            # 取最后一个时间步的输出\n   115\t            lstm_out = lstm_out[:, -1, :]\n   116\t\n   117\t            # 通过全连接层\n   118\t            output = self.fc(lstm_out)\n   119\t\n   120\t            return output\n   121\t\n   122\t\n   123\t    class YawnDetector:\n   124\t        \&quot;\&quot;\&quot;专门用于打哈欠检测的类\&quot;\&quot;\&quot;\n   125\t        def __init__(self, model_path=None, seq_length=30, consecutive_frames=15):\n   126\t            self.model = None\n   127\t            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if PYTORCH_AVAILABLE else None\n   128\t            self.seq_length = seq_length\n   129\t            self.features_buffer = []\n   130\t            self.is_available = False\n   131\t\n   132\t            # 添加连续帧判断逻辑（参考real_pljc）\n   133\t            self.consecutive_frames = consecutive_frames\n   134\t            self.fatigue_frames = 0\n   135\t            self.frame_count = 0\n   136\t\n   137\t            if PYTORCH_AVAILABLE and model_path and os.path.exists(model_path):\n   138\t                self.load_model(model_path)\n   139\t\n   140\t        def load_model(self, model_path):\n   141\t            \&quot;\&quot;\&quot;加载训练好的CNN+LSTM模型\&quot;\&quot;\&quot;\n   142\t            try:\n   143\t                # 初始化模型（与训练时的结构保持一致）\n   144\t                self.model = YawnCNNLSTM(\n   145\t                    input_size=138,  # 2 (EAR, MAR) + 68*2 (landmark coordinates)\n   146\t                    hidden_size=64,\n   147\t                    num_layers=1,\n   148\t                    output_size=1\n   149\t                ).to(self.device)\n   150\t\n   151\t                # 加载模型权重\n   152\t                self.model.load_state_dict(torch.load(model_path, map_location=self.device, weights_only=False))\n   153\t                self.model.eval()\n   154\t                self.is_available = True\n   155\t                print(f\&quot;✅ CNN+LSTM打哈欠检测模型加载成功: {model_path}\&quot;)\n   156\t\n   157\t            except Exception as e:\n   158\t                print(f\&quot;❌ CNN+LSTM打哈欠检测模型加载失败: {e}\&quot;)\n   159\t                self.model = None\n   160\t                self.is_available = False\n   161\t\n   162\t        def extract_features(self, landmarks, ear, mar, frame_height=480):\n   163\t            \&quot;\&quot;\&quot;提取特征向量（与real_pljc保持一致）\&quot;\&quot;\&quot;\n   164\t            if landmarks is None:\n   165\t                return None\n   166\t\n   167\t            try:\n   168\t                # 归一化关键点坐标（以鼻尖为基准，使用帧高度归一化）\n   169\t                nose = landmarks[30]  # 鼻尖关键点\n   170\t                normalized_landmarks = (landmarks - nose).flatten() / frame_height  # 使用帧高度归一化\n   171\t\n   172\t                # 组合特征：EAR, MAR + 68个关键点的x,y坐标\n   173\t                features = np.concatenate([[ear, mar], normalized_landmarks])\n   174\t                return features\n   175\t\n   176\t            except Exception as e:\n   177\t                print(f\&quot;特征提取失败: {e}\&quot;)\n   178\t                return None\n   179\t\n   180\t        def update_buffer(self, features):\n   181\t            \&quot;\&quot;\&quot;更新特征缓冲区\&quot;\&quot;\&quot;\n   182\t            if features is not None:\n   183\t                self.features_buffer.append(features)\n   184\t                if len(self.features_buffer) &gt; self.seq_length:\n   185\t                    self.features_buffer.pop(0)\n   186\t\n   187\t        def predict_yawn(self, detection_enabled=True):\n   188\t            \&quot;\&quot;\&quot;预测是否打哈欠（与real_pljc保持一致的逻辑）\&quot;\&quot;\&quot;\n   189\t            if not self.is_available or len(self.features_buffer) &lt; self.seq_length:\n   190\t                return False, 0.0\n   191\t\n   192\t            try:\n   193\t                # 准备输入序列\n   194\t                input_seq = np.array([self.features_buffer])\n   195\t                input_tensor = torch.FloatTensor(input_seq).to(self.device)\n   196\t\n   197\t                # 模型预测\n   198\t                with torch.no_grad():\n   199\t                    logits = self.model(input_tensor).item()\n   200\t                    prediction = torch.sigmoid(torch.tensor(logits)).item()\n   201\t\n   202\t                # 更新疲劳状态（参考real_pljc的连续帧判断逻辑）\n   203\t                self.frame_count += 1\n   204\t                if prediction &gt;= 0.5:  # 单帧预测阈值\n   205\t                    self.fatigue_frames += 1\n   206\t                else:\n   207\t                    self.fatigue_frames = 0\n   208\t\n   209\t                # 判定疲劳需要连续帧数达到阈值\n   210\t                is_fatigued = self.fatigue_frames &gt;= self.consecutive_frames\n   211\t\n   212\t                # 如果检测被禁用（冷却期），重置连续帧计数\n   213\t                if not detection_enabled and is_fatigued:\n   214\t                    self.fatigue_frames = 0\n   215\t                    is_fatigued = False\n   216\t\n   217\t                return is_fatigued, prediction\n   218\t\n   219\t            except Exception as e:\n   220\t                print(f\&quot;CNN+LSTM打哈欠预测失败: {e}\&quot;)\n   221\t                return False, 0.0\n   222\t\n   223\t        def reset_state(self):\n   224\t            \&quot;\&quot;\&quot;重置检测器状态（用于冷却期）\&quot;\&quot;\&quot;\n   225\t            self.fatigue_frames = 0\n   226\t            self.frame_count = 0\n   227\t\n   228\t\n   229\t# 如果PyTorch不可用，定义占位符类\n   230\tif not PYTORCH_AVAILABLE:\n   231\t    class YawnCNNLSTM:\n   232\t        def __init__(self, *args, **kwargs):\n   233\t            pass\n   234\t\n   235\t    class YawnDetector:\n   236\t        def __init__(self, *args, **kwargs):\n   237\t            self.is_available = False\n   238\t\n   239\t\n   240\t# 处理TensorFlow导入失败的情况\n   241\ttry:\n   242\t    # 这个try块是为了兼容原有的except ImportError\n   243\t    pass\n   244\texcept ImportError:\n   245\t    # 如果导入失败，定义基本占位符\n   246\t    class Sequential:\n   247\t        def __init__(self):\n   248\t            pass\n   249\t\n   250\t        def add(self, layer):\n   251\t            pass\n   252\t\n   253\t    class ResNet50:\n   254\t        def __init__(self, *args, **kwargs):\n   255\t            pass\n   256\t\n   257\t    class LSTM:\n   258\t        def __init__(self, *args, **kwargs):\n   259\t            pass\n   260\t\n   261\t    class Dense:\n   262\t        def __init__(self, *args, **kwargs):\n   263\t            pass\n   264\t\n   265\t    class Conv2D:\n   266\t        def __init__(self, *args, **kwargs):\n   267\t            pass\n   268\t\n   269\t    class MaxPooling2D:\n   270\t        def __init__(self, *args, **kwargs):\n   271\t            pass\n   272\t\n   273\t    class Flatten:\n   274\t        def __init__(self, *args, **kwargs):\n   275\t            pass\n   276\t\n   277\t    tf = None\n   278\t    TENSORFLOW_AVAILABLE = False\n   279\t\n   280\timport main_ui\n   281\t\n   282\t\n   283\t# pyside6-uic -o main_ui.py main.ui\n   284\t\n   285\tclass MainUI(QtWidgets.QWidget, main_ui.Ui_Form):\n   286\t    # 信号，在UI线程中，不能在其他线程直接操作UI\n   287\t    thread_signal = QtCore.Signal(dict)\n   288\t\n   289\t    def __init__(self):\n   290\t        super().__init__()\n   291\t        self.setupUi(self)\n   292\t\n   293\t        # 初始化摄像头列表\n   294\t        self.cameras = []\n   295\t        self.init_camera_list()\n   296\t\n   297\t        # 连接信号\n   298\t        self.pushButton.clicked.connect(self.button_clicked)\n   299\t\n   300\t        # 初始化疲劳统计模块\n   301\t        try:\n   302\t            from fatigue_statistics import FatigueStatistics\n   303\t            self.fatigue_stats = FatigueStatistics()\n   304\t            print(\&quot;✅ 疲劳统计模块已初始化\&quot;)\n   305\t        except Exception as e:\n   306\t            print(f\&quot;⚠️ 疲劳统计模块初始化失败: {e}\&quot;)\n   307\t            self.fatigue_stats = None\n   308\t\n   309\t        # 初始化深度学习模型\n   310\t        self.resnet = None\n   311\t        self.cnn_model = None\n   312\t        self.lstm_model = None\n   313\t\n   314\t        # 初始化CNN疲劳检测器\n   315\t        self.cnn_detector = None\n   316\t\n   317\t        # 初始化CNN+LSTM打哈欠检测器\n   318\t        self.yawn_detector = None\n   319\t\n   320\t        if TENSORFLOW_AVAILABLE:\n   321\t            try:\n   322\t                self.init_models()\n   323\t                self.init_cnn_detector()\n   324\t            except Exception as e:\n   325\t                print(f\&quot;模型初始化失败: {e}\&quot;)\n   326\t        else:\n   327\t            print(\&quot;TensorFlow未安装，使用简化功能\&quot;)\n   328\t\n   329\t        # 初始化CNN+LSTM打哈欠检测器\n   330\t        if PYTORCH_AVAILABLE:\n   331\t            try:\n   332\t                self.init_yawn_detector()\n   333\t            except Exception as e:\n   334\t                print(f\&quot;CNN+LSTM打哈欠检测器初始化失败: {e}\&quot;)\n   335\t\n   336\t        # 连接信号\n   337\t        # self.thread_signal.connect(self.thread_singnal_slot)\n   338\t\n   339\t        # 六个功能是否要用\n   340\t        self.fun = [True] * 6\n   341\t\n   342\t        # 兼容新旧界面的复选框名称\n   343\t        try:\n   344\t            # 尝试使用新界面的复选框\n   345\t            if hasattr(self, 'checkBox'):\n   346\t                self.checkBox_11 = self.checkBox\n   347\t                self.checkBox_12 = self.checkBox_2\n   348\t                self.checkBox_21 = self.checkBox_3\n   349\t                self.checkBox_22 = self.checkBox_4\n   350\t                # 为缺失的复选框创建占位符\n   351\t                if not hasattr(self, 'checkBox_31'):\n   352\t                    from PySide6.QtWidgets import QCheckBox\n   353\t                    self.checkBox_31 = QCheckBox()\n   354\t                    self.checkBox_32 = QCheckBox()\n   355\t\n   356\t            self.checkBox_11.setChecked(self.fun[0])\n   357\t            self.checkBox_12.setChecked(self.fun[1])\n   358\t            self.checkBox_21.setChecked(self.fun[2])\n   359\t            self.checkBox_22.setChecked(self.fun[3])\n   360\t            if hasattr(self, 'checkBox_31'):\n   361\t                self.checkBox_31.setChecked(self.fun[4])\n   362\t            if hasattr(self, 'checkBox_32'):\n   363\t                self.checkBox_32.setChecked(self.fun[5])\n   364\t\n   365\t            self.checkBox_11.stateChanged.connect(self.select_changed)\n   366\t            self.checkBox_12.stateChanged.connect(self.select_changed)\n   367\t            self.checkBox_21.stateChanged.connect(self.select_changed)\n   368\t            self.checkBox_22.stateChanged.connect(self.select_changed)\n   369\t            if hasattr(self, 'checkBox_31'):\n   370\t                self.checkBox_31.stateChanged.connect(self.select_changed)\n   371\t            if hasattr(self, 'checkBox_32'):\n   372\t                self.checkBox_32.stateChanged.connect(self.select_changed)\n   373\t        except AttributeError as e:\n   374\t            print(f\&quot;界面组件初始化警告: {e}\&quot;)\n   375\t            # 创建默认的功能状态\n   376\t            pass\n   377\t\n   378\t        # 阈值\n   379\t        self.values = [3,2,3,5,2]\n   380\t\n   381\t        # 兼容新旧界面的spinBox名称\n   382\t        try:\n   383\t            # 为缺失的spinBox创建占位符\n   384\t            if not hasattr(self, 'spinBox_2'):\n   385\t                from PySide6.QtWidgets import QSpinBox\n   386\t                self.spinBox_2 = QSpinBox()\n   387\t                self.spinBox_2.setValue(2)\n   388\t\n   389\t            if hasattr(self, 'spinBox_1'):\n   390\t                self.spinBox_1.setValue(self.values[0])\n   391\t                self.spinBox_1.valueChanged.connect(self.value_changed)\n   392\t            if hasattr(self, 'spinBox_2'):\n   393\t                self.spinBox_2.setValue(self.values[1])\n   394\t                self.spinBox_2.valueChanged.connect(self.value_changed)\n   395\t            if hasattr(self, 'spinBox_3'):\n   396\t                self.spinBox_3.setValue(self.values[2])\n   397\t                self.spinBox_3.valueChanged.connect(self.value_changed)\n   398\t            if hasattr(self, 'spinBox_4'):\n   399\t                self.spinBox_4.setValue(self.values[3])\n   400\t                self.spinBox_4.valueChanged.connect(self.value_changed)\n   401\t            if hasattr(self, 'spinBox_5'):\n   402\t                self.spinBox_5.setValue(self.values[4])\n   403\t                self.spinBox_5.valueChanged.connect(self.value_changed)\n   404\t        except AttributeError as e:\n   405\t            print(f\&quot;SpinBox初始化警告: {e}\&quot;)\n   406\t\n   407\t        self.thread_signal.connect(self.thread_singnal_slot)\n   408\t\n   409\t        # 兼容新旧界面的图片显示标签\n   410\t        if not hasattr(self, 'label_img'):\n   411\t            from PySide6.QtWidgets import QLabel\n   412\t            self.label_img = QLabel()\n   413\t            self.label_img.setScaledContents(True)\n   414\t            # 设置最小尺寸确保有足够空间显示人脸\n   415\t            self.label_img.setMinimumSize(640, 480)\n   416\t            # 如果有视频显示区域，可以将label_img添加到其中\n   417\t            print(\&quot;创建了默认的图片显示标签\&quot;)\n   418\t        else:\n   419\t            self.label_img.setScaledContents(True)\n   420\t            # 确保现有的label_img也有合适的最小尺寸\n   421\t            if self.label_img.minimumSize().width() &lt; 640:\n   422\t                self.label_img.setMinimumSize(640, 480)\n   423\t\n   424\t        if hasattr(self, 'plainTextEdit_tip'):\n   425\t            self.plainTextEdit_tip.appendPlainText('等待开始\\n')\n   426\t        else:\n   427\t            print(\&quot;等待开始\&quot;)\n   428\t\n   429\t\n   430\t        \&quot;\&quot;\&quot;参数\&quot;\&quot;\&quot;\n   431\t        # 默认为摄像头0\n   432\t        self.VIDEO_STREAM = 0\n   433\t        self.CAMERA_STYLE = False  # False未打开摄像头，True摄像头已打开\n   434\t\n   435\t        # 优化后的眨眼检测参数 - 进一步提高敏感度\n   436\t        self.EYE_AR_THRESH = 0.20  # 进一步降低阈值，提高敏感度（原0.22）\n   437\t        self.EYE_AR_CONSEC_FRAMES = 2  # 保持较低的连续帧要求\n   438\t        self.EYE_AR_UPPER_THRESH = 0.40  # 适当提高上限，避免过滤正常眨眼\n   439\t\n   440\t        # 优化后的打哈欠检测参数 - 进一步提高敏感度\n   441\t        self.MAR_THRESH = 0.40  # 进一步降低阈值，提高敏感度（原0.45）\n   442\t        self.MAR_DURATION_THRESH = 0.6  # 进一步降低哈欠持续时间阈值（秒）\n   443\t        self.MOUTH_AR_CONSEC_FRAMES = 2  # 减少连续帧要求\n   444\t\n   445\t        # 优化后的瞌睡点头检测参数\n   446\t        self.HAR_THRESH_LOW = 15.0  # 轻微点头角度阈值（度）\n   447\t        self.HAR_THRESH_HIGH = 25.0  # 明显点头角度阈值（度）\n   448\t        self.NOD_AR_CONSEC_FRAMES = 4  # 增加连续帧要求，减少误检\n   449\t\n   450\t        # 其他检测参数\n   451\t        self.AR_CONSEC_FRAMES_check = 3\n   452\t        self.OUT_AR_CONSEC_FRAMES_check = 5\n   453\t\n   454\t        \&quot;\&quot;\&quot;计数\&quot;\&quot;\&quot;\n   455\t        # 初始化帧计数器和眨眼总数\n   456\t        self.COUNTER = 0\n   457\t        self.TOTAL = 0\n   458\t        # 初始化帧计数器和打哈欠总数\n   459\t        self.mCOUNTER = 0\n   460\t        self.mTOTAL = 0\n   461\t        # 初始化帧计数器和点头总数\n   462\t        self.hCOUNTER = 0\n   463\t        self.hTOTAL = 0\n   464\t        # 离职时间长度\n   465\t        self.oCOUNTER = 0\n   466\t\n   467\t        # 新增：改进的检测状态跟踪\n   468\t        self.yawn_start_time = None  # 哈欠开始时间\n   469\t        self.last_ear_values = []  # 最近的EAR值历史\n   470\t        self.last_mar_values = []  # 最近的MAR值历史\n   471\t        self.fatigue_score = 0.0  # 疲劳评分\n   472\t        self.baseline_ear = 0.3  # 基线EAR值（将动态调整）\n   473\t        self.baseline_mar = 0.4  # 基线MAR值（将动态调整）\n   474\t\n   475\t        # 打哈欠冷却机制\n   476\t        self.last_yawn_time = None  # 上次检测到打哈欠的时间\n   477\t        self.yawn_cooldown_seconds = 3.0  # 打哈欠冷却时间（秒）\n   478\t        self.yawn_detection_enabled = True  # 打哈欠检测是否启用\n   479\t\n   480\t        # 自适应阈值调整\n   481\t        self.calibration_frames = 0  # 校准帧数\n   482\t        self.calibration_period = 300  # 校准周期（帧数）\n   483\t        self.adaptive_mode = True  # 是否启用自适应模式\n   484\t\n   485\t        \&quot;\&quot;\&quot;姿态\&quot;\&quot;\&quot;\n   486\t        # 世界坐标系(UVW)：填写3D参考点，该模型参考http://aifi.isr.uc.pt/Downloads/OpenGL/glAnthropometric3DModel.cpp\n   487\t        self.object_pts = np.float32([[6.825897, 6.760612, 4.402142],  #33左眉左上角\n   488\t                                 [1.330353, 7.122144, 6.903745],  #29左眉右角\n   489\t                                 [-1.330353, 7.122144, 6.903745], #34右眉左角\n   490\t                                 [-6.825897, 6.760612, 4.402142], #38右眉右上角\n   491\t                                 [5.311432, 5.485328, 3.987654],  #13左眼左上角\n   492\t                                 [1.789930, 5.393625, 4.413414],  #17左眼右上角\n   493\t                                 [-1.789930, 5.393625, 4.413414], #25右眼左上角\n   494\t                                 [-5.311432, 5.485328, 3.987654], #21右眼右上角\n   495\t                                 [2.005628, 1.409845, 6.165652],  #55鼻子左上角\n   496\t                                 [-2.005628, 1.409845, 6.165652], #49鼻子右上角\n   497\t                                 [2.774015, -2.080775, 5.048531], #43嘴左上角\n   498\t                                 [-2.774015, -2.080775, 5.048531],#39嘴右上角\n   499\t                                 [0.000000, -3.116408, 6.097667], #45嘴中央下角\n   500\t                                 [0.000000, -7.415691, 4.070434]])#6下巴角\n   501\t\n   502\t        # 相机坐标系(XYZ)：添加相机内参\n   503\t        self.K = [6.5308391993466671e+002, 0.0, 3.1950000000000000e+002,\n   504\t                 0.0, 6.5308391993466671e+002, 2.3950000000000000e+002,\n   505\t                 0.0, 0.0, 1.0]# 等价于矩阵[fx, 0, cx; 0, fy, cy; 0, 0, 1]\n   506\t        # 图像中心坐标系(uv)：相机畸变参数[k1, k2, p1, p2, k3]\n   507\t        self.D = [7.0834633684407095e-002, 6.9140193737175351e-002, 0.0, 0.0, -1.3073460323689292e+000]\n   508\t\n   509\t        # 像素坐标系(xy)：填写凸轮的本征和畸变系数\n   510\t        self.cam_matrix = np.array(self.K).reshape(3, 3).astype(np.float32)\n   511\t        self.dist_coeffs = np.array(self.D).reshape(5, 1).astype(np.float32)\n   512\t\n   513\t        # 重新投影3D点的世界坐标轴以验证结果姿势\n   514\t        self.reprojectsrc = np.float32([[10.0, 10.0, 10.0],\n   515\t                                       [10.0, 10.0, -10.0],\n   516\t                                       [10.0, -10.0, -10.0],\n   517\t                                       [10.0, -10.0, 10.0],\n   518\t                                       [-10.0, 10.0, 10.0],\n   519\t                                       [-10.0, 10.0, -10.0],\n   520\t                                       [-10.0, -10.0, -10.0],\n   521\t                                       [-10.0, -10.0, 10.0]])\n   522\t        # 绘制正方体12轴\n   523\t        self.line_pairs = [[0, 1], [1, 2], [2, 3], [3, 0],\n   524\t                          [4, 5], [5, 6], [6, 7], [7, 4],\n   525\t                          [0, 4], [1, 5], [2, 6], [3, 7]]\n   526\t\n   527\t\n   528\t        # 线程\n   529\t        self.thread = None\n   530\t        self.sound_thread = None\n   531\t        self.is_running = True  # 添加运行状态标志\n   532\t\n   533\t    def safe_emit_signal(self, data):\n   534\t        \&quot;\&quot;\&quot;安全地发送信号\&quot;\&quot;\&quot;\n   535\t        try:\n   536\t            if self.is_running and hasattr(self, 'thread_signal'):\n   537\t                # 进一步减少调试输出，只在重要事件时打印\n   538\t                if data['type'] == 'msg' and ('疲劳' in str(data.get('value', '')) or 'CNN检测' in str(data.get('value', ''))):\n   539\t                    print(f\&quot;发送重要信号: {data['type']} - {data.get('value', '')}\&quot;)\n   540\t                elif data['type'] == 'res' and hasattr(self, 'frame_count') and self.frame_count % 300 == 0:  # 每10秒打印一次状态\n   541\t                    print(f\&quot;系统状态正常: FPS={data.get('value', ['', '', '0'])[2]}\&quot;)\n   542\t\n   543\t                self.thread_signal.emit(data)\n   544\t                return True\n   545\t            else:\n   546\t                # 只在第一次出现错误时记录，避免重复日志\n   547\t                if not hasattr(self, '_signal_error_logged'):\n   548\t                    print(f\&quot;检测已停止，停止信号发送\&quot;)\n   549\t                    self._signal_error_logged = True\n   550\t                return False\n   551\t        except RuntimeError as e:\n   552\t            # Qt对象已被销毁的情况\n   553\t            if not hasattr(self, '_runtime_error_logged'):\n   554\t                print(f\&quot;UI已关闭，停止信号发送\&quot;)\n   555\t                self._runtime_error_logged = True\n   556\t            return False\n   557\t        except Exception as e:\n   558\t            if not hasattr(self, '_unknown_error_logged'):\n   559\t                print(f\&quot;信号发送异常: {e}\&quot;)\n   560\t                self._unknown_error_logged = True\n   561\t            return False\n   562\t\n   563\t    def _optimize_camera_brightness(self):\n   564\t        \&quot;\&quot;\&quot;优化摄像头设置 - 使用配置文件参数\&quot;\&quot;\&quot;\n   565\t        if self.cap is None or not self.cap.isOpened():\n   566\t            return\n   567\t\n   568\t        print(\&quot;正在优化摄像头设置...\&quot;)\n   569\t\n   570\t        try:\n   571\t            # 使用配置文件中的摄像头属性\n   572\t            camera_props = camera_config.get_camera_properties()\n   573\t\n   574\t            for prop, value in camera_props.items():\n   575\t                try:\n   576\t                    self.cap.set(prop, value)\n   577\t                except Exception as e:\n   578\t                    # 某些属性可能不被支持，继续设置其他属性\n   579\t                    pass\n   580\t\n   581\t            print(f\&quot;摄像头优化设置完成:\&quot;)\n   582\t            print(f\&quot;  分辨率: {camera_config.CAMERA_WIDTH}x{camera_config.CAMERA_HEIGHT}\&quot;)\n   583\t            print(f\&quot;  FPS: {camera_config.CAMERA_FPS}\&quot;)\n   584\t            print(f\&quot;  亮度: {camera_config.BRIGHTNESS}\&quot;)\n   585\t            print(f\&quot;  对比度: {camera_config.CONTRAST}\&quot;)\n   586\t            print(f\&quot;  增益: {camera_config.GAIN}\&quot;)\n   587\t            print(\&quot;  已针对暗环境和帧率进行优化\&quot;)\n   588\t\n   589\t        except Exception as e:\n   590\t            print(f\&quot;摄像头参数设置失败: {e}\&quot;)\n   591\t            # 即使设置失败也继续，使用默认参数\n   592\t\n   593\t    def _enhance_dark_frame(self, frame):\n   594\t        \&quot;\&quot;\&quot;增强暗图像 - 使用配置文件参数\&quot;\&quot;\&quot;\n   595\t        if frame is None:\n   596\t            return frame\n   597\t\n   598\t        # 检查图像亮度\n   599\t        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n   600\t        mean_brightness = np.mean(gray)\n   601\t\n   602\t        # 如果图像太暗，应用多步骤增强\n   603\t        if mean_brightness &lt; camera_config.DARK_THRESHOLD:\n   604\t            enhanced_frame = frame.copy()\n   605\t\n   606\t            # 步骤1: 基础亮度和对比度调整\n   607\t            enhanced_frame = cv2.convertScaleAbs(\n   608\t                enhanced_frame,\n   609\t                alpha=camera_config.BRIGHTNESS_ALPHA,\n   610\t                beta=camera_config.BRIGHTNESS_BETA\n   611\t            )\n   612\t\n   613\t            # 步骤2: 应用CLAHE（对比度限制自适应直方图均衡化）\n   614\t            clahe = cv2.createCLAHE(\n   615\t                clipLimit=camera_config.CLAHE_CLIP_LIMIT,\n   616\t                tileGridSize=camera_config.CLAHE_TILE_SIZE\n   617\t            )\n   618\t\n   619\t            # 在LAB色彩空间中处理亮度通道\n   620\t            lab = cv2.cvtColor(enhanced_frame, cv2.COLOR_BGR2LAB)\n   621\t            lab[:,:,0] = clahe.apply(lab[:,:,0])\n   622\t            enhanced_frame = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n   623\t\n   624\t            # 步骤3: Gamma校正进一步提亮暗部\n   625\t            gamma = camera_config.GAMMA_CORRECTION\n   626\t            inv_gamma = 1.0 / gamma\n   627\t            table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\&quot;uint8\&quot;)\n   628\t            enhanced_frame = cv2.LUT(enhanced_frame, table)\n   629\t\n   630\t            # 步骤4: 与原图混合，保持自然效果\n   631\t            enhanced_frame = cv2.addWeighted(\n   632\t                frame,\n   633\t                1.0 - camera_config.ENHANCEMENT_WEIGHT,\n   634\t                enhanced_frame,\n   635\t                camera_config.ENHANCEMENT_WEIGHT,\n   636\t                0\n   637\t            )\n   638\t\n   639\t            return enhanced_frame\n   640\t\n   641\t        return frame\n   642\t\n   643\t    def init_camera_list(self):\n   644\t        \&quot;\&quot;\&quot;初始化摄像头列表\&quot;\&quot;\&quot;\n   645\t        self.cameras = []\n   646\t        print(\&quot;正在扫描可用摄像头...\&quot;)\n   647\t\n   648\t        for i in range(5):\n   649\t            print(f\&quot;  测试摄像头索引 {i}...\&quot;)\n   650\t            cap = None\n   651\t            try:\n   652\t                cap = cv2.VideoCapture(i, cv2.CAP_DSHOW)\n   653\t                if cap.isOpened():\n   654\t                    # 测试是否能读取帧\n   655\t                    ret, frame = cap.read()\n   656\t                    if ret and frame is not None and frame.size &gt; 0:\n   657\t                        self.cameras.append(i)\n   658\t                        print(f\&quot;    ✓ 摄像头 {i} 可用\&quot;)\n   659\t                    else:\n   660\t                        print(f\&quot;    ✗ 摄像头 {i} 无法读取帧\&quot;)\n   661\t                else:\n   662\t                    print(f\&quot;    ✗ 摄像头 {i} 无法打开\&quot;)\n   663\t\n   664\t                if cap is not None:\n   665\t                    cap.release()\n   666\t\n   667\t            except Exception as e:\n   668\t                print(f\&quot;    ✗ 摄像头 {i} 测试异常: {e}\&quot;)\n   669\t                if cap is not None:\n   670\t                    try:\n   671\t                        cap.release()\n   672\t                    except:\n   673\t                        pass\n   674\t\n   675\t        print(f\&quot;找到 {len(self.cameras)} 个摄像头设备\&quot;)\n   676\t        if self.cameras:\n   677\t            print(\&quot;可用摄像头:\&quot;, self.cameras)\n   678\t        else:\n   679\t            print(\&quot;未找到摄像头设备\&quot;)\n   680\t\n   681\t    def _init_camera_robust(self, camera_index):\n   682\t        \&quot;\&quot;\&quot;简化的摄像头初始化方法 - 避免闪烁\&quot;\&quot;\&quot;\n   683\t        print(f\&quot;正在初始化摄像头 {camera_index}...\&quot;)\n   684\t\n   685\t        # 优先使用DirectShow后端（Windows最稳定）\n   686\t        try:\n   687\t            print(f\&quot;  使用DirectShow后端...\&quot;)\n   688\t            cap = cv2.VideoCapture(camera_index, cv2.CAP_DSHOW)\n   689\t\n   690\t            if cap.isOpened():\n   691\t                # 立即测试读取\n   692\t                ret, frame = cap.read()\n   693\t                if ret and frame is not None and frame.size &gt; 0:\n   694\t                    print(f\&quot;摄像头 {camera_index} 初始化成功\&quot;)\n   695\t                    return cap\n   696\t                else:\n   697\t                    print(f\&quot;  无法读取帧\&quot;)\n   698\t                    cap.release()\n   699\t            else:\n   700\t                print(f\&quot;  无法打开摄像头\&quot;)\n   701\t                if cap is not None:\n   702\t                    cap.release()\n   703\t\n   704\t        except Exception as e:\n   705\t            print(f\&quot;  DirectShow初始化失败: {e}\&quot;)\n   706\t            if 'cap' in locals() and cap is not None:\n   707\t                try:\n   708\t                    cap.release()\n   709\t                except:\n   710\t                    pass\n   711\t\n   712\t        # 如果DirectShow失败，尝试默认后端\n   713\t        try:\n   714\t            print(f\&quot;  尝试默认后端...\&quot;)\n   715\t            cap = cv2.VideoCapture(camera_index)\n   716\t\n   717\t            if cap.isOpened():\n   718\t                ret, frame = cap.read()\n   719\t                if ret and frame is not None and frame.size &gt; 0:\n   720\t                    print(f\&quot;摄像头 {camera_index} 初始化成功（默认后端）\&quot;)\n   721\t                    return cap\n   722\t                else:\n   723\t                    cap.release()\n   724\t            else:\n   725\t                if cap is not None:\n   726\t                    cap.release()\n   727\t\n   728\t        except Exception as e:\n   729\t            print(f\&quot;  默认后端初始化失败: {e}\&quot;)\n   730\t            if 'cap' in locals() and cap is not None:\n   731\t                try:\n   732\t                    cap.release()\n   733\t                except:\n   734\t                    pass\n   735\t\n   736\t        print(f\&quot;摄像头 {camera_index} 初始化失败\&quot;)\n   737\t        return None\n   738\t\n   739\t    def _check_camera_health(self):\n   740\t        \&quot;\&quot;\&quot;检查摄像头健康状态\&quot;\&quot;\&quot;\n   741\t        if not hasattr(self, 'cap') or self.cap is None:\n   742\t            return False\n   743\t\n   744\t        try:\n   745\t            # 检查摄像头是否仍然打开\n   746\t            if not self.cap.isOpened():\n   747\t                return False\n   748\t\n   749\t            # 尝试获取一些基本属性\n   750\t            width = self.cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n   751\t            height = self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n   752\t\n   753\t            if width &lt;= 0 or height &lt;= 0:\n   754\t                return False\n   755\t\n   756\t            return True\n   757\t\n   758\t        except Exception as e:\n   759\t            print(f\&quot;摄像头健康检查失败: {e}\&quot;)\n   760\t            return False\n   761\t\n   762\t    def _reconnect_camera(self):\n   763\t        \&quot;\&quot;\&quot;重新连接摄像头\&quot;\&quot;\&quot;\n   764\t        print(\&quot;尝试重新连接摄像头...\&quot;)\n   765\t\n   766\t        # 释放当前摄像头\n   767\t        if hasattr(self, 'cap') and self.cap is not None:\n   768\t            try:\n   769\t                self.cap.release()\n   770\t            except:\n   771\t                pass\n   772\t            self.cap = None\n   773\t\n   774\t        # 短暂等待\n   775\t        time.sleep(0.2)\n   776\t\n   777\t        # 尝试重新打开\n   778\t        try:\n   779\t            self.cap = cv2.VideoCapture(self.VIDEO_STREAM, cv2.CAP_DSHOW)\n   780\t            if self.cap.isOpened():\n   781\t                # 重新设置基本参数\n   782\t                self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n   783\t                self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n   784\t                self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n   785\t\n   786\t                # 测试读取\n   787\t                ret, test_frame = self.cap.read()\n   788\t                if ret and test_frame is not None:\n   789\t                    print(\&quot;摄像头重连成功\&quot;)\n   790\t                    return True\n   791\t                else:\n   792\t                    print(\&quot;摄像头重连后无法读取帧\&quot;)\n   793\t                    self.cap.release()\n   794\t                    self.cap = None\n   795\t                    return False\n   796\t            else:\n   797\t                print(\&quot;摄像头重连失败\&quot;)\n   798\t                return False\n   799\t\n   800\t        except Exception as e:\n   801\t            print(f\&quot;摄像头重连异常: {e}\&quot;)\n   802\t            return False\n   803\t\n   804\t\n   805\t\n   806\t    def init_models(self):\n   807\t        \&quot;\&quot;\&quot;初始化深度学习模型\&quot;\&quot;\&quot;\n   808\t        self.resnet = self._init_resnet()\n   809\t        self.cnn_model = self._init_cnn_model()\n   810\t        self.lstm_model = self._init_lstm_model()\n   811\t\n   812\t        if all([self.resnet, self.cnn_model, self.lstm_model]):\n   813\t            print(\&quot;所有模型初始化完成\&quot;)\n   814\t        else:\n   815\t            print(\&quot;部分模型初始化失败\&quot;)\n   816\t\n   817\t    def init_cnn_detector(self):\n   818\t        \&quot;\&quot;\&quot;初始化CNN疲劳检测器\&quot;\&quot;\&quot;\n   819\t        try:\n   820\t            # 尝试导入CNN检测器\n   821\t            from simple_cnn_detector import CNNFatigueDetector\n   822\t\n   823\t            # 检查是否有训练好的模型\n   824\t            model_path = './model/fatigue_model_mobilenet.h5'\n   825\t            if os.path.exists(model_path):\n   826\t                self.cnn_detector = CNNFatigueDetector(model_path)\n   827\t                if self.cnn_detector.is_available():\n   828\t                    print(\&quot;✅ CNN疲劳检测器已加载\&quot;)\n   829\t                else:\n   830\t                    print(\&quot;⚠️ CNN检测器加载失败\&quot;)\n   831\t                    self.cnn_detector = None\n   832\t            else:\n   833\t                print(\&quot;⚠️ 未找到训练好的CNN模型，将创建简化版本\&quot;)\n   834\t                # 创建一个使用预训练ResNet的简化检测器\n   835\t                self.cnn_detector = self._create_simple_detector()\n   836\t\n   837\t        except ImportError as e:\n   838\t            print(f\&quot;CNN检测器导入失败: {e}\&quot;)\n   839\t            self.cnn_detector = None\n   840\t        except Exception as e:\n   841\t            print(f\&quot;CNN检测器初始化失败: {e}\&quot;)\n   842\t            self.cnn_detector = None\n   843\t\n   844\t    def _create_simple_detector(self):\n   845\t        \&quot;\&quot;\&quot;创建简化的疲劳检测器\&quot;\&quot;\&quot;\n   846\t        class SimpleFatigueDetector:\n   847\t            def __init__(self, resnet_model):\n   848\t                self.resnet = resnet_model\n   849\t                self.available = resnet_model is not None\n   850\t\n   851\t            def is_available(self):\n   852\t                return self.available\n   853\t\n   854\t            def predict_fatigue(self, face_image):\n   855\t                \&quot;\&quot;\&quot;使用ResNet特征进行简单的疲劳检测\&quot;\&quot;\&quot;\n   856\t                if not self.available or face_image is None or face_image.size == 0:\n   857\t                    return None\n   858\t\n   859\t                try:\n   860\t                    # 预处理图像\n   861\t                    img = cv2.resize(face_image, (224, 224))\n   862\t                    img = tf.keras.applications.resnet50.preprocess_input(img)\n   863\t\n   864\t                    # 提取特征\n   865\t                    features = self.resnet.predict(np.expand_dims(img, axis=0), verbose=0)\n   866\t\n   867\t                    # 简单的疲劳判断（基于特征的统计特性）\n   868\t                    feature_mean = np.mean(features)\n   869\t                    feature_std = np.std(features)\n   870\t\n   871\t                    # 简化的疲劳判断逻辑\n   872\t                    fatigue_score = feature_mean * feature_std\n   873\t                    confidence = min(abs(fatigue_score) * 100, 1.0)\n   874\t\n   875\t                    is_fatigue = fatigue_score &lt; -0.1  # 阈值可调整\n   876\t\n   877\t                    return {\n   878\t                        'predicted_class': 'drowsy' if is_fatigue else 'alert',\n   879\t                        'confidence': confidence,\n   880\t                        'fatigue_level': '疲劳' if is_fatigue else '正常',\n   881\t                        'fatigue_detected': is_fatigue,\n   882\t                        'feature_mean': feature_mean,\n   883\t                        'feature_std': feature_std\n   884\t                    }\n   885\t\n   886\t                except Exception as e:\n   887\t                    print(f\&quot;简化检测器预测失败: {e}\&quot;)\n   888\t                    return None\n   889\t\n   890\t        return SimpleFatigueDetector(self.resnet)\n   891\t\n   892\t    def init_yawn_detector(self):\n   893\t        \&quot;\&quot;\&quot;初始化CNN+LSTM打哈欠检测器\&quot;\&quot;\&quot;\n   894\t        try:\n   895\t            # 检查是否有训练好的模型 - 现在模型文件在model文件夹中\n   896\t            model_path = './model/best_fatigue_model.pth'\n   897\t            if not os.path.exists(model_path):\n   898\t                # 尝试其他可能的路径\n   899\t                model_path = '../real_pljc/models/best_fatigue_model.pth'\n   900\t            if not os.path.exists(model_path):\n   901\t                # 尝试相对路径\n   902\t                model_path = './real_pljc/models/best_fatigue_model.pth'\n   903\t            if not os.path.exists(model_path):\n   904\t                # 尝试绝对路径\n   905\t                model_path = 'D:/code/PythonProject2/real_pljc/models/best_fatigue_model.pth'\n   906\t\n   907\t            if os.path.exists(model_path):\n   908\t                self.yawn_detector = YawnDetector(model_path)\n   909\t                if self.yawn_detector.is_available:\n   910\t                    print(\&quot;✅ CNN+LSTM打哈欠检测器已加载\&quot;)\n   911\t                else:\n   912\t                    print(\&quot;⚠️ CNN+LSTM打哈欠检测器加载失败\&quot;)\n   913\t                    self.yawn_detector = None\n   914\t            else:\n   915\t                print(\&quot;⚠️ 未找到CNN+LSTM打哈欠检测模型\&quot;)\n   916\t                self.yawn_detector = None\n   917\t\n   918\t        except Exception as e:\n   919\t            print(f\&quot;CNN+LSTM打哈欠检测器初始化失败: {e}\&quot;)\n   920\t            self.yawn_detector = None\n   921\t\n   922\t    def _init_resnet(self):\n   923\t        \&quot;\&quot;\&quot;初始化ResNet50模型\&quot;\&quot;\&quot;\n   924\t        try:\n   925\t            print(\&quot;正在加载ResNet50模型...\&quot;)\n   926\t            resnet = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n   927\t            print(\&quot;ResNet50模型加载成功\&quot;)\n   928\t            return resnet\n   929\t        except Exception as e:\n   930\t            print(f\&quot;ResNet50模型加载失败: {e}\&quot;)\n   931\t            return None\n   932\t\n   933\t    def _init_cnn_model(self):\n   934\t        \&quot;\&quot;\&quot;初始化CNN模型\&quot;\&quot;\&quot;\n   935\t        try:\n   936\t            print(\&quot;正在创建CNN模型...\&quot;)\n   937\t            cnn_model = Sequential()\n   938\t\n   939\t            if TENSORFLOW_AVAILABLE:\n   940\t                cnn_model.add(Input(shape=(64, 64, 3)))\n   941\t\n   942\t            cnn_model.add(Conv2D(32, (3, 3), activation='relu'))\n   943\t            cnn_model.add(MaxPooling2D((2, 2)))\n   944\t            cnn_model.add(Conv2D(64, (3, 3), activation='relu'))\n   945\t            cnn_model.add(MaxPooling2D((2, 2)))\n   946\t            cnn_model.add(Flatten())\n   947\t            cnn_model.add(Dense(128, activation='relu'))\n   948\t            cnn_model.add(Dense(1, activation='sigmoid'))\n   949\t            print(\&quot;CNN模型创建成功\&quot;)\n   950\t            return cnn_model\n   951\t        except Exception as e:\n   952\t            print(f\&quot;CNN模型创建失败: {e}\&quot;)\n   953\t            return None\n   954\t\n   955\t    def _init_lstm_model(self):\n   956\t        \&quot;\&quot;\&quot;初始化LSTM模型\&quot;\&quot;\&quot;\n   957\t        try:\n   958\t            print(\&quot;正在创建LSTM模型...\&quot;)\n   959\t            lstm_model = Sequential()\n   960\t\n   961\t            if TENSORFLOW_AVAILABLE:\n   962\t                lstm_model.add(Input(shape=(10, 2048)))\n   963\t\n   964\t            lstm_model.add(LSTM(64, return_sequences=True))\n   965\t            lstm_model.add(LSTM(32))\n   966\t            lstm_model.add(Dense(1, activation='sigmoid'))\n   967\t            print(\&quot;LSTM模型创建成功\&quot;)\n   968\t            return lstm_model\n   969\t        except Exception as e:\n   970\t            print(f\&quot;LSTM模型创建失败: {e}\&quot;)\n   971\t            return None\n   972\t\n   973\t    def extract_resnet_features(self, face_img):\n   974\t        \&quot;\&quot;\&quot;使用ResNet提取人脸特征\&quot;\&quot;\&quot;\n   975\t        if self.resnet is None or face_img.size == 0:\n   976\t            return None\n   977\t\n   978\t        # 预处理图像\n   979\t        img = cv2.resize(face_img, (224, 224))\n   980\t        img = tf.keras.applications.resnet50.preprocess_input(img)\n   981\t        features = self.resnet.predict(np.expand_dims(img, axis=0))\n   982\t        return features\n   983\t\n   984\t    def detect_cnn_fatigue(self, face_img):\n   985\t        \&quot;\&quot;\&quot;使用CNN进行疲劳检测\&quot;\&quot;\&quot;\n   986\t        if self.cnn_model is None or face_img.size == 0:\n   987\t            return None\n   988\t\n   989\t        # 预处理图像\n   990\t        img = cv2.resize(face_img, (64, 64))\n   991\t        img = img / 255.0  # 归一化\n   992\t        prediction = self.cnn_model.predict(np.expand_dims(img, axis=0))\n   993\t        return '疲劳' if prediction[0][0] &gt; 0.5 else '正常'\n   994\t\n   995\t    def analyze_lstm_fatigue(self, features_sequence):\n   996\t        \&quot;\&quot;\&quot;使用LSTM分析时序特征\&quot;\&quot;\&quot;\n   997\t        if self.lstm_model is None or not features_sequence:\n   998\t            return None\n   999\t\n  1000\t        # 确保序列长度一致\n  1001\t        if len(features_sequence) != 10:\n  1002\t            return None\n  1003\t\n  1004\t        prediction = self.lstm_model.predict(np.expand_dims(features_sequence, axis=0))\n  1005\t        return '深度疲劳' if prediction[0][0] &gt; 0.7 else '轻度疲劳'\n  1006\t\n  1007\t    def calculate_fatigue_score(self, blink_count, yawn_count, nod_count, time_window=60):\n  1008\t        \&quot;\&quot;\&quot;计算综合疲劳评分\&quot;\&quot;\&quot;\n  1009\t        # 权重设置（基于疲劳检测研究）\n  1010\t        blink_weight = 0.3  # 眨眼权重\n  1011\t        yawn_weight = 0.5   # 哈欠权重（更重要）\n  1012\t        nod_weight = 0.4    # 点头权重\n  1013\t\n  1014\t        # 标准化到每分钟的频率\n  1015\t        blink_rate = (blink_count / time_window) * 60\n  1016\t        yawn_rate = (yawn_count / time_window) * 60\n  1017\t        nod_rate = (nod_count / time_window) * 60\n  1018\t\n  1019\t        # 正常基线值（每分钟）\n  1020\t        normal_blink_rate = 15  # 正常眨眼频率\n  1021\t        normal_yawn_rate = 0.5  # 正常哈欠频率\n  1022\t        normal_nod_rate = 1     # 正常点头频率\n  1023\t\n  1024\t        # 计算偏离度\n  1025\t        blink_deviation = max(0, blink_rate - normal_blink_rate) / normal_blink_rate\n  1026\t        yawn_deviation = max(0, yawn_rate - normal_yawn_rate) / normal_yawn_rate\n  1027\t        nod_deviation = max(0, nod_rate - normal_nod_rate) / normal_nod_rate\n  1028\t\n  1029\t        # 综合评分\n  1030\t        fatigue_score = (blink_deviation * blink_weight +\n  1031\t                        yawn_deviation * yawn_weight +\n  1032\t                        nod_deviation * nod_weight)\n  1033\t\n  1034\t        return min(fatigue_score, 1.0)  # 限制在0-1之间\n  1035\t\n  1036\t    def get_fatigue_level(self, score):\n  1037\t        \&quot;\&quot;\&quot;根据评分获取疲劳等级\&quot;\&quot;\&quot;\n  1038\t        if score &lt; 0.2:\n  1039\t            return '正常'\n  1040\t        elif score &lt; 0.4:\n  1041\t            return '轻微疲劳'\n  1042\t        elif score &lt; 0.7:\n  1043\t            return '中度疲劳'\n  1044\t        else:\n  1045\t            return '重度疲劳'\n  1046\t\n  1047\t    def adaptive_threshold_adjustment(self, ear, mar):\n  1048\t        \&quot;\&quot;\&quot;自适应阈值调整\&quot;\&quot;\&quot;\n  1049\t        if not self.adaptive_mode:\n  1050\t            return\n  1051\t\n  1052\t        self.calibration_frames += 1\n  1053\t\n  1054\t        # 在校准期间收集数据\n  1055\t        if self.calibration_frames &lt;= self.calibration_period:\n  1056\t            # 更新基线值\n  1057\t            self.update_baseline_values(ear, mar)\n  1058\t\n  1059\t            # 校准完成后调整阈值\n  1060\t            if self.calibration_frames == self.calibration_period:\n  1061\t                self._adjust_thresholds()\n  1062\t                print(f\&quot;自适应校准完成 - EAR基线: {self.baseline_ear:.3f}, MAR基线: {self.baseline_mar:.3f}\&quot;)\n  1063\t\n  1064\t        # 定期重新校准（每1000帧）\n  1065\t        elif self.calibration_frames % 1000 == 0:\n  1066\t            self._adjust_thresholds()\n  1067\t\n  1068\t    def _adjust_thresholds(self):\n  1069\t        \&quot;\&quot;\&quot;根据基线值调整检测阈值\&quot;\&quot;\&quot;\n  1070\t        if len(self.last_ear_values) &gt;= 10:\n  1071\t            # 动态调整眨眼阈值\n  1072\t            ear_std = np.std(self.last_ear_values)\n  1073\t            self.EYE_AR_THRESH = max(0.2, self.baseline_ear - 2 * ear_std)\n  1074\t            self.EYE_AR_UPPER_THRESH = self.baseline_ear + 2 * ear_std\n  1075\t\n  1076\t        if len(self.last_mar_values) &gt;= 10:\n  1077\t            # 动态调整哈欠阈值\n  1078\t            mar_std = np.std(self.last_mar_values)\n  1079\t            self.MAR_THRESH = max(0.5, self.baseline_mar + 1.5 * mar_std)\n  1080\t\n  1081\t    def get_detection_confidence(self, ear, mar):\n  1082\t        \&quot;\&quot;\&quot;计算检测置信度\&quot;\&quot;\&quot;\n  1083\t        ear_confidence = 1.0\n  1084\t        mar_confidence = 1.0\n  1085\t\n  1086\t        if len(self.last_ear_values) &gt;= 10:\n  1087\t            ear_std = np.std(self.last_ear_values)\n  1088\t            ear_z_score = abs(ear - self.baseline_ear) / (ear_std + 1e-6)\n  1089\t            ear_confidence = min(1.0, ear_z_score / 3.0)  # 3-sigma规则\n  1090\t\n  1091\t        if len(self.last_mar_values) &gt;= 10:\n  1092\t            mar_std = np.std(self.last_mar_values)\n  1093\t            mar_z_score = abs(mar - self.baseline_mar) / (mar_std + 1e-6)\n  1094\t            mar_confidence = min(1.0, mar_z_score / 3.0)\n  1095\t\n  1096\t        return (ear_confidence + mar_confidence) / 2.0\n  1097\t\n  1098\t    def get_head_pose(self,shape):# 头部姿态估计\n  1099\t        # （像素坐标集合）填写2D参考点，注释遵循https://ibug.doc.ic.ac.uk/resources/300-W/\n  1100\t        # 17左眉左上角/21左眉右角/22右眉左上角/26右眉右上角/36左眼左上角/39左眼右上角/42右眼左上角/\n  1101\t        # 45右眼右上角/31鼻子左上角/35鼻子右上角/48左上角/54嘴右上角/57嘴中央下角/8下巴角\n  1102\t        image_pts = np.float32([shape[17], shape[21], shape[22], shape[26], shape[36],\n  1103\t                                shape[39], shape[42], shape[45], shape[31], shape[35],\n  1104\t                                shape[48], shape[54], shape[57], shape[8]])\n  1105\t        # solvePnP计算姿势——求解旋转和平移矩阵：\n  1106\t        # rotation_vec表示旋转矩阵，translation_vec表示平移矩阵，cam_matrix与K矩阵对应，dist_coeffs与D矩阵对应。\n  1107\t        _, rotation_vec, translation_vec = cv2.solvePnP(self.object_pts, image_pts, self.cam_matrix, self.dist_coeffs)\n  1108\t        # projectPoints重新投影误差：原2d点和重投影2d点的距离（输入3d点、相机内参、相机畸变、r、t，输出重投影2d点）\n  1109\t        reprojectdst, _ = cv2.projectPoints(self.reprojectsrc, rotation_vec, translation_vec, self.cam_matrix,self.dist_coeffs)\n  1110\t        reprojectdst = tuple(map(tuple, reprojectdst.reshape(8, 2)))# 以8行2列显示\n  1111\t\n  1112\t        # 计算欧拉角calc euler angle\n  1113\t        # 参考https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#decomposeprojectionmatrix\n  1114\t        rotation_mat, _ = cv2.Rodrigues(rotation_vec) #罗德里格斯公式（将旋转矩阵转换为旋转向量）\n  1115\t        pose_mat = cv2.hconcat((rotation_mat, translation_vec)) # 水平拼接，vconcat垂直拼接\n  1116\t        # decomposeProjectionMatrix将投影矩阵分解为旋转矩阵和相机矩阵\n  1117\t        _, _, _, _, _, _, euler_angle = cv2.decomposeProjectionMatrix(pose_mat)\n  1118\t\n  1119\t        # 确保从数组中提取标量值\n  1120\t        pitch = math.radians(euler_angle[0].item())\n  1121\t        yaw = math.radians(euler_angle[1].item())\n  1122\t        roll = math.radians(euler_angle[2].item())\n  1123\t\n  1124\t        pitch = math.degrees(math.asin(math.sin(pitch)))\n  1125\t        roll = -math.degrees(math.asin(math.sin(roll)))\n  1126\t        yaw = math.degrees(math.asin(math.sin(yaw)))\n  1127\t        #print('pitch:{}, yaw:{}, roll:{}'.format(pitch, yaw, roll))\n  1128\t\n  1129\t        return reprojectdst, euler_angle# 投影误差，欧拉角\n  1130\t    def eye_aspect_ratio(self,eye):\n  1131\t        # 垂直眼标志（X，Y）坐标\n  1132\t        A = dist.euclidean(eye[1], eye[5])# 计算两个集合之间的欧式距离\n  1133\t        B = dist.euclidean(eye[2], eye[4])\n  1134\t        # 计算水平之间的欧几里得距离\n  1135\t        # 水平眼标志（X，Y）坐标\n  1136\t        C = dist.euclidean(eye[0], eye[3])\n  1137\t        # 眼睛长宽比的计算\n  1138\t        ear = (A + B) / (2.0 * C)\n  1139\t        # 返回眼睛的长宽比\n  1140\t        return ear\n  1141\t\n  1142\t    def mouth_aspect_ratio(self,mouth):# 嘴部\n  1143\t        A = np.linalg.norm(mouth[2] - mouth[9])  # 51, 59\n  1144\t        B = np.linalg.norm(mouth[4] - mouth[7])  # 53, 57\n  1145\t        C = np.linalg.norm(mouth[0] - mouth[6])  # 49, 55\n  1146\t        mar = (A + B) / (2.0 * C)\n  1147\t        return mar\n  1148\t\n  1149\t    def update_baseline_values(self, ear, mar):\n  1150\t        \&quot;\&quot;\&quot;动态更新基线值\&quot;\&quot;\&quot;\n  1151\t        # 保持最近50个值的历史\n  1152\t        self.last_ear_values.append(ear)\n  1153\t        self.last_mar_values.append(mar)\n  1154\t\n  1155\t        if len(self.last_ear_values) &gt; 50:\n  1156\t            self.last_ear_values.pop(0)\n  1157\t        if len(self.last_mar_values) &gt; 50:\n  1158\t            self.last_mar_values.pop(0)\n  1159\t\n  1160\t        # 更新基线值（使用中位数，更稳定）\n  1161\t        if len(self.last_ear_values) &gt;= 10:\n  1162\t            self.baseline_ear = np.median(self.last_ear_values)\n  1163\t        if len(self.last_mar_values) &gt;= 10:\n  1164\t            self.baseline_mar = np.median(self.last_mar_values)\n  1165\t\n  1166\t    def is_valid_blink(self, ear):\n  1167\t        \&quot;\&quot;\&quot;改进的眨眼检测 - 简化版本\&quot;\&quot;\&quot;\n  1168\t        # 使用固定阈值，更容易触发\n  1169\t        thresh = self.EYE_AR_THRESH\n  1170\t\n  1171\t        # 检查是否在合理范围内（过滤异常值）\n  1172\t        if ear &gt; self.EYE_AR_UPPER_THRESH or ear &lt; 0.1:  # 添加下限检查\n  1173\t            return False\n  1174\t\n  1175\t        is_blink = ear &lt; thresh\n  1176\t\n  1177\t        # 添加调试信息\n  1178\t        if not hasattr(self, '_blink_detail_counter'):\n  1179\t            self._blink_detail_counter = 0\n  1180\t        self._blink_detail_counter += 1\n  1181\t\n  1182\t        if self._blink_detail_counter % 60 == 0:  # 每60帧打印一次详细信息\n  1183\t            print(f\&quot; 眨眼检测详情 - EAR: {ear:.3f}, 阈值: {thresh:.3f}, 是否眨眼: {is_blink}\&quot;)\n  1184\t\n  1185\t        return is_blink\n  1186\t\n  1187\t    def is_valid_yawn(self, mar, current_time):\n  1188\t        \&quot;\&quot;\&quot;改进的打哈欠检测 - 更宽松的条件\&quot;\&quot;\&quot;\n  1189\t        # 使用简化的阈值检测\n  1190\t        thresh = self.MAR_THRESH\n  1191\t\n  1192\t        is_mouth_open = mar &gt; thresh\n  1193\t\n  1194\t        if is_mouth_open:\n  1195\t            if self.yawn_start_time is None:\n  1196\t                self.yawn_start_time = current_time\n  1197\t                print(f\&quot; 开始检测哈欠，MAR: {mar:.3f}, 阈值: {thresh:.3f}\&quot;)\n  1198\t            return False  # 还在张嘴过程中\n  1199\t        else:\n  1200\t            if self.yawn_start_time is not None:\n  1201\t                # 检查持续时间\n  1202\t                duration = (current_time - self.yawn_start_time).total_seconds()\n  1203\t                self.yawn_start_time = None\n  1204\t\n  1205\t                print(f\&quot; 哈欠持续时间: {duration:.2f}秒\&quot;)\n  1206\t                # 进一步放宽哈欠持续时间要求：0.3-3.0秒\n  1207\t                is_valid = 0.3 &lt;= duration &lt;= 3.0\n  1208\t                if is_valid:\n  1209\t                    print(f\&quot;✅ 有效哈欠，持续时间: {duration:.2f}秒\&quot;)\n  1210\t                else:\n  1211\t                    print(f\&quot;❌ 无效哈欠，持续时间: {duration:.2f}秒（要求0.3-3.0秒）\&quot;)\n  1212\t                return is_valid\n  1213\t            return False\n  1214\t\n  1215\t\n  1216\t    def _learning_face(self):\n  1217\t        \&quot;\&quot;\&quot;dlib的初始化调用 - 增强版本\&quot;\&quot;\&quot;\n  1218\t        try:\n  1219\t            # 检查模型文件是否存在\n  1220\t            model_path = \&quot;./model/shape_predictor_68_face_landmarks.dat\&quot;\n  1221\t            if not os.path.exists(model_path):\n  1222\t                data = {\n  1223\t                    'type':'msg',\n  1224\t                    'value':u\&quot;❌ 模型文件不存在，请下载 shape_predictor_68_face_landmarks.dat\\n\&quot;\n  1225\t                }\n  1226\t                self.safe_emit_signal(data)\n  1227\t                return\n  1228\t\n  1229\t            # 初始化检测器\n  1230\t            print(\&quot; 正在初始化dlib人脸检测器...\&quot;)\n  1231\t            self.detector = dlib.get_frontal_face_detector()\n  1232\t            print(\&quot; 正在加载特征点预测器...\&quot;)\n  1233\t            self.predictor = dlib.shape_predictor(model_path)\n  1234\t\n  1235\t            # 验证模型加载\n  1236\t            if self.detector is None or self.predictor is None:\n  1237\t                print(\&quot;❌ 检测器或预测器为None\&quot;)\n  1238\t                data = {\n  1239\t                    'type':'msg',\n  1240\t                    'value':u\&quot;❌ 人脸检测模型加载失败\\n\&quot;\n  1241\t                }\n  1242\t                self.safe_emit_signal(data)\n  1243\t                return\n  1244\t\n  1245\t            print(\&quot;✅ 检测器初始化成功\&quot;)\n  1246\t            print(f\&quot;   检测器类型: {type(self.detector)}\&quot;)\n  1247\t            print(f\&quot;   预测器类型: {type(self.predictor)}\&quot;)\n  1248\t\n  1249\t            data = {\n  1250\t                'type':'msg',\n  1251\t                'value':u\&quot;✅ 人脸检测模型加载成功!!!\\n\&quot;\n  1252\t            }\n  1253\t            self.safe_emit_signal(data)\n  1254\t\n  1255\t        except Exception as e:\n  1256\t            data = {\n  1257\t                'type':'msg',\n  1258\t                'value':f\&quot;❌ 模型初始化失败: {e}\\n\&quot;\n  1259\t            }\n  1260\t            self.safe_emit_signal(data)\n  1261\t            return\n  1262\t\n  1263\t        (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\&quot;left_eye\&quot;]\n  1264\t        (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\&quot;right_eye\&quot;]\n  1265\t        (mStart, mEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\&quot;mouth\&quot;]\n  1266\t\n  1267\t        self.cap = None\n  1268\t\n  1269\t        # 简化的摄像头初始化\n  1270\t        print(f\&quot;尝试打开摄像头，索引: {self.VIDEO_STREAM}\&quot;)\n  1271\t\n  1272\t        # 直接尝试打开摄像头，不过度复杂化\n  1273\t        success = False\n  1274\t        for camera_index in [self.VIDEO_STREAM, 0, 1]:  # 尝试当前索引、0、1\n  1275\t            try:\n  1276\t                print(f\&quot;  尝试摄像头索引 {camera_index}...\&quot;)\n  1277\t                self.cap = cv2.VideoCapture(camera_index, cv2.CAP_DSHOW)\n  1278\t\n  1279\t                if self.cap.isOpened():\n  1280\t                    # 基本设置\n  1281\t                    self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n  1282\t                    self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n  1283\t                    self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n  1284\t\n  1285\t                    # 测试读取\n  1286\t                    ret, test_frame = self.cap.read()\n  1287\t                    if ret and test_frame is not None:\n  1288\t                        print(f\&quot;  ✓ 摄像头 {camera_index} 工作正常\&quot;)\n  1289\t                        self.VIDEO_STREAM = camera_index\n  1290\t                        self.CAMERA_STYLE = True\n  1291\t                        success = True\n  1292\t                        data['value'] = f\&quot;打开摄像头成功(索引{camera_index})!!!\&quot;\n  1293\t                        break\n  1294\t                    else:\n  1295\t                        print(f\&quot;  ✗ 摄像头 {camera_index} 无法读取\&quot;)\n  1296\t                        self.cap.release()\n  1297\t                        self.cap = None\n  1298\t                else:\n  1299\t                    print(f\&quot;  ✗ 摄像头 {camera_index} 无法打开\&quot;)\n  1300\t                    if self.cap is not None:\n  1301\t                        self.cap.release()\n  1302\t                        self.cap = None\n  1303\t\n  1304\t            except Exception as e:\n  1305\t                print(f\&quot;  ✗ 摄像头 {camera_index} 异常: {e}\&quot;)\n  1306\t                if hasattr(self, 'cap') and self.cap is not None:\n  1307\t                    try:\n  1308\t                        self.cap.release()\n  1309\t                    except:\n  1310\t                        pass\n  1311\t                    self.cap = None\n  1312\t\n  1313\t        if not success:\n  1314\t            data['value'] = u\&quot;摄像头打开失败!!!\&quot;\n  1315\t            print(\&quot;未找到可用的摄像头设备\&quot;)\n  1316\t        else:\n  1317\t            # 简化的参数设置\n  1318\t            self._optimize_camera_brightness()\n  1319\t        self.safe_emit_signal(data)\n  1320\t\n  1321\t        # 打印最终摄像头状态\n  1322\t        if hasattr(self, 'cap') and self.cap is not None and self.cap.isOpened():\n  1323\t            width = self.cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n  1324\t            height = self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n  1325\t            fps = self.cap.get(cv2.CAP_PROP_FPS)\n  1326\t            backend = self.cap.getBackendName()\n  1327\t            print(f\&quot;摄像头状态: 已打开\&quot;)\n  1328\t            print(f\&quot;  分辨率: {width}x{height}\&quot;)\n  1329\t            print(f\&quot;  设置FPS: {fps}\&quot;)\n  1330\t            print(f\&quot;  后端: {backend}\&quot;)\n  1331\t\n  1332\t            # 如果FPS为0，尝试手动设置\n  1333\t            if fps == 0:\n  1334\t                print(\&quot;检测到FPS为0，尝试手动设置...\&quot;)\n  1335\t                self.cap.set(cv2.CAP_PROP_FPS, 30)\n  1336\t                new_fps = self.cap.get(cv2.CAP_PROP_FPS)\n  1337\t                print(f\&quot;  重新设置后FPS: {new_fps}\&quot;)\n  1338\t        else:\n  1339\t            print(\&quot;未找到可用的视频源\&quot;)\n  1340\t\n  1341\t        # 初始化FPS计算变量\n  1342\t        self.frame_count = 0\n  1343\t        self.fps_start_time = time.time()\n  1344\t        self.actual_fps = 0\n  1345\t\n  1346\t        # 初始化时间变量\n  1347\t        t_time = datetime.datetime.now()\n  1348\t        e_time = datetime.datetime.now()\n  1349\t        h_time = datetime.datetime.now()\n  1350\t\n  1351\t        # 成功打开视频，循环读取视频流\n  1352\t        print(\&quot;开始视频流处理...\&quot;)\n  1353\t\n  1354\t        # 初始化性能监控变量\n  1355\t        frame_count = 0\n  1356\t        error_count = 0\n  1357\t        last_status_time = time.time()\n  1358\t\n  1359\t        while self.is_running:\n  1360\t            try:\n  1361\t                # 在循环开始时立即检查停止标志\n  1362\t                if not self.is_running:\n  1363\t                    break\n  1364\t\n  1365\t                start_time = datetime.datetime.now()\n  1366\t                res = ['-' for _ in range(9)]\n  1367\t\n  1368\t                # 初始化CNN检测结果变量，确保在整个循环中都有定义\n  1369\t                cnn_result = None\n  1370\t\n  1371\t                # 简单的摄像头状态检查\n  1372\t                if not hasattr(self, 'cap') or self.cap is None:\n  1373\t                    print(\&quot;摄像头未初始化\&quot;)\n  1374\t                    break\n  1375\t\n  1376\t                # 简化的摄像头检查 - 只在必要时检查\n  1377\t                if not self.cap.isOpened():\n  1378\t                    print(\&quot;摄像头连接丢失，尝试重新连接...\&quot;)\n  1379\t                    if not self._reconnect_camera():\n  1380\t                        print(\&quot;摄像头重连失败，停止检测\&quot;)\n  1381\t                        break\n  1382\t                    continue\n  1383\t\n  1384\t                # 优化的帧读取 - 减少卡顿\n  1385\t                flag, im_rd = self.cap.read()\n  1386\t\n  1387\t                if not flag or im_rd is None or im_rd.size == 0:\n  1388\t                    error_count += 1\n  1389\t                    # 减少错误报告频率\n  1390\t                    if error_count % 30 == 0:\n  1391\t                        print(f\&quot;读取帧失败，错误计数: {error_count}\&quot;)\n  1392\t\n  1393\t                    # 如果连续错误太多，尝试重连\n  1394\t                    if error_count &gt; 100:\n  1395\t                        print(\&quot;错误过多，尝试重连摄像头...\&quot;)\n  1396\t                        if not self._reconnect_camera():\n  1397\t                            break\n  1398\t                        error_count = 0\n  1399\t\n  1400\t                    # 使用配置文件中的延迟时间\n  1401\t                    time.sleep(camera_config.MAIN_LOOP_DELAY)\n  1402\t                    continue\n  1403\t\n  1404\t                # 成功读取帧，重置错误计数\n  1405\t                if error_count &gt; 0:\n  1406\t                    error_count = 0\n  1407\t\n  1408\t                frame_count += 1\n  1409\t\n  1410\t                # 跳帧处理 - 使用配置文件参数\n  1411\t                skip_detection = (frame_count % camera_config.FRAME_SKIP_DETECTION != 0)\n  1412\t\n  1413\t                # 每10秒报告一次状态，减少输出频率\n  1414\t                current_time_float = time.time()\n  1415\t                if current_time_float - last_status_time &gt;= 10.0:\n  1416\t                    fps = frame_count / (current_time_float - last_status_time)\n  1417\t                    print(f\&quot;处理状态: {frame_count} 帧, FPS: {fps:.1f}, 错误: {error_count}\&quot;)\n  1418\t                    frame_count = 0\n  1419\t                    last_status_time = current_time_float\n  1420\t\n  1421\t                # 改进的图像处理 - 增强人脸检测\n  1422\t                try:\n  1423\t                    # 验证图像格式和尺寸\n  1424\t                    if len(im_rd.shape) != 3 or im_rd.shape[2] != 3:\n  1425\t                        print(f\&quot;图像格式异常: {im_rd.shape}\&quot;)\n  1426\t                        continue\n  1427\t\n  1428\t                    height, width = im_rd.shape[:2]\n  1429\t                    if height &lt; 100 or width &lt; 100:\n  1430\t                        print(f\&quot;图像尺寸过小: {width}x{height}\&quot;)\n  1431\t                        continue\n  1432\t\n  1433\t                    # 应用暗图像增强 - 解决摄像头太暗的问题\n  1434\t                    im_rd = self._enhance_dark_frame(im_rd)\n  1435\t\n  1436\t                    # 转换为灰度图像\n  1437\t                    img_gray = cv2.cvtColor(im_rd, cv2.COLOR_BGR2GRAY)\n  1438\t\n  1439\t                    # 验证灰度图像\n  1440\t                    if img_gray is None or img_gray.size == 0:\n  1441\t                        print(\&quot;灰度转换失败\&quot;)\n  1442\t                        continue\n  1443\t\n  1444\t                    # 增强图像质量以提高人脸检测率\n  1445\t                    mean_brightness = np.mean(img_gray)\n  1446\t\n  1447\t                    # 创建增强版本的图像\n  1448\t                    enhanced_gray = img_gray.copy()\n  1449\t\n  1450\t                    # 增强的亮度调整 - 针对暗环境优化\n  1451\t                    if mean_brightness &lt; 120:  # 提高阈值，更积极地增强暗图像\n  1452\t                        # 图像太暗，大幅增加亮度和对比度\n  1453\t                        enhanced_gray = cv2.convertScaleAbs(enhanced_gray, alpha=1.6, beta=40)\n  1454\t\n  1455\t                        # 使用CLAHE进行局部对比度增强\n  1456\t                        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n  1457\t                        enhanced_gray = clahe.apply(enhanced_gray)\n  1458\t\n  1459\t                        #&lt;response clipped&gt;&lt;NOTE&gt;To save on context only part of this file has been shown to you.&lt;/NOTE&gt;\nTotal lines in file: 2432\n&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;67ba8c2a-ffdc-47e0-9a76-ac469d8768bc;toolu_01LQY4oCJQh7aJ3U8V5RcT9L&quot;:{&quot;requestId&quot;:&quot;67ba8c2a-ffdc-47e0-9a76-ac469d8768bc&quot;,&quot;toolUseId&quot;:&quot;toolu_01LQY4oCJQh7aJ3U8V5RcT9L&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;Regex search results for pattern: class YawnCNNLSTM|def __init__|CNN\\+LSTM|打哈欠检测 in Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\nFound 55 matching lines:\n\n     36\t\n     37\texcept ImportError as e:\n     38\t    print(f\&quot;TensorFlow导入失败: {e}\&quot;)\n     39\t    TENSORFLOW_AVAILABLE = False\n     40\t\n&gt;    41\t# 尝试导入PyTorch用于CNN+LSTM打哈欠检测\n     42\ttry:\n     43\t    import torch\n     44\t    import torch.nn as nn\n     45\t    PYTORCH_AVAILABLE = True\n     46\t    print(\&quot;✅ PyTorch可用，将启用CNN+LSTM打哈欠检测\&quot;)\n     47\texcept ImportError as e:\n     48\t    print(f\&quot;PyTorch导入失败: {e}\&quot;)\n     49\t    PYTORCH_AVAILABLE = False\n     50\t\n     51\t\n&gt;    52\t# CNN+LSTM模型定义（用于打哈欠检测）\n     53\tif PYTORCH_AVAILABLE:\n     54\t    class YawnCNNLSTM(nn.Module):\n     55\t        \&quot;\&quot;\&quot;专门用于打哈欠检测的CNN+LSTM模型\&quot;\&quot;\&quot;\n     56\t        def __init__(self, input_size, hidden_size, num_layers, output_size):\n     57\t            super(YawnCNNLSTM, self).__init__()\n     58\t\n     59\t            # CNN layers - Extract spatial features\n     60\t            self.cnn = nn.Sequential(\n     61\t                nn.Conv1d(input_size, 64, kernel_size=3, padding=1),\n...\n    119\t\n    120\t            return output\n    121\t\n    122\t\n    123\t    class YawnDetector:\n&gt;   124\t        \&quot;\&quot;\&quot;专门用于打哈欠检测的类\&quot;\&quot;\&quot;\n    125\t        def __init__(self, model_path=None, seq_length=30, consecutive_frames=15):\n    126\t            self.model = None\n    127\t            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if PYTORCH_AVAILABLE else None\n    128\t            self.seq_length = seq_length\n    129\t            self.features_buffer = []\n    130\t            self.is_available = False\n...\n    136\t\n    137\t            if PYTORCH_AVAILABLE and model_path and os.path.exists(model_path):\n    138\t                self.load_model(model_path)\n    139\t\n    140\t        def load_model(self, model_path):\n&gt;   141\t            \&quot;\&quot;\&quot;加载训练好的CNN+LSTM模型\&quot;\&quot;\&quot;\n    142\t            try:\n    143\t                # 初始化模型（与训练时的结构保持一致）\n    144\t                self.model = YawnCNNLSTM(\n    145\t                    input_size=138,  # 2 (EAR, MAR) + 68*2 (landmark coordinates)\n    146\t                    hidden_size=64,\n...\n    150\t\n    151\t                # 加载模型权重\n    152\t                self.model.load_state_dict(torch.load(model_path, map_location=self.device, weights_only=False))\n    153\t                self.model.eval()\n    154\t                self.is_available = True\n&gt;   155\t                print(f\&quot;✅ CNN+LSTM打哈欠检测模型加载成功: {model_path}\&quot;)\n    156\t\n    157\t            except Exception as e:\n    158\t                print(f\&quot;❌ CNN+LSTM打哈欠检测模型加载失败: {e}\&quot;)\n    159\t                self.model = None\n    160\t                self.is_available = False\n    161\t\n    162\t        def extract_features(self, landmarks, ear, mar, frame_height=480):\n    163\t            \&quot;\&quot;\&quot;提取特征向量（与real_pljc保持一致）\&quot;\&quot;\&quot;\n...\n    215\t                    is_fatigued = False\n    216\t\n    217\t                return is_fatigued, prediction\n    218\t\n    219\t            except Exception as e:\n&gt;   220\t                print(f\&quot;CNN+LSTM打哈欠预测失败: {e}\&quot;)\n    221\t                return False, 0.0\n    222\t\n    223\t        def reset_state(self):\n    224\t            \&quot;\&quot;\&quot;重置检测器状态（用于冷却期）\&quot;\&quot;\&quot;\n    225\t            self.fatigue_frames = 0\n    226\t            self.frame_count = 0\n    227\t\n    228\t\n    229\t# 如果PyTorch不可用，定义占位符类\n    230\tif not PYTORCH_AVAILABLE:\n&gt;   231\t    class YawnCNNLSTM:\n    232\t        def __init__(self, *args, **kwargs):\n    233\t            pass\n    234\t\n    235\t    class YawnDetector:\n    236\t        def __init__(self, *args, **kwargs):\n    237\t            self.is_available = False\n    238\t\n    239\t\n    240\t# 处理TensorFlow导入失败的情况\n    241\ttry:\n    242\t    # 这个try块是为了兼容原有的except ImportError\n    243\t    pass\n    244\texcept ImportError:\n    245\t    # 如果导入失败，定义基本占位符\n    246\t    class Sequential:\n&gt;   247\t        def __init__(self):\n    248\t            pass\n    249\t\n    250\t        def add(self, layer):\n    251\t            pass\n    252\t\n    253\t    class ResNet50:\n&gt;   254\t        def __init__(self, *args, **kwargs):\n    255\t            pass\n    256\t\n    257\t    class LSTM:\n    258\t        def __init__(self, *args, **kwargs):\n    259\t            pass\n    260\t\n    261\t    class Dense:\n    262\t        def __init__(self, *args, **kwargs):\n    263\t            pass\n    264\t\n    265\t    class Conv2D:\n    266\t        def __init__(self, *args, **kwargs):\n    267\t            pass\n    268\t\n    269\t    class MaxPooling2D:\n    270\t        def __init__(self, *args, **kwargs):\n    271\t            pass\n    272\t\n    273\t    class Flatten:\n    274\t        def __init__(self, *args, **kwargs):\n    275\t            pass\n    276\t\n    277\t    tf = None\n    278\t    TENSORFLOW_AVAILABLE = False\n    279\t\n...\n    284\t\n    285\tclass MainUI(QtWidgets.QWidget, main_ui.Ui_Form):\n    286\t    # 信号，在UI线程中，不能在其他线程直接操作UI\n    287\t    thread_signal = QtCore.Signal(dict)\n    288\t\n&gt;   289\t    def __init__(self):\n    290\t        super().__init__()\n    291\t        self.setupUi(self)\n    292\t\n    293\t        # 初始化摄像头列表\n    294\t        self.cameras = []\n...\n    312\t        self.lstm_model = None\n    313\t\n    314\t        # 初始化CNN疲劳检测器\n    315\t        self.cnn_detector = None\n    316\t\n&gt;   317\t        # 初始化CNN+LSTM打哈欠检测器\n    318\t        self.yawn_detector = None\n    319\t\n    320\t        if TENSORFLOW_AVAILABLE:\n    321\t            try:\n    322\t                self.init_models()\n...\n    324\t            except Exception as e:\n    325\t                print(f\&quot;模型初始化失败: {e}\&quot;)\n    326\t        else:\n    327\t            print(\&quot;TensorFlow未安装，使用简化功能\&quot;)\n    328\t\n&gt;   329\t        # 初始化CNN+LSTM打哈欠检测器\n    330\t        if PYTORCH_AVAILABLE:\n    331\t            try:\n    332\t                self.init_yawn_detector()\n    333\t            except Exception as e:\n    334\t                print(f\&quot;CNN+LSTM打哈欠检测器初始化失败: {e}\&quot;)\n    335\t\n    336\t        # 连接信号\n    337\t        # self.thread_signal.connect(self.thread_singnal_slot)\n    338\t\n    339\t        # 六个功能是否要用\n...\n    435\t        # 优化后的眨眼检测参数 - 进一步提高敏感度\n    436\t        self.EYE_AR_THRESH = 0.20  # 进一步降低阈值，提高敏感度（原0.22）\n    437\t        self.EYE_AR_CONSEC_FRAMES = 2  # 保持较低的连续帧要求\n    438\t        self.EYE_AR_UPPER_THRESH = 0.40  # 适当提高上限，避免过滤正常眨眼\n    439\t\n&gt;   440\t        # 优化后的打哈欠检测参数 - 进一步提高敏感度\n    441\t        self.MAR_THRESH = 0.40  # 进一步降低阈值，提高敏感度（原0.45）\n    442\t        self.MAR_DURATION_THRESH = 0.6  # 进一步降低哈欠持续时间阈值（秒）\n    443\t        self.MOUTH_AR_CONSEC_FRAMES = 2  # 减少连续帧要求\n    444\t\n    445\t        # 优化后的瞌睡点头检测参数\n...\n    473\t        self.baseline_mar = 0.4  # 基线MAR值（将动态调整）\n    474\t\n    475\t        # 打哈欠冷却机制\n    476\t        self.last_yawn_time = None  # 上次检测到打哈欠的时间\n    477\t        self.yawn_cooldown_seconds = 3.0  # 打哈欠冷却时间（秒）\n&gt;   478\t        self.yawn_detection_enabled = True  # 打哈欠检测是否启用\n    479\t\n    480\t        # 自适应阈值调整\n    481\t        self.calibration_frames = 0  # 校准帧数\n    482\t        self.calibration_period = 300  # 校准周期（帧数）\n    483\t        self.adaptive_mode = True  # 是否启用自适应模式\n...\n    842\t            self.cnn_detector = None\n    843\t\n    844\t    def _create_simple_detector(self):\n    845\t        \&quot;\&quot;\&quot;创建简化的疲劳检测器\&quot;\&quot;\&quot;\n    846\t        class SimpleFatigueDetector:\n&gt;   847\t            def __init__(self, resnet_model):\n    848\t                self.resnet = resnet_model\n    849\t                self.available = resnet_model is not None\n    850\t\n    851\t            def is_available(self):\n    852\t                return self.available\n...\n    888\t                    return None\n    889\t\n    890\t        return SimpleFatigueDetector(self.resnet)\n    891\t\n    892\t    def init_yawn_detector(self):\n&gt;   893\t        \&quot;\&quot;\&quot;初始化CNN+LSTM打哈欠检测器\&quot;\&quot;\&quot;\n    894\t        try:\n    895\t            # 检查是否有训练好的模型 - 现在模型文件在model文件夹中\n    896\t            model_path = './model/best_fatigue_model.pth'\n    897\t            if not os.path.exists(model_path):\n    898\t                # 尝试其他可能的路径\n...\n    905\t                model_path = 'D:/code/PythonProject2/real_pljc/models/best_fatigue_model.pth'\n    906\t\n    907\t            if os.path.exists(model_path):\n    908\t                self.yawn_detector = YawnDetector(model_path)\n    909\t                if self.yawn_detector.is_available:\n&gt;   910\t                    print(\&quot;✅ CNN+LSTM打哈欠检测器已加载\&quot;)\n    911\t                else:\n    912\t                    print(\&quot;⚠️ CNN+LSTM打哈欠检测器加载失败\&quot;)\n    913\t                    self.yawn_detector = None\n    914\t            else:\n    915\t                print(\&quot;⚠️ 未找到CNN+LSTM打哈欠检测模型\&quot;)\n    916\t                self.yawn_detector = None\n    917\t\n    918\t        except Exception as e:\n    919\t            print(f\&quot;CNN+LSTM打哈欠检测器初始化失败: {e}\&quot;)\n    920\t            self.yawn_detector = None\n    921\t\n    922\t    def _init_resnet(self):\n    923\t        \&quot;\&quot;\&quot;初始化ResNet50模型\&quot;\&quot;\&quot;\n    924\t        try:\n...\n   1183\t            print(f\&quot; 眨眼检测详情 - EAR: {ear:.3f}, 阈值: {thresh:.3f}, 是否眨眼: {is_blink}\&quot;)\n   1184\t\n   1185\t        return is_blink\n   1186\t\n   1187\t    def is_valid_yawn(self, mar, current_time):\n&gt;  1188\t        \&quot;\&quot;\&quot;改进的打哈欠检测 - 更宽松的条件\&quot;\&quot;\&quot;\n   1189\t        # 使用简化的阈值检测\n   1190\t        thresh = self.MAR_THRESH\n   1191\t\n   1192\t        is_mouth_open = mar &gt; thresh\n   1193\t\n...\n   1592\t                            except Exception as e:\n   1593\t                                if frame_count % 60 == 0:  # 减少错误信息频率\n   1594\t                                    print(f\&quot;CNN检测失败: {e}\&quot;)\n   1595\t                                cnn_result = None  # 确保在异常情况下重置为None\n   1596\t\n&gt;  1597\t                        # 预先计算EAR和MAR，因为CNN+LSTM模型需要这些值\n   1598\t                        # 提取左眼和右眼坐标\n   1599\t                        leftEye = shape[lStart:lEnd]\n   1600\t                        rightEye = shape[rStart:rEnd]\n   1601\t                        # 构造函数计算左右眼的EAR值，使用平均值作为最终的EAR\n   1602\t                        leftEAR = self.eye_aspect_ratio(leftEye)\n...\n   1609\t\n   1610\t                        # 获取当前时间（确保在所有地方都可用）\n   1611\t                        current_time = datetime.datetime.now()\n   1612\t\n   1613\t                        \&quot;\&quot;\&quot;\n&gt;  1614\t                        打哈欠 - 集成CNN+LSTM和启发式检测（带冷却机制）\n   1615\t                        \&quot;\&quot;\&quot;\n   1616\t                        if self.fun[1]:\n   1617\t                            # 使用cv2.convexHull获得凸包位置，使用drawContours画出轮廓位置进行画图操作\n   1618\t                            mouthHull = cv2.convexHull(mouth)\n   1619\t                            cv2.drawContours(im_rd, [mouthHull], -1, (0, 255, 0), 1)\n...\n   1628\t                                    cv2.putText(im_rd, f\&quot;Yawn Cooldown: {cooldown_remaining:.1f}s\&quot;, (10, 160),\n   1629\t                                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n   1630\t                                else:\n   1631\t                                    self.yawn_detection_enabled = True\n   1632\t\n&gt;  1633\t                            # CNN+LSTM打哈欠检测（只在启用时进行）\n   1634\t                            cnn_lstm_yawn_detected = False\n   1635\t                            cnn_lstm_confidence = 0.0\n   1636\t\n   1637\t                            if self.yawn_detector and self.yawn_detector.is_available:\n   1638\t                                try:\n...\n   1640\t                                    frame_height = im_rd.shape[0]\n   1641\t                                    features = self.yawn_detector.extract_features(shape, ear, mar, frame_height)\n   1642\t                                    if features is not None:\n   1643\t                                        self.yawn_detector.update_buffer(features)\n   1644\t\n&gt;  1645\t                                        # 进行CNN+LSTM预测（传递检测启用状态）\n   1646\t                                        cnn_lstm_yawn_detected, cnn_lstm_confidence = self.yawn_detector.predict_yawn(self.yawn_detection_enabled)\n   1647\t\n   1648\t                                        # 在界面上显示CNN+LSTM检测结果\n   1649\t                                        if frame_count % 10 == 0:  # 每10帧显示一次\n   1650\t                                            status = \&quot;ENABLED\&quot; if self.yawn_detection_enabled else \&quot;COOLDOWN\&quot;\n   1651\t                                            cnn_lstm_text = f\&quot;CNN+LSTM: {cnn_lstm_confidence:.2f} (连续:{self.yawn_detector.fatigue_frames}) [{status}]\&quot;\n   1652\t                                            color = (0, 0, 255) if cnn_lstm_yawn_detected else (0, 255, 0)\n   1653\t                                            cv2.putText(im_rd, cnn_lstm_text, (10, 140), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n   1654\t\n   1655\t                                except Exception as e:\n   1656\t                                    if frame_count % 60 == 0:  # 减少错误信息频率\n&gt;  1657\t                                        print(f\&quot;CNN+LSTM打哈欠检测失败: {e}\&quot;)\n   1658\t\n   1659\t                            # 只使用CNN+LSTM检测打哈欠（移除启发式检测）\n   1660\t                            yawn_detected = False\n   1661\t                            detection_method = \&quot;\&quot;\n   1662\t\n   1663\t                            # 注意：cnn_lstm_yawn_detected已经包含了连续帧判断\n   1664\t                            if cnn_lstm_yawn_detected:\n&gt;  1665\t                                # CNN+LSTM检测到打哈欠（已经通过连续帧验证）\n   1666\t                                yawn_detected = True\n   1667\t                                detection_method = \&quot;CNN+LSTM\&quot;\n   1668\t                                print(f\&quot; CNN+LSTM检测到打哈欠: 置信度={cnn_lstm_confidence:.3f}, 连续帧={self.yawn_detector.fatigue_frames}\&quot;)\n   1669\t\n   1670\t                            # 如果CNN+LSTM不可用，显示提示信息\n   1671\t                            if not (self.yawn_detector and self.yawn_detector.is_available):\n   1672\t                                # 在界面上显示CNN+LSTM不可用的提示\n   1673\t                                cv2.putText(im_rd, \&quot;CNN+LSTM Yawn Detection: UNAVAILABLE\&quot;, (10, 140),\n   1674\t                                          cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n   1675\t                                if self._yawn_debug_counter % 300 == 0:  # 每300帧提示一次\n   1676\t                                    print(\&quot;⚠️ CNN+LSTM打哈欠检测不可用，请检查模型文件和PyTorch安装\&quot;)\n   1677\t\n   1678\t                            # 添加哈欠检测调试信息\n   1679\t                            if not hasattr(self, '_yawn_debug_counter'):\n   1680\t                                self._yawn_debug_counter = 0\n   1681\t                            self._yawn_debug_counter += 1\n   1682\t                            if self._yawn_debug_counter % 60 == 0:  # 每60帧打印一次\n   1683\t                                if self.yawn_detector and self.yawn_detector.is_available:\n&gt;  1684\t                                    print(f\&quot; 哈欠检测 - MAR: {mar:.3f}, CNN+LSTM: {cnn_lstm_confidence:.3f}, 连续帧: {self.yawn_detector.fatigue_frames}\&quot;)\n   1685\t                                else:\n   1686\t                                    print(f\&quot; 哈欠检测 - CNN+LSTM不可用，跳过检测\&quot;)\n   1687\t\n   1688\t                            # 只在检测启用且检测到打哈欠时才计数和记录\n   1689\t                            if yawn_detected and self.yawn_detection_enabled:\n   1690\t                                self.mTOTAL += 1\n   1691\t                                print(f\&quot; 检测到哈欠！方法: {detection_method}, 总计: {self.mTOTAL}, MAR: {mar:.3f}, CNN+LSTM置信度: {cnn_lstm_confidence:.3f}\&quot;)\n   1692\t                                self.safe_emit_signal({'type':'msg','value':time.strftime('%Y-%m-%d %H:%M ', time.localtime()) + f\&quot;打哈欠({detection_method})\&quot;})\n   1693\t                                res[4] = '哈欠'\n   1694\t\n   1695\t                                # 设置冷却时间\n   1696\t                                self.last_yawn_time = current_time\n   1697\t                                self.yawn_detection_enabled = False\n&gt;  1698\t                                print(f\&quot; 打哈欠检测进入冷却期 {self.yawn_cooldown_seconds} 秒\&quot;)\n   1699\t\n   1700\t                                # 记录哈欠事件到统计数据库\n   1701\t                                if self.fatigue_stats:\n   1702\t                                    try:\n   1703\t                                        from fatigue_statistics import FatigueEvent\n&gt;  1704\t                                        # 使用CNN+LSTM的置信度\n   1705\t                                        confidence = cnn_lstm_confidence\n   1706\t                                        event = FatigueEvent(\n   1707\t                                            timestamp=current_time,\n   1708\t                                            event_type='yawn',\n   1709\t                                            value=1.0,\n\nTotal matches: 55\nTotal lines in file: 2432&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;ab16ec88-288b-4277-a058-d33621d1f7bc;toolu_014KkNVqKW6bXo5zogVj5Wsd&quot;:{&quot;requestId&quot;:&quot;ab16ec88-288b-4277-a058-d33621d1f7bc&quot;,&quot;toolUseId&quot;:&quot;toolu_014KkNVqKW6bXo5zogVj5Wsd&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;Here's the result of running `cat -n` on Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py:\n    54\t    class YawnCNNLSTM(nn.Module):\n    55\t        \&quot;\&quot;\&quot;专门用于打哈欠检测的CNN+LSTM模型\&quot;\&quot;\&quot;\n    56\t        def __init__(self, input_size, hidden_size, num_layers, output_size):\n    57\t            super(YawnCNNLSTM, self).__init__()\n    58\t\n    59\t            # CNN layers - Extract spatial features\n    60\t            self.cnn = nn.Sequential(\n    61\t                nn.Conv1d(input_size, 64, kernel_size=3, padding=1),\n    62\t                nn.BatchNorm1d(64),\n    63\t                nn.ReLU(),\n    64\t                nn.MaxPool1d(kernel_size=2),\n    65\t                nn.Conv1d(64, 128, kernel_size=3, padding=1),\n    66\t                nn.BatchNorm1d(128),\n    67\t                nn.ReLU(),\n    68\t                nn.MaxPool1d(kernel_size=2),\n    69\t                nn.Conv1d(128, 256, kernel_size=3, padding=1),\n    70\t                nn.BatchNorm1d(256),\n    71\t                nn.ReLU(),\n    72\t                nn.MaxPool1d(kernel_size=2)\n    73\t            )\n    74\t\n    75\t            # CNN output channels is 256\n    76\t            cnn_output_size = 256\n    77\t\n    78\t            # LSTM layers - Process temporal features\n    79\t            self.lstm = nn.LSTM(\n    80\t                input_size=cnn_output_size,\n    81\t                hidden_size=hidden_size,\n    82\t                num_layers=num_layers,\n    83\t                batch_first=True,\n    84\t                dropout=0.2,\n    85\t                bidirectional=True\n    86\t            )\n    87\t\n    88\t            # Fully connected layers - Output classification results\n    89\t            self.fc = nn.Sequential(\n    90\t                nn.Linear(hidden_size * 2, 128),\n    91\t                nn.ReLU(),\n    92\t                nn.Dropout(0.5),\n    93\t                nn.Linear(128, 64),\n    94\t                nn.ReLU(),\n    95\t                nn.Dropout(0.3),\n    96\t                nn.Linear(64, output_size)\n    97\t            )\n    98\t\n    99\t        def forward(self, x):\n   100\t            batch_size, seq_len, features = x.size()\n   101\t\n   102\t            # 重塑输入以通过CNN: (batch_size, features, seq_len)\n   103\t            x = x.permute(0, 2, 1)\n   104\t\n   105\t            # 通过CNN\n   106\t            cnn_out = self.cnn(x)\n   107\t\n   108\t            # 重塑CNN输出以通过LSTM: (batch_size, seq_len, cnn_features)\n   109\t            cnn_out = cnn_out.permute(0, 2, 1)\n   110\t\n   111\t            # 通过LSTM\n   112\t            lstm_out, _ = self.lstm(cnn_out)\n   113\t\n   114\t            # 取最后一个时间步的输出\n   115\t            lstm_out = lstm_out[:, -1, :]\n   116\t\n   117\t            # 通过全连接层\n   118\t            output = self.fc(lstm_out)\n   119\t\n   120\t            return output\n   121\t\nTotal lines in file: 2432\n&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;d3f64a10-0bce-41ec-8b24-fe0e60d0962e;toolu_019NN3GXj1PLQ8BypEs3jW9g&quot;:{&quot;requestId&quot;:&quot;d3f64a10-0bce-41ec-8b24-fe0e60d0962e&quot;,&quot;toolUseId&quot;:&quot;toolu_019NN3GXj1PLQ8BypEs3jW9g&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;Here's the result of running `cat -n` on Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py:\n   123\t    class YawnDetector:\n   124\t        \&quot;\&quot;\&quot;专门用于打哈欠检测的类\&quot;\&quot;\&quot;\n   125\t        def __init__(self, model_path=None, seq_length=30, consecutive_frames=15):\n   126\t            self.model = None\n   127\t            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if PYTORCH_AVAILABLE else None\n   128\t            self.seq_length = seq_length\n   129\t            self.features_buffer = []\n   130\t            self.is_available = False\n   131\t\n   132\t            # 添加连续帧判断逻辑（参考real_pljc）\n   133\t            self.consecutive_frames = consecutive_frames\n   134\t            self.fatigue_frames = 0\n   135\t            self.frame_count = 0\n   136\t\n   137\t            if PYTORCH_AVAILABLE and model_path and os.path.exists(model_path):\n   138\t                self.load_model(model_path)\n   139\t\n   140\t        def load_model(self, model_path):\n   141\t            \&quot;\&quot;\&quot;加载训练好的CNN+LSTM模型\&quot;\&quot;\&quot;\n   142\t            try:\n   143\t                # 初始化模型（与训练时的结构保持一致）\n   144\t                self.model = YawnCNNLSTM(\n   145\t                    input_size=138,  # 2 (EAR, MAR) + 68*2 (landmark coordinates)\n   146\t                    hidden_size=64,\n   147\t                    num_layers=1,\n   148\t                    output_size=1\n   149\t                ).to(self.device)\n   150\t\n   151\t                # 加载模型权重\n   152\t                self.model.load_state_dict(torch.load(model_path, map_location=self.device, weights_only=False))\n   153\t                self.model.eval()\n   154\t                self.is_available = True\n   155\t                print(f\&quot;✅ CNN+LSTM打哈欠检测模型加载成功: {model_path}\&quot;)\n   156\t\n   157\t            except Exception as e:\n   158\t                print(f\&quot;❌ CNN+LSTM打哈欠检测模型加载失败: {e}\&quot;)\n   159\t                self.model = None\n   160\t                self.is_available = False\n   161\t\n   162\t        def extract_features(self, landmarks, ear, mar, frame_height=480):\n   163\t            \&quot;\&quot;\&quot;提取特征向量（与real_pljc保持一致）\&quot;\&quot;\&quot;\n   164\t            if landmarks is None:\n   165\t                return None\n   166\t\n   167\t            try:\n   168\t                # 归一化关键点坐标（以鼻尖为基准，使用帧高度归一化）\n   169\t                nose = landmarks[30]  # 鼻尖关键点\n   170\t                normalized_landmarks = (landmarks - nose).flatten() / frame_height  # 使用帧高度归一化\n   171\t\n   172\t                # 组合特征：EAR, MAR + 68个关键点的x,y坐标\n   173\t                features = np.concatenate([[ear, mar], normalized_landmarks])\n   174\t                return features\n   175\t\n   176\t            except Exception as e:\n   177\t                print(f\&quot;特征提取失败: {e}\&quot;)\n   178\t                return None\n   179\t\n   180\t        def update_buffer(self, features):\n   181\t            \&quot;\&quot;\&quot;更新特征缓冲区\&quot;\&quot;\&quot;\n   182\t            if features is not None:\n   183\t                self.features_buffer.append(features)\n   184\t                if len(self.features_buffer) &gt; self.seq_length:\n   185\t                    self.features_buffer.pop(0)\n   186\t\n   187\t        def predict_yawn(self, detection_enabled=True):\n   188\t            \&quot;\&quot;\&quot;预测是否打哈欠（与real_pljc保持一致的逻辑）\&quot;\&quot;\&quot;\n   189\t            if not self.is_available or len(self.features_buffer) &lt; self.seq_length:\n   190\t                return False, 0.0\n   191\t\n   192\t            try:\n   193\t                # 准备输入序列\n   194\t                input_seq = np.array([self.features_buffer])\n   195\t                input_tensor = torch.FloatTensor(input_seq).to(self.device)\n   196\t\n   197\t                # 模型预测\n   198\t                with torch.no_grad():\n   199\t                    logits = self.model(input_tensor).item()\n   200\t                    prediction = torch.sigmoid(torch.tensor(logits)).item()\n   201\t\n   202\t                # 更新疲劳状态（参考real_pljc的连续帧判断逻辑）\n   203\t                self.frame_count += 1\n   204\t                if prediction &gt;= 0.5:  # 单帧预测阈值\n   205\t                    self.fatigue_frames += 1\n   206\t                else:\n   207\t                    self.fatigue_frames = 0\n   208\t\n   209\t                # 判定疲劳需要连续帧数达到阈值\n   210\t                is_fatigued = self.fatigue_frames &gt;= self.consecutive_frames\n   211\t\n   212\t                # 如果检测被禁用（冷却期），重置连续帧计数\n   213\t                if not detection_enabled and is_fatigued:\n   214\t                    self.fatigue_frames = 0\n   215\t                    is_fatigued = False\n   216\t\n   217\t                return is_fatigued, prediction\n   218\t\n   219\t            except Exception as e:\n   220\t                print(f\&quot;CNN+LSTM打哈欠预测失败: {e}\&quot;)\n   221\t                return False, 0.0\n   222\t\n   223\t        def reset_state(self):\n   224\t            \&quot;\&quot;\&quot;重置检测器状态（用于冷却期）\&quot;\&quot;\&quot;\n   225\t            self.fatigue_frames = 0\n   226\t            self.frame_count = 0\n   227\t\nTotal lines in file: 2432\n&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;e58c8682-c5e7-4a7d-a0b4-e603c4451c64;toolu_01Wg2YwP3S84X44Hj1MVjcVc&quot;:{&quot;requestId&quot;:&quot;e58c8682-c5e7-4a7d-a0b4-e603c4451c64&quot;,&quot;toolUseId&quot;:&quot;toolu_01Wg2YwP3S84X44Hj1MVjcVc&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;The following code sections were retrieved:\nPath: Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py\n...\n   161\t\n   162\t        def extract_features(self, landmarks, ear, mar, frame_height=480):\n   163\t            \&quot;\&quot;\&quot;提取特征向量（与real_pljc保持一致）\&quot;\&quot;\&quot;\n   164\t            if landmarks is None:\n   165\t                return None\n   166\t\n   167\t            try:\n   168\t                # 归一化关键点坐标（以鼻尖为基准，使用帧高度归一化）\n   169\t                nose = landmarks[30]  # 鼻尖关键点\n   170\t                normalized_landmarks = (landmarks - nose).flatten() / frame_height  # 使用帧高度归一化\n   171\t\n   172\t                # 组合特征：EAR, MAR + 68个关键点的x,y坐标\n   173\t                features = np.concatenate([[ear, mar], normalized_landmarks])\n   174\t                return features\n   175\t\n   176\t            except Exception as e:\n   177\t                print(f\&quot;特征提取失败: {e}\&quot;)\n   178\t                return None\n   179\t\n   180\t        def update_buffer(self, features):\n   181\t            \&quot;\&quot;\&quot;更新特征缓冲区\&quot;\&quot;\&quot;\n   182\t            if features is not None:\n   183\t                self.features_buffer.append(features)\n   184\t                if len(self.features_buffer) &gt; self.seq_length:\n   185\t                    self.features_buffer.pop(0)\n...\n   680\t\n   681\t    def _init_camera_robust(self, camera_index):\n   682\t        \&quot;\&quot;\&quot;简化的摄像头初始化方法 - 避免闪烁\&quot;\&quot;\&quot;\n   683\t        print(f\&quot;正在初始化摄像头 {camera_index}...\&quot;)\n   684\t\n   685\t        # 优先使用DirectShow后端（Windows最稳定）\n   686\t        try:\n   687\t            print(f\&quot;  使用DirectShow后端...\&quot;)\n   688\t            cap = cv2.VideoCapture(camera_index, cv2.CAP_DSHOW)\n   689\t\n   690\t            if cap.isOpened():\n   691\t                # 立即测试读取\n   692\t                ret, frame = cap.read()\n   693\t                if ret and frame is not None and frame.size &gt; 0:\n   694\t                    print(f\&quot;摄像头 {camera_index} 初始化成功\&quot;)\n   695\t                    return cap\n   696\t                else:\n   697\t                    print(f\&quot;  无法读取帧\&quot;)\n   698\t                    cap.release()\n   699\t            else:\n   700\t                print(f\&quot;  无法打开摄像头\&quot;)\n   701\t                if cap is not None:\n   702\t                    cap.release()\n   703\t\n   704\t        except Exception as e:\n   705\t            print(f\&quot;  DirectShow初始化失败: {e}\&quot;)\n   706\t            if 'cap' in locals() and cap is not None:\n   707\t                try:\n   708\t                    cap.release()\n   709\t                except:\n   710\t                    pass\n...\n  1244\t\n  1245\t            print(\&quot;✅ 检测器初始化成功\&quot;)\n  1246\t            print(f\&quot;   检测器类型: {type(self.detector)}\&quot;)\n  1247\t            print(f\&quot;   预测器类型: {type(self.predictor)}\&quot;)\n  1248\t\n  1249\t            data = {\n  1250\t                'type':'msg',\n  1251\t                'value':u\&quot;✅ 人脸检测模型加载成功!!!\\n\&quot;\n  1252\t            }\n  1253\t            self.safe_emit_signal(data)\n  1254\t\n  1255\t        except Exception as e:\n  1256\t            data = {\n  1257\t                'type':'msg',\n  1258\t                'value':f\&quot;❌ 模型初始化失败: {e}\\n\&quot;\n  1259\t            }\n  1260\t            self.safe_emit_signal(data)\n  1261\t            return\n  1262\t\n  1263\t        (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\&quot;left_eye\&quot;]\n  1264\t        (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\&quot;right_eye\&quot;]\n  1265\t        (mStart, mEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\&quot;mouth\&quot;]\n  1266\t\n  1267\t        self.cap = None\n  1268\t\n  1269\t        # 简化的摄像头初始化\n  1270\t        print(f\&quot;尝试打开摄像头，索引: {self.VIDEO_STREAM}\&quot;)\n  1271\t\n  1272\t        # 直接尝试打开摄像头，不过度复杂化\n  1273\t        success = False\n  1274\t        for camera_index in [self.VIDEO_STREAM, 0, 1]:  # 尝试当前索引、0、1\n  1275\t            try:\n  1276\t                print(f\&quot;  尝试摄像头索引 {camera_index}...\&quot;)\n  1277\t                self.cap = cv2.VideoCapture(camera_index, cv2.CAP_DSHOW)\n  1278\t\n  1279\t                if self.cap.isOpened():\n  1280\t                    # 基本设置\n  1281\t                    self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n  1282\t                    self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n  1283\t                    self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n  1284\t\n  1285\t                    # 测试读取\n  1286\t                    ret, test_frame = self.cap.read()\n  1287\t                    if ret and test_frame is not None:\n  1288\t                        print(f\&quot;  ✓ 摄像头 {camera_index} 工作正常\&quot;)\n  1289\t                        self.VIDEO_STREAM = camera_index\n  1290\t                        self.CAMERA_STYLE = True\n  1291\t                        success = True\n  1292\t                        data['value'] = f\&quot;打开摄像头成功(索引{camera_index})!!!\&quot;\n  1293\t                        break\n  1294\t                    else:\n  1295\t                        print(f\&quot;  ✗ 摄像头 {camera_index} 无法读取\&quot;)\n  1296\t                        self.cap.release()\n  1297\t                        self.cap = None\n  1298\t                else:\n  1299\t                    print(f\&quot;  ✗ 摄像头 {camera_index} 无法打开\&quot;)\n  1300\t                    if self.cap is not None:\n  1301\t                        self.cap.release()\n  1302\t                        self.cap = None\n  1303\t\n  1304\t            except Exception as e:\n  1305\t                print(f\&quot;  ✗ 摄像头 {camera_index} 异常: {e}\&quot;)\n  1306\t                if hasattr(self, 'cap') and self.cap is not None:\n  1307\t                    try:\n  1308\t                        self.cap.release()\n  1309\t                    except:\n  1310\t                        pass\n  1311\t                    self.cap = None\n  1312\t\n  1313\t        if not success:\n  1314\t            data['value'] = u\&quot;摄像头打开失败!!!\&quot;\n  1315\t            print(\&quot;未找到可用的摄像头设备\&quot;)\n  1316\t        else:\n  1317\t            # 简化的参数设置\n  1318\t            self._optimize_camera_brightness()\n  1319\t        self.safe_emit_signal(data)\n  1320\t\n  1321\t        # 打印最终摄像头状态\n  1322\t        if hasattr(self, 'cap') and self.cap is not None and self.cap.isOpened():\n  1323\t            width = self.cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n  1324\t            height = self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n  1325\t            fps = self.cap.get(cv2.CAP_PROP_FPS)\n  1326\t            backend = self.cap.getBackendName()\n  1327\t            print(f\&quot;摄像头状态: 已打开\&quot;)\n  1328\t            print(f\&quot;  分辨率: {width}x{height}\&quot;)\n  1329\t            print(f\&quot;  设置FPS: {fps}\&quot;)\n  1330\t            print(f\&quot;  后端: {backend}\&quot;)\n  1331\t\n  1332\t            # 如果FPS为0，尝试手动设置\n  1333\t            if fps == 0:\n  1334\t                print(\&quot;检测到FPS为0，尝试手动设置...\&quot;)\n  1335\t                self.cap.set(cv2.CAP_PROP_FPS, 30)\n  1336\t                new_fps = self.cap.get(cv2.CAP_PROP_FPS)\n  1337\t                print(f\&quot;  重新设置后FPS: {new_fps}\&quot;)\n  1338\t        else:\n  1339\t            print(\&quot;未找到可用的视频源\&quot;)\n  1340\t\n  1341\t        # 初始化FPS计算变量\n  1342\t        self.frame_count = 0\n  1343\t        self.fps_start_time = time.time()\n  1344\t        self.actual_fps = 0\n  1345\t\n  1346\t        # 初始化时间变量\n  1347\t        t_time = datetime.datetime.now()\n  1348\t        e_time = datetime.datetime.now()\n  1349\t        h_time = datetime.datetime.now()\n  1350\t\n  1351\t        # 成功打开视频，循环读取视频流\n  1352\t        print(\&quot;开始视频流处理...\&quot;)\n  1353\t\n  1354\t        # 初始化性能监控变量\n  1355\t        frame_count = 0\n  1356\t        error_count = 0\n  1357\t        last_status_time = time.time()\n  1358\t\n  1359\t        while self.is_running:\n  1360\t            try:\n  1361\t                # 在循环开始时立即检查停止标志\n  1362\t                if not self.is_running:\n  1363\t                    break\n  1364\t\n  1365\t                start_time = datetime.datetime.now()\n  1366\t                res = ['-' for _ in range(9)]\n  1367\t\n  1368\t                # 初始化CNN检测结果变量，确保在整个循环中都有定义\n  1369\t                cnn_result = None\n  1370\t\n  1371\t                # 简单的摄像头状态检查\n  1372\t                if not hasattr(self, 'cap') or self.cap is None:\n  1373\t                    print(\&quot;摄像头未初始化\&quot;)\n  1374\t                    break\n  1375\t\n  1376\t                # 简化的摄像头检查 - 只在必要时检查\n  1377\t                if not self.cap.isOpened():\n  1378\t                    print(\&quot;摄像头连接丢失，尝试重新连接...\&quot;)\n  1379\t                    if not self._reconnect_camera():\n  1380\t                        print(\&quot;摄像头重连失败，停止检测\&quot;)\n  1381\t                        break\n  1382\t                    continue\n...\n  1478\t\n  1479\t                # 使用跳帧策略：只在特定帧进行人脸检测，其他帧使用缓存结果\n  1480\t                if not skip_detection:\n  1481\t                    try:\n  1482\t                        # 检查检测器是否已初始化\n  1483\t                        if not hasattr(self, 'detector') or self.detector is None:\n  1484\t                            if frame_count % 60 == 0:  # 减少错误信息频率\n  1485\t                                print(\&quot;❌ 人脸检测器未初始化\&quot;)\n  1486\t                            faces = []\n  1487\t                        else:\n  1488\t                            # 多策略人脸检测，提高检测成功率\n  1489\t                            faces = []\n  1490\t\n  1491\t                            # 策略1: 使用增强后的图像，不同上采样级别\n  1492\t                            for upsample in [0, 1]:  # 先尝试0（更快），再尝试1\n  1493\t                                if len(faces) == 0:\n  1494\t                                    try:\n  1495\t                                        faces = self.detector(enhanced_gray, upsample)\n  1496\t                                        if len(faces) &gt; 0:\n  1497\t                                            break\n  1498\t                                    except:\n  1499\t                                        continue\n  1500\t\n  1501\t                            # 策略2: 如果增强图像失败，尝试原始灰度图像\n  1502\t                            if len(faces) == 0:\n  1503\t                                try:\n  1504\t                                    faces = self.detector(img_gray, 0)\n  1505\t                                except:\n  1506\t                                    pass\n  1507\t\n  1508\t                            # 策略3: 尝试直方图均衡化\n  1509\t                            if len(faces) == 0:\n  1510\t                                try:\n  1511\t                                    equalized = cv2.equalizeHist(img_gray)\n  1512\t                                    faces = self.detector(equalized, 0)\n  1513\t                                except:\n  1514\t                                    pass\n...\n  1553\t\n  1554\t                    except Exception as e:\n  1555\t                        if frame_count % 60 == 0:  # 减少错误信息频率\n  1556\t                            print(f\&quot;人脸检测失败: {e}\&quot;)\n  1557\t                        faces = []\n  1558\t                else:\n  1559\t                    # 跳帧时使用上次的检测结果\n  1560\t                    if hasattr(self, 'last_faces'):\n  1561\t                        faces = self.last_faces\n  1562\t                    else:\n  1563\t                        faces = []\n  1564\t                # 如果检测到人脸\n  1565\t                if (len(faces) != 0):\n  1566\t                    res[0] = '识别到人脸'\n  1567\t                    # enumerate方法同时返回数据对象的索引和数据，k为索引，d为faces中的对象\n  1568\t                    for _, d in enumerate(faces):\n  1569\t                        # 用红色矩形框出人脸\n  1570\t                        cv2.rectangle(im_rd, (d.left(), d.top()), (d.right(), d.bottom()), (0, 0, 255), 1)\n  1571\t                        # 使用预测器得到68点数据的坐标\n  1572\t                        shape = self.predictor(im_rd, d)\n  1573\t                        # 圆圈显示每个特征点\n  1574\t                        for i in range(68):\n  1575\t                            cv2.circle(im_rd, (shape.part(i).x, shape.part(i).y), 2, (0, 255, 0), -1, 8)\n  1576\t                        # 将脸部特征信息转换为数组array的格式\n  1577\t                        shape = face_utils.shape_to_np(shape)\n  1578\t\n  1579\t                        # 提取人脸区域用于CNN分析\n  1580\t                        face_img = im_rd[d.top():d.bottom(), d.left():d.right()]\n...\n  1636\t\n  1637\t                            if self.yawn_detector and self.yawn_detector.is_available:\n  1638\t                                try:\n  1639\t                                    # 提取特征并更新缓冲区（传递帧高度用于归一化）\n  1640\t                                    frame_height = im_rd.shape[0]\n  1641\t                                    features = self.yawn_detector.extract_features(shape, ear, mar, frame_height)\n  1642\t                                    if features is not None:\n  1643\t                                        self.yawn_detector.update_buffer(features)\n  1644\t\n  1645\t                                        # 进行CNN+LSTM预测（传递检测启用状态）\n  1646\t                                        cnn_lstm_yawn_detected, cnn_lstm_confidence = self.yawn_detector.predict_yawn(self.yawn_detection_enabled)\n...\n  2018\t\n  2019\t                # 优化的图像显示处理 - 减少卡顿\n  2020\t                try:\n  2021\t                    # 验证图像有效性\n  2022\t                    if im_rd is None or im_rd.size == 0:\n  2023\t                        if frame_count % 60 == 0:  # 减少错误信息频率\n  2024\t                            print(\&quot;图像数据无效，跳过显示\&quot;)\n  2025\t                        continue\n  2026\t\n  2027\t                    height, width = im_rd.shape[:2]\n  2028\t\n  2029\t                    # 确保图像有效\n  2030\t                    if height &gt; 0 and width &gt; 0:\n  2031\t                        # 转换颜色空间从BGR到RGB\n  2032\t                        RGBImg = cv2.cvtColor(im_rd, cv2.COLOR_BGR2RGB)\n  2033\t\n  2034\t                        # 验证转换后的图像\n  2035\t                        if RGBImg is None or RGBImg.size == 0:\n  2036\t                            if frame_count % 60 == 0:  # 减少错误信息频率\n  2037\t                                print(\&quot;RGB转换失败\&quot;)\n  2038\t                            continue\n  2039\t\n  2040\t                        # 减少调试信息的频率，避免控制台刷屏\n  2041\t                        if self.frame_count % 300 == 0:  # 每300帧打印一次（约每10秒）\n  2042\t                            print(f\&quot;发送图像数据: {width}x{height}, 人脸数: {len(faces)}\&quot;)\n  2043\t\n  2044\t                        # 发送图像数据到UI线程 - 确保视频流畅显示\n  2045\t                        # 每帧都发送图像数据以保证视频连续性\n  2046\t                        data = {'type':'img','value':RGBImg}\n  2047\t                        self.safe_emit_signal(data)\n  2048\t                    else:\n  2049\t                        if frame_count % 60 == 0:  # 减少错误信息频率\n  2050\t                            print(\&quot;图像尺寸无效，跳过显示\&quot;)\n...\nPath: Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/fatigue_gui.py\n...\n   339\t\n   340\t        faces = np.array(list(self.face_buffer))\n   341\t        landmarks = np.array(list(self.landmark_buffer))\n   342\t\n   343\t        faces_tensor = torch.from_numpy(faces).float().unsqueeze(0)\n   344\t        landmarks_tensor = torch.from_numpy(landmarks).float().unsqueeze(0)\n   345\t\n   346\t        faces_tensor = faces_tensor.permute(0, 1, 4, 2, 3)\n   347\t        landmarks_tensor = landmarks_tensor.reshape(1, SEQUENCE_LENGTH, -1)\n   348\t        faces_tensor = faces_tensor / 255.0\n   349\t\n   350\t        faces_tensor = faces_tensor.to(self.device)\n   351\t        landmarks_tensor = landmarks_tensor.to(self.device)\n   352\t\n   353\t        with torch.no_grad():\n   354\t            outputs = self.model(faces_tensor, landmarks_tensor)\n   355\t            probabilities = torch.softmax(outputs, dim=1)\n   356\t            yawn_prob = probabilities[0, 1].item()\n   357\t            prediction = 1 if yawn_prob &gt; self.yawn_threshold.get() else 0\n   358\t\n   359\t        return yawn_prob, prediction\n...\nPath: Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/data_preprocessing.py\n...\n    71\t        \n    72\t        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    73\t        fps = cap.get(cv2.CAP_PROP_FPS)\n    74\t        \n    75\t        self.logger.info(f\&quot;处理视频: {video_path}, 总帧数: {total_frames}, FPS: {fps}\&quot;)\n    76\t        \n    77\t        # 提取所有帧的人脸和特征点\n    78\t        faces = []\n    79\t        landmarks = []\n    80\t        frame_indices = []\n    81\t        \n    82\t        frame_idx = 0\n    83\t\n    84\t        # 调试模式下限制处理的帧数\n    85\t        if DEBUG_MODE:\n    86\t            total_frames = min(total_frames, DEBUG_MAX_FRAMES)\n    87\t            self.logger.info(f\&quot;调试模式：限制处理帧数为 {total_frames}\&quot;)\n    88\t\n    89\t        pbar = tqdm(total=total_frames, desc=f\&quot;提取特征\&quot;)\n    90\t\n    91\t        while True:\n    92\t            ret, frame = cap.read()\n    93\t            if not ret:\n    94\t                break\n    95\t\n    96\t            # 调试模式下的帧数限制\n    97\t            if DEBUG_MODE and frame_idx &gt;= DEBUG_MAX_FRAMES:\n    98\t                break\n    99\t            \n   100\t            # 提取人脸和特征点\n   101\t            face_img, face_landmarks = extract_face_landmarks(frame, self.detector, self.predictor)\n...\nPath: Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/simple_cnn_detector.py\n...\n   125\t        \n   126\t        try:\n   127\t            # 预测\n   128\t            predictions = self.model.predict(processed_image, verbose=0)\n   129\t            predicted_class_idx = np.argmax(predictions[0])\n   130\t            confidence = float(predictions[0][predicted_class_idx])\n   131\t            \n   132\t            # 获取类别名称\n   133\t            class_name = None\n   134\t            for name, idx in self.class_indices.items():\n   135\t                if idx == predicted_class_idx:\n   136\t                    class_name = name\n   137\t                    break\n   138\t            \n   139\t            if class_name is None:\n   140\t                class_name = f\&quot;class_{predicted_class_idx}\&quot;\n   141\t            \n   142\t            # 添加到历史记录\n   143\t            self.prediction_history.append({\n   144\t                'class': class_name,\n   145\t                'confidence': confidence\n   146\t            })\n   147\t            \n   148\t            # 保持历史记录长度\n   149\t            if len(self.prediction_history) &gt; self.history_length:\n   150\t                self.prediction_history.pop(0)\n   151\t            \n   152\t            # 计算平滑后的结果\n   153\t            smoothed_result = self._smooth_predictions()\n   154\t            \n   155\t            result = {\n   156\t                'predicted_class': class_name,\n   157\t                'confidence': confidence,\n   158\t                'smoothed_class': smoothed_result['class'],\n   159\t                'smoothed_confidence': smoothed_result['confidence'],\n   160\t                'fatigue_level': self._map_to_fatigue_level(smoothed_result['class']),\n   161\t                'all_predictions': predictions[0].tolist()\n   162\t            }\n   163\t            \n   164\t            return result\n   165\t            \n   166\t        except Exception as e:\n   167\t            print(f\&quot;疲劳预测失败: {e}\&quot;)\n   168\t            return None\n...\n   217\t    \n   218\t    def get_fatigue_analysis(self, face_image: np.ndarray) -&gt; Optional[Dict]:\n   219\t        \&quot;\&quot;\&quot;\n   220\t        获取疲劳分析结果\n   221\t        \n   222\t        Args:\n   223\t            face_image: 人脸图像\n   224\t            \n   225\t        Returns:\n   226\t            疲劳分析结果\n   227\t        \&quot;\&quot;\&quot;\n   228\t        prediction_result = self.predict_fatigue(face_image)\n   229\t        \n   230\t        if prediction_result is None:\n   231\t            return None\n   232\t        \n   233\t        # 构建分析结果\n   234\t        analysis = {\n   235\t            'fatigue_detected': prediction_result['smoothed_class'] != 'alert',\n   236\t            'fatigue_level': prediction_result['fatigue_level'],\n   237\t            'confidence': prediction_result['smoothed_confidence'],\n   238\t            'raw_prediction': prediction_result['predicted_class'],\n   239\t            'recommendation': self._generate_recommendation(prediction_result['fatigue_level'])\n   240\t        }\n   241\t        \n   242\t        return analysis\n...\n&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;8942d239-c747-4a02-be4f-859d68159c94;toolu_01FU8ztzTwKkejJrorkDB4fK&quot;:{&quot;requestId&quot;:&quot;8942d239-c747-4a02-be4f-859d68159c94&quot;,&quot;toolUseId&quot;:&quot;toolu_01FU8ztzTwKkejJrorkDB4fK&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;Here's the result of running `cat -n` on Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py:\n  1383\t\n  1384\t                # 优化的帧读取 - 减少卡顿\n  1385\t                flag, im_rd = self.cap.read()\n  1386\t\n  1387\t                if not flag or im_rd is None or im_rd.size == 0:\n  1388\t                    error_count += 1\n  1389\t                    # 减少错误报告频率\n  1390\t                    if error_count % 30 == 0:\n  1391\t                        print(f\&quot;读取帧失败，错误计数: {error_count}\&quot;)\n  1392\t\n  1393\t                    # 如果连续错误太多，尝试重连\n  1394\t                    if error_count &gt; 100:\n  1395\t                        print(\&quot;错误过多，尝试重连摄像头...\&quot;)\n  1396\t                        if not self._reconnect_camera():\n  1397\t                            break\n  1398\t                        error_count = 0\n  1399\t\n  1400\t                    # 使用配置文件中的延迟时间\n  1401\t                    time.sleep(camera_config.MAIN_LOOP_DELAY)\n  1402\t                    continue\n  1403\t\n  1404\t                # 成功读取帧，重置错误计数\n  1405\t                if error_count &gt; 0:\n  1406\t                    error_count = 0\n  1407\t\n  1408\t                frame_count += 1\n  1409\t\n  1410\t                # 跳帧处理 - 使用配置文件参数\n  1411\t                skip_detection = (frame_count % camera_config.FRAME_SKIP_DETECTION != 0)\n  1412\t\n  1413\t                # 每10秒报告一次状态，减少输出频率\n  1414\t                current_time_float = time.time()\n  1415\t                if current_time_float - last_status_time &gt;= 10.0:\n  1416\t                    fps = frame_count / (current_time_float - last_status_time)\n  1417\t                    print(f\&quot;处理状态: {frame_count} 帧, FPS: {fps:.1f}, 错误: {error_count}\&quot;)\n  1418\t                    frame_count = 0\n  1419\t                    last_status_time = current_time_float\n  1420\t\n  1421\t                # 改进的图像处理 - 增强人脸检测\n  1422\t                try:\n  1423\t                    # 验证图像格式和尺寸\n  1424\t                    if len(im_rd.shape) != 3 or im_rd.shape[2] != 3:\n  1425\t                        print(f\&quot;图像格式异常: {im_rd.shape}\&quot;)\n  1426\t                        continue\n  1427\t\n  1428\t                    height, width = im_rd.shape[:2]\n  1429\t                    if height &lt; 100 or width &lt; 100:\n  1430\t                        print(f\&quot;图像尺寸过小: {width}x{height}\&quot;)\n  1431\t                        continue\n  1432\t\n  1433\t                    # 应用暗图像增强 - 解决摄像头太暗的问题\n  1434\t                    im_rd = self._enhance_dark_frame(im_rd)\n  1435\t\n  1436\t                    # 转换为灰度图像\n  1437\t                    img_gray = cv2.cvtColor(im_rd, cv2.COLOR_BGR2GRAY)\n  1438\t\n  1439\t                    # 验证灰度图像\n  1440\t                    if img_gray is None or img_gray.size == 0:\n  1441\t                        print(\&quot;灰度转换失败\&quot;)\n  1442\t                        continue\n  1443\t\n  1444\t                    # 增强图像质量以提高人脸检测率\n  1445\t                    mean_brightness = np.mean(img_gray)\n  1446\t\n  1447\t                    # 创建增强版本的图像\n  1448\t                    enhanced_gray = img_gray.copy()\n  1449\t\n  1450\t                    # 增强的亮度调整 - 针对暗环境优化\nTotal lines in file: 2432\n&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;1fde2629-4000-4e12-bf9e-fa0c857db00f;toolu_01CMsNEpUWJsn7KEHpLquH5J&quot;:{&quot;requestId&quot;:&quot;1fde2629-4000-4e12-bf9e-fa0c857db00f&quot;,&quot;toolUseId&quot;:&quot;toolu_01CMsNEpUWJsn7KEHpLquH5J&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;Here's the result of running `cat -n` on Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py:\n  1565\t                if (len(faces) != 0):\n  1566\t                    res[0] = '识别到人脸'\n  1567\t                    # enumerate方法同时返回数据对象的索引和数据，k为索引，d为faces中的对象\n  1568\t                    for _, d in enumerate(faces):\n  1569\t                        # 用红色矩形框出人脸\n  1570\t                        cv2.rectangle(im_rd, (d.left(), d.top()), (d.right(), d.bottom()), (0, 0, 255), 1)\n  1571\t                        # 使用预测器得到68点数据的坐标\n  1572\t                        shape = self.predictor(im_rd, d)\n  1573\t                        # 圆圈显示每个特征点\n  1574\t                        for i in range(68):\n  1575\t                            cv2.circle(im_rd, (shape.part(i).x, shape.part(i).y), 2, (0, 255, 0), -1, 8)\n  1576\t                        # 将脸部特征信息转换为数组array的格式\n  1577\t                        shape = face_utils.shape_to_np(shape)\n  1578\t\n  1579\t                        # 提取人脸区域用于CNN分析\n  1580\t                        face_img = im_rd[d.top():d.bottom(), d.left():d.right()]\n  1581\t\n  1582\t                        # 使用CNN进行疲劳检测 - 降低调用频率以提高性能\n  1583\t                        # 只在每10帧调用一次CNN检测，减少计算负担\n  1584\t                        if (self.cnn_detector and self.cnn_detector.is_available() and\n  1585\t                            frame_count % 10 == 0):\n  1586\t                            try:\n  1587\t                                cnn_result = self.cnn_detector.predict_fatigue(face_img)\n  1588\t                                if cnn_result:\n  1589\t                                    # 在界面上显示CNN检测结果\n  1590\t                                    cnn_text = f\&quot;CNN: {cnn_result['fatigue_level']} ({cnn_result['confidence']:.2f})\&quot;\n  1591\t                                    cv2.putText(im_rd, cnn_text, (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n  1592\t                            except Exception as e:\n  1593\t                                if frame_count % 60 == 0:  # 减少错误信息频率\n  1594\t                                    print(f\&quot;CNN检测失败: {e}\&quot;)\n  1595\t                                cnn_result = None  # 确保在异常情况下重置为None\n  1596\t\n  1597\t                        # 预先计算EAR和MAR，因为CNN+LSTM模型需要这些值\n  1598\t                        # 提取左眼和右眼坐标\n  1599\t                        leftEye = shape[lStart:lEnd]\n  1600\t                        rightEye = shape[rStart:rEnd]\n  1601\t                        # 构造函数计算左右眼的EAR值，使用平均值作为最终的EAR\n  1602\t                        leftEAR = self.eye_aspect_ratio(leftEye)\n  1603\t                        rightEAR = self.eye_aspect_ratio(rightEye)\n  1604\t                        ear = (leftEAR + rightEAR) / 2.0\n  1605\t\n  1606\t                        # 嘴巴坐标和MAR计算\n  1607\t                        mouth = shape[mStart:mEnd]\n  1608\t                        mar = self.mouth_aspect_ratio(mouth)\n  1609\t\n  1610\t                        # 获取当前时间（确保在所有地方都可用）\n  1611\t                        current_time = datetime.datetime.now()\n  1612\t\n  1613\t                        \&quot;\&quot;\&quot;\n  1614\t                        打哈欠 - 集成CNN+LSTM和启发式检测（带冷却机制）\n  1615\t                        \&quot;\&quot;\&quot;\n  1616\t                        if self.fun[1]:\n  1617\t                            # 使用cv2.convexHull获得凸包位置，使用drawContours画出轮廓位置进行画图操作\n  1618\t                            mouthHull = cv2.convexHull(mouth)\n  1619\t                            cv2.drawContours(im_rd, [mouthHull], -1, (0, 255, 0), 1)\n  1620\t\n  1621\t                            # 检查打哈欠冷却状态\n  1622\t                            if self.last_yawn_time is not None:\n  1623\t                                time_since_last_yawn = (current_time - self.last_yawn_time).total_seconds()\n  1624\t                                if time_since_last_yawn &lt; self.yawn_cooldown_seconds:\n  1625\t                                    self.yawn_detection_enabled = False\n  1626\t                                    # 显示冷却状态\n  1627\t                                    cooldown_remaining = self.yawn_cooldown_seconds - time_since_last_yawn\n  1628\t                                    cv2.putText(im_rd, f\&quot;Yawn Cooldown: {cooldown_remaining:.1f}s\&quot;, (10, 160),\n  1629\t                                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n  1630\t                                else:\n  1631\t                                    self.yawn_detection_enabled = True\n  1632\t\n  1633\t                            # CNN+LSTM打哈欠检测（只在启用时进行）\n  1634\t                            cnn_lstm_yawn_detected = False\n  1635\t                            cnn_lstm_confidence = 0.0\n  1636\t\n  1637\t                            if self.yawn_detector and self.yawn_detector.is_available:\n  1638\t                                try:\n  1639\t                                    # 提取特征并更新缓冲区（传递帧高度用于归一化）\n  1640\t                                    frame_height = im_rd.shape[0]\n  1641\t                                    features = self.yawn_detector.extract_features(shape, ear, mar, frame_height)\n  1642\t                                    if features is not None:\n  1643\t                                        self.yawn_detector.update_buffer(features)\n  1644\t\n  1645\t                                        # 进行CNN+LSTM预测（传递检测启用状态）\n  1646\t                                        cnn_lstm_yawn_detected, cnn_lstm_confidence = self.yawn_detector.predict_yawn(self.yawn_detection_enabled)\n  1647\t\n  1648\t                                        # 在界面上显示CNN+LSTM检测结果\n  1649\t                                        if frame_count % 10 == 0:  # 每10帧显示一次\n  1650\t                                            status = \&quot;ENABLED\&quot; if self.yawn_detection_enabled else \&quot;COOLDOWN\&quot;\nTotal lines in file: 2432\n&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;c1ec9173-4140-4043-805a-85890e042c64;toolu_01D44TKNjazPjYXfoKMZiCwU&quot;:{&quot;requestId&quot;:&quot;c1ec9173-4140-4043-805a-85890e042c64&quot;,&quot;toolUseId&quot;:&quot;toolu_01D44TKNjazPjYXfoKMZiCwU&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;Here's the result of running `cat -n` on Python_ConvolutionalNeuralNetwork_FaceRecognition_fatiguedriving-master/main.py:\n   187\t        def predict_yawn(self, detection_enabled=True):\n   188\t            \&quot;\&quot;\&quot;预测是否打哈欠\&quot;\&quot;\&quot;\n   189\t            if not self.is_available or len(self.features_buffer) &lt; self.seq_length:\n   190\t                return False, 0.0\n   191\t\n   192\t            try:\n   193\t                # 准备输入序列\n   194\t                input_seq = np.array([self.features_buffer])\n   195\t                input_tensor = torch.FloatTensor(input_seq).to(self.device)\n   196\t\n   197\t                # 模型预测\n   198\t                with torch.no_grad():\n   199\t                    logits = self.model(input_tensor).item()\n   200\t                    prediction = torch.sigmoid(torch.tensor(logits)).item()\n   201\t\n   202\t                # 更新疲劳状态（参考real_pljc的连续帧判断逻辑）\n   203\t                self.frame_count += 1\n   204\t                if prediction &gt;= 0.5:  # 单帧预测阈值\n   205\t                    self.fatigue_frames += 1\n   206\t                else:\n   207\t                    self.fatigue_frames = 0\n   208\t\n   209\t                # 判定疲劳需要连续帧数达到阈值\n   210\t                is_fatigued = self.fatigue_frames &gt;= self.consecutive_frames\n   211\t\n   212\t                # 如果检测被禁用（冷却期），重置连续帧计数\n   213\t                if not detection_enabled and is_fatigued:\n   214\t                    self.fatigue_frames = 0\n   215\t                    is_fatigued = False\n   216\t\n   217\t                return is_fatigued, prediction\n   218\t\n   219\t            except Exception as e:\n   220\t                print(f\&quot;CNN+LSTM打哈欠预测失败: {e}\&quot;)\n   221\t                return False, 0.0\nTotal lines in file: 2432\n&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}}},&quot;draftExchange&quot;:{&quot;request_message&quot;:&quot;&quot;,&quot;rich_text_json_repr&quot;:{&quot;type&quot;:&quot;doc&quot;,&quot;content&quot;:[{&quot;type&quot;:&quot;paragraph&quot;}]},&quot;status&quot;:&quot;draft&quot;},&quot;requestIds&quot;:[],&quot;isPinned&quot;:false,&quot;isShareable&quot;:true,&quot;extraData&quot;:{&quot;isAgentConversation&quot;:true,&quot;baselineTimestamp&quot;:0,&quot;hasDirtyEdits&quot;:false},&quot;personaType&quot;:0,&quot;rootTaskUuid&quot;:&quot;10d40be5-d6c0-42ae-a03a-2163a7ddbb0c&quot;}},&quot;agentExecutionMode&quot;:&quot;manual&quot;,&quot;isPanelCollapsed&quot;:true,&quot;displayedAnnouncements&quot;:[]}" />
      </map>
    </option>
  </component>
</project>