import datetime
import math
import os
import sys
import threading
import time

import cv2
import dlib
import numpy as np
import pygame
from PySide6 import QtCore, QtGui, QtWidgets
from PySide6.QtGui import QImage
from PySide6.QtCore import Qt
from imutils import face_utils
from scipy.spatial import distance as dist
from camera_config import camera_config

# å°è¯•å¯¼å…¥æ·±åº¦å­¦ä¹ åº“ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨å ä½ç¬¦
try:
    # é¦–å…ˆå®šä¹‰TENSORFLOW_AVAILABLEå˜é‡
    TENSORFLOW_AVAILABLE = False

    import tensorflow as tf
    from tensorflow.keras.applications import ResNet50
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Dense, Conv2D, MaxPooling2D, Flatten

    # å¦‚æœå¯¼å…¥æˆåŠŸï¼Œè®¾ç½®TENSORFLOW_AVAILABLEä¸ºTrue
    TENSORFLOW_AVAILABLE = True

    # åªæœ‰å½“TensorFlowå¯ç”¨æ—¶æ‰å°è¯•å¯¼å…¥Inputå±‚
    if TENSORFLOW_AVAILABLE:
        from tensorflow.keras.layers import Input
        pass

except ImportError as e:
    print(f"TensorFlowå¯¼å…¥å¤±è´¥: {e}")
    TENSORFLOW_AVAILABLE = False

# å°è¯•å¯¼å…¥PyTorchç”¨äºCNN+LSTMæ‰“å“ˆæ¬ æ£€æµ‹
try:
    import torch
    import torch.nn as nn
    PYTORCH_AVAILABLE = True
    print("âœ… PyTorchå¯ç”¨ï¼Œå°†å¯ç”¨CNN+LSTMæ‰“å“ˆæ¬ æ£€æµ‹")
except ImportError as e:
    print(f"PyTorchå¯¼å…¥å¤±è´¥: {e}")
    PYTORCH_AVAILABLE = False


# CNN+LSTMæ¨¡å‹å®šä¹‰ï¼ˆç”¨äºæ‰“å“ˆæ¬ æ£€æµ‹ï¼‰
if PYTORCH_AVAILABLE:
    class YawnCNNLSTM(nn.Module):
        """ä¸“é—¨ç”¨äºæ‰“å“ˆæ¬ æ£€æµ‹çš„CNN+LSTMæ¨¡å‹"""
        def __init__(self, input_size, hidden_size, num_layers, output_size):
            super(YawnCNNLSTM, self).__init__()

            # CNN layers - Extract spatial features
            self.cnn = nn.Sequential(
                nn.Conv1d(input_size, 64, kernel_size=3, padding=1),
                nn.BatchNorm1d(64),
                nn.ReLU(),
                nn.MaxPool1d(kernel_size=2),
                nn.Conv1d(64, 128, kernel_size=3, padding=1),
                nn.BatchNorm1d(128),
                nn.ReLU(),
                nn.MaxPool1d(kernel_size=2),
                nn.Conv1d(128, 256, kernel_size=3, padding=1),
                nn.BatchNorm1d(256),
                nn.ReLU(),
                nn.MaxPool1d(kernel_size=2)
            )

            # CNN output channels is 256
            cnn_output_size = 256

            # LSTM layers - Process temporal features
            self.lstm = nn.LSTM(
                input_size=cnn_output_size,
                hidden_size=hidden_size,
                num_layers=num_layers,
                batch_first=True,
                dropout=0.2,
                bidirectional=True
            )

            # Fully connected layers - Output classification results
            self.fc = nn.Sequential(
                nn.Linear(hidden_size * 2, 128),
                nn.ReLU(),
                nn.Dropout(0.5),
                nn.Linear(128, 64),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(64, output_size)
            )

        def forward(self, x):
            batch_size, seq_len, features = x.size()

            # é‡å¡‘è¾“å…¥ä»¥é€šè¿‡CNN: (batch_size, features, seq_len)
            x = x.permute(0, 2, 1)

            # é€šè¿‡CNN
            cnn_out = self.cnn(x)

            # é‡å¡‘CNNè¾“å‡ºä»¥é€šè¿‡LSTM: (batch_size, seq_len, cnn_features)
            cnn_out = cnn_out.permute(0, 2, 1)

            # é€šè¿‡LSTM
            lstm_out, _ = self.lstm(cnn_out)

            # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
            lstm_out = lstm_out[:, -1, :]

            # é€šè¿‡å…¨è¿æ¥å±‚
            output = self.fc(lstm_out)

            return output


    class YawnDetector:
        """ä¸“é—¨ç”¨äºæ‰“å“ˆæ¬ æ£€æµ‹çš„ç±»"""
        def __init__(self, model_path=None, seq_length=30, consecutive_frames=15):
            self.model = None
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if PYTORCH_AVAILABLE else None
            self.seq_length = seq_length
            self.features_buffer = []
            self.is_available = False

            # æ·»åŠ è¿ç»­å¸§åˆ¤æ–­é€»è¾‘ï¼ˆå‚è€ƒreal_pljcï¼‰
            self.consecutive_frames = consecutive_frames
            self.fatigue_frames = 0
            self.frame_count = 0

            if PYTORCH_AVAILABLE and model_path and os.path.exists(model_path):
                self.load_model(model_path)

        def load_model(self, model_path):
            """åŠ è½½è®­ç»ƒå¥½çš„CNN+LSTMæ¨¡å‹"""
            try:
                # åˆå§‹åŒ–æ¨¡å‹ï¼ˆä¸è®­ç»ƒæ—¶çš„ç»“æ„ä¿æŒä¸€è‡´ï¼‰
                self.model = YawnCNNLSTM(
                    input_size=138,  # 2 (EAR, MAR) + 68*2 (landmark coordinates)
                    hidden_size=64,
                    num_layers=1,
                    output_size=1
                ).to(self.device)

                # åŠ è½½æ¨¡å‹æƒé‡
                self.model.load_state_dict(torch.load(model_path, map_location=self.device, weights_only=False))
                self.model.eval()
                self.is_available = True
                print(f"âœ… CNN+LSTMæ‰“å“ˆæ¬ æ£€æµ‹æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")

            except Exception as e:
                print(f"âŒ CNN+LSTMæ‰“å“ˆæ¬ æ£€æµ‹æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.model = None
                self.is_available = False

        def extract_features(self, landmarks, ear, mar, frame_height=480):
            """æå–ç‰¹å¾å‘é‡ï¼ˆä¸real_pljcä¿æŒä¸€è‡´ï¼‰"""
            if landmarks is None:
                return None

            try:
                # å½’ä¸€åŒ–å…³é”®ç‚¹åæ ‡ï¼ˆä»¥é¼»å°–ä¸ºåŸºå‡†ï¼Œä½¿ç”¨å¸§é«˜åº¦å½’ä¸€åŒ–ï¼‰
                nose = landmarks[30]  # é¼»å°–å…³é”®ç‚¹
                normalized_landmarks = (landmarks - nose).flatten() / frame_height  # ä½¿ç”¨å¸§é«˜åº¦å½’ä¸€åŒ–

                # ç»„åˆç‰¹å¾ï¼šEAR, MAR + 68ä¸ªå…³é”®ç‚¹çš„x,yåæ ‡
                features = np.concatenate([[ear, mar], normalized_landmarks])
                return features

            except Exception as e:
                print(f"ç‰¹å¾æå–å¤±è´¥: {e}")
                return None

        def update_buffer(self, features):
            """æ›´æ–°ç‰¹å¾ç¼“å†²åŒº"""
            if features is not None:
                self.features_buffer.append(features)
                if len(self.features_buffer) > self.seq_length:
                    self.features_buffer.pop(0)

        def predict_yawn(self, detection_enabled=True):
            """é¢„æµ‹æ˜¯å¦æ‰“å“ˆæ¬ ï¼ˆä¸real_pljcä¿æŒä¸€è‡´çš„é€»è¾‘ï¼‰"""
            if not self.is_available or len(self.features_buffer) < self.seq_length:
                return False, 0.0

            try:
                # å‡†å¤‡è¾“å…¥åºåˆ—
                input_seq = np.array([self.features_buffer])
                input_tensor = torch.FloatTensor(input_seq).to(self.device)

                # æ¨¡å‹é¢„æµ‹
                with torch.no_grad():
                    logits = self.model(input_tensor).item()
                    prediction = torch.sigmoid(torch.tensor(logits)).item()

                # æ›´æ–°ç–²åŠ³çŠ¶æ€ï¼ˆå‚è€ƒreal_pljcçš„è¿ç»­å¸§åˆ¤æ–­é€»è¾‘ï¼‰
                self.frame_count += 1
                if prediction >= 0.5:  # å•å¸§é¢„æµ‹é˜ˆå€¼
                    self.fatigue_frames += 1
                else:
                    self.fatigue_frames = 0

                # åˆ¤å®šç–²åŠ³éœ€è¦è¿ç»­å¸§æ•°è¾¾åˆ°é˜ˆå€¼
                is_fatigued = self.fatigue_frames >= self.consecutive_frames

                # å¦‚æœæ£€æµ‹è¢«ç¦ç”¨ï¼ˆå†·å´æœŸï¼‰ï¼Œé‡ç½®è¿ç»­å¸§è®¡æ•°
                if not detection_enabled and is_fatigued:
                    self.fatigue_frames = 0
                    is_fatigued = False

                return is_fatigued, prediction

            except Exception as e:
                print(f"CNN+LSTMæ‰“å“ˆæ¬ é¢„æµ‹å¤±è´¥: {e}")
                return False, 0.0

        def reset_state(self):
            """é‡ç½®æ£€æµ‹å™¨çŠ¶æ€ï¼ˆç”¨äºå†·å´æœŸï¼‰"""
            self.fatigue_frames = 0
            self.frame_count = 0


# å¦‚æœPyTorchä¸å¯ç”¨ï¼Œå®šä¹‰å ä½ç¬¦ç±»
if not PYTORCH_AVAILABLE:
    class YawnCNNLSTM:
        def __init__(self, *args, **kwargs):
            pass

    class YawnDetector:
        def __init__(self, *args, **kwargs):
            self.is_available = False


# å¤„ç†TensorFlowå¯¼å…¥å¤±è´¥çš„æƒ…å†µ
try:
    # è¿™ä¸ªtryå—æ˜¯ä¸ºäº†å…¼å®¹åŸæœ‰çš„except ImportError
    pass
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œå®šä¹‰åŸºæœ¬å ä½ç¬¦
    class Sequential:
        def __init__(self):
            pass

        def add(self, layer):
            pass

    class ResNet50:
        def __init__(self, *args, **kwargs):
            pass

    class LSTM:
        def __init__(self, *args, **kwargs):
            pass

    class Dense:
        def __init__(self, *args, **kwargs):
            pass

    class Conv2D:
        def __init__(self, *args, **kwargs):
            pass

    class MaxPooling2D:
        def __init__(self, *args, **kwargs):
            pass

    class Flatten:
        def __init__(self, *args, **kwargs):
            pass

    tf = None
    TENSORFLOW_AVAILABLE = False

import main_ui


# pyside6-uic -o main_ui.py main.ui

class MainUI(QtWidgets.QWidget, main_ui.Ui_Form):
    # ä¿¡å·ï¼Œåœ¨UIçº¿ç¨‹ä¸­ï¼Œä¸èƒ½åœ¨å…¶ä»–çº¿ç¨‹ç›´æ¥æ“ä½œUI
    thread_signal = QtCore.Signal(dict)

    def __init__(self):
        super().__init__()
        self.setupUi(self)

        # åˆå§‹åŒ–æ‘„åƒå¤´åˆ—è¡¨
        self.cameras = []
        self.init_camera_list()

        # è¿æ¥ä¿¡å·
        self.pushButton.clicked.connect(self.button_clicked)

        # åˆå§‹åŒ–ç–²åŠ³ç»Ÿè®¡æ¨¡å—
        try:
            from fatigue_statistics import FatigueStatistics
            self.fatigue_stats = FatigueStatistics()
            print("âœ… ç–²åŠ³ç»Ÿè®¡æ¨¡å—å·²åˆå§‹åŒ–")
        except Exception as e:
            print(f"âš ï¸ ç–²åŠ³ç»Ÿè®¡æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
            self.fatigue_stats = None

        # åˆå§‹åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹
        self.resnet = None
        self.cnn_model = None
        self.lstm_model = None

        # åˆå§‹åŒ–CNNç–²åŠ³æ£€æµ‹å™¨
        self.cnn_detector = None

        # åˆå§‹åŒ–CNN+LSTMæ‰“å“ˆæ¬ æ£€æµ‹å™¨
        self.yawn_detector = None

        if TENSORFLOW_AVAILABLE:
            try:
                self.init_models()
                self.init_cnn_detector()
            except Exception as e:
                print(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            print("TensorFlowæœªå®‰è£…ï¼Œä½¿ç”¨ç®€åŒ–åŠŸèƒ½")

        # åˆå§‹åŒ–CNN+LSTMæ‰“å“ˆæ¬ æ£€æµ‹å™¨
        if PYTORCH_AVAILABLE:
            try:
                self.init_yawn_detector()
            except Exception as e:
                print(f"CNN+LSTMæ‰“å“ˆæ¬ æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")

        # è¿æ¥ä¿¡å·
        # self.thread_signal.connect(self.thread_singnal_slot)

        # å…­ä¸ªåŠŸèƒ½æ˜¯å¦è¦ç”¨
        self.fun = [True] * 6

        # å…¼å®¹æ–°æ—§ç•Œé¢çš„å¤é€‰æ¡†åç§°
        try:
            # å°è¯•ä½¿ç”¨æ–°ç•Œé¢çš„å¤é€‰æ¡†
            if hasattr(self, 'checkBox'):
                self.checkBox_11 = self.checkBox
                self.checkBox_12 = self.checkBox_2
                self.checkBox_21 = self.checkBox_3
                self.checkBox_22 = self.checkBox_4
                # ä¸ºç¼ºå¤±çš„å¤é€‰æ¡†åˆ›å»ºå ä½ç¬¦
                if not hasattr(self, 'checkBox_31'):
                    from PySide6.QtWidgets import QCheckBox
                    self.checkBox_31 = QCheckBox()
                    self.checkBox_32 = QCheckBox()

            self.checkBox_11.setChecked(self.fun[0])
            self.checkBox_12.setChecked(self.fun[1])
            self.checkBox_21.setChecked(self.fun[2])
            self.checkBox_22.setChecked(self.fun[3])
            if hasattr(self, 'checkBox_31'):
                self.checkBox_31.setChecked(self.fun[4])
            if hasattr(self, 'checkBox_32'):
                self.checkBox_32.setChecked(self.fun[5])

            self.checkBox_11.stateChanged.connect(self.select_changed)
            self.checkBox_12.stateChanged.connect(self.select_changed)
            self.checkBox_21.stateChanged.connect(self.select_changed)
            self.checkBox_22.stateChanged.connect(self.select_changed)
            if hasattr(self, 'checkBox_31'):
                self.checkBox_31.stateChanged.connect(self.select_changed)
            if hasattr(self, 'checkBox_32'):
                self.checkBox_32.stateChanged.connect(self.select_changed)
        except AttributeError as e:
            print(f"ç•Œé¢ç»„ä»¶åˆå§‹åŒ–è­¦å‘Š: {e}")
            # åˆ›å»ºé»˜è®¤çš„åŠŸèƒ½çŠ¶æ€
            pass

        # é˜ˆå€¼
        self.values = [3,2,3,5,2]

        # å…¼å®¹æ–°æ—§ç•Œé¢çš„spinBoxåç§°
        try:
            # ä¸ºç¼ºå¤±çš„spinBoxåˆ›å»ºå ä½ç¬¦
            if not hasattr(self, 'spinBox_2'):
                from PySide6.QtWidgets import QSpinBox
                self.spinBox_2 = QSpinBox()
                self.spinBox_2.setValue(2)

            if hasattr(self, 'spinBox_1'):
                self.spinBox_1.setValue(self.values[0])
                self.spinBox_1.valueChanged.connect(self.value_changed)
            if hasattr(self, 'spinBox_2'):
                self.spinBox_2.setValue(self.values[1])
                self.spinBox_2.valueChanged.connect(self.value_changed)
            if hasattr(self, 'spinBox_3'):
                self.spinBox_3.setValue(self.values[2])
                self.spinBox_3.valueChanged.connect(self.value_changed)
            if hasattr(self, 'spinBox_4'):
                self.spinBox_4.setValue(self.values[3])
                self.spinBox_4.valueChanged.connect(self.value_changed)
            if hasattr(self, 'spinBox_5'):
                self.spinBox_5.setValue(self.values[4])
                self.spinBox_5.valueChanged.connect(self.value_changed)
        except AttributeError as e:
            print(f"SpinBoxåˆå§‹åŒ–è­¦å‘Š: {e}")

        self.thread_signal.connect(self.thread_singnal_slot)

        # å…¼å®¹æ–°æ—§ç•Œé¢çš„å›¾ç‰‡æ˜¾ç¤ºæ ‡ç­¾
        if not hasattr(self, 'label_img'):
            from PySide6.QtWidgets import QLabel
            self.label_img = QLabel()
            self.label_img.setScaledContents(True)
            # è®¾ç½®æœ€å°å°ºå¯¸ç¡®ä¿æœ‰è¶³å¤Ÿç©ºé—´æ˜¾ç¤ºäººè„¸
            self.label_img.setMinimumSize(640, 480)
            # å¦‚æœæœ‰è§†é¢‘æ˜¾ç¤ºåŒºåŸŸï¼Œå¯ä»¥å°†label_imgæ·»åŠ åˆ°å…¶ä¸­
            print("åˆ›å»ºäº†é»˜è®¤çš„å›¾ç‰‡æ˜¾ç¤ºæ ‡ç­¾")
        else:
            self.label_img.setScaledContents(True)
            # ç¡®ä¿ç°æœ‰çš„label_imgä¹Ÿæœ‰åˆé€‚çš„æœ€å°å°ºå¯¸
            if self.label_img.minimumSize().width() < 640:
                self.label_img.setMinimumSize(640, 480)

        if hasattr(self, 'plainTextEdit_tip'):
            self.plainTextEdit_tip.appendPlainText('ç­‰å¾…å¼€å§‹\n')
        else:
            print("ç­‰å¾…å¼€å§‹")


        """å‚æ•°"""
        # é»˜è®¤ä¸ºæ‘„åƒå¤´0
        self.VIDEO_STREAM = 0
        self.CAMERA_STYLE = False  # Falseæœªæ‰“å¼€æ‘„åƒå¤´ï¼ŒTrueæ‘„åƒå¤´å·²æ‰“å¼€

        # ä¼˜åŒ–åçš„çœ¨çœ¼æ£€æµ‹å‚æ•° - è¿›ä¸€æ­¥æé«˜æ•æ„Ÿåº¦
        self.EYE_AR_THRESH = 0.20  # è¿›ä¸€æ­¥é™ä½é˜ˆå€¼ï¼Œæé«˜æ•æ„Ÿåº¦ï¼ˆåŸ0.22ï¼‰
        self.EYE_AR_CONSEC_FRAMES = 2  # ä¿æŒè¾ƒä½çš„è¿ç»­å¸§è¦æ±‚
        self.EYE_AR_UPPER_THRESH = 0.40  # é€‚å½“æé«˜ä¸Šé™ï¼Œé¿å…è¿‡æ»¤æ­£å¸¸çœ¨çœ¼

        # ä¼˜åŒ–åçš„æ‰“å“ˆæ¬ æ£€æµ‹å‚æ•° - è¿›ä¸€æ­¥æé«˜æ•æ„Ÿåº¦
        self.MAR_THRESH = 0.40  # è¿›ä¸€æ­¥é™ä½é˜ˆå€¼ï¼Œæé«˜æ•æ„Ÿåº¦ï¼ˆåŸ0.45ï¼‰
        self.MAR_DURATION_THRESH = 0.6  # è¿›ä¸€æ­¥é™ä½å“ˆæ¬ æŒç»­æ—¶é—´é˜ˆå€¼ï¼ˆç§’ï¼‰
        self.MOUTH_AR_CONSEC_FRAMES = 2  # å‡å°‘è¿ç»­å¸§è¦æ±‚

        # ä¼˜åŒ–åçš„çŒç¡ç‚¹å¤´æ£€æµ‹å‚æ•°
        self.HAR_THRESH_LOW = 15.0  # è½»å¾®ç‚¹å¤´è§’åº¦é˜ˆå€¼ï¼ˆåº¦ï¼‰
        self.HAR_THRESH_HIGH = 25.0  # æ˜æ˜¾ç‚¹å¤´è§’åº¦é˜ˆå€¼ï¼ˆåº¦ï¼‰
        self.NOD_AR_CONSEC_FRAMES = 4  # å¢åŠ è¿ç»­å¸§è¦æ±‚ï¼Œå‡å°‘è¯¯æ£€

        # å…¶ä»–æ£€æµ‹å‚æ•°
        self.AR_CONSEC_FRAMES_check = 3
        self.OUT_AR_CONSEC_FRAMES_check = 5

        """è®¡æ•°"""
        # åˆå§‹åŒ–å¸§è®¡æ•°å™¨å’Œçœ¨çœ¼æ€»æ•°
        self.COUNTER = 0
        self.TOTAL = 0
        # åˆå§‹åŒ–å¸§è®¡æ•°å™¨å’Œæ‰“å“ˆæ¬ æ€»æ•°
        self.mCOUNTER = 0
        self.mTOTAL = 0
        # åˆå§‹åŒ–å¸§è®¡æ•°å™¨å’Œç‚¹å¤´æ€»æ•°
        self.hCOUNTER = 0
        self.hTOTAL = 0
        # ç¦»èŒæ—¶é—´é•¿åº¦
        self.oCOUNTER = 0

        # æ–°å¢ï¼šæ”¹è¿›çš„æ£€æµ‹çŠ¶æ€è·Ÿè¸ª
        self.yawn_start_time = None  # å“ˆæ¬ å¼€å§‹æ—¶é—´
        self.last_ear_values = []  # æœ€è¿‘çš„EARå€¼å†å²
        self.last_mar_values = []  # æœ€è¿‘çš„MARå€¼å†å²
        self.fatigue_score = 0.0  # ç–²åŠ³è¯„åˆ†
        self.baseline_ear = 0.3  # åŸºçº¿EARå€¼ï¼ˆå°†åŠ¨æ€è°ƒæ•´ï¼‰
        self.baseline_mar = 0.4  # åŸºçº¿MARå€¼ï¼ˆå°†åŠ¨æ€è°ƒæ•´ï¼‰

        # æ‰“å“ˆæ¬ å†·å´æœºåˆ¶
        self.last_yawn_time = None  # ä¸Šæ¬¡æ£€æµ‹åˆ°æ‰“å“ˆæ¬ çš„æ—¶é—´
        self.yawn_cooldown_seconds = 3.0  # æ‰“å“ˆæ¬ å†·å´æ—¶é—´ï¼ˆç§’ï¼‰
        self.yawn_detection_enabled = True  # æ‰“å“ˆæ¬ æ£€æµ‹æ˜¯å¦å¯ç”¨

        # è‡ªé€‚åº”é˜ˆå€¼è°ƒæ•´
        self.calibration_frames = 0  # æ ¡å‡†å¸§æ•°
        self.calibration_period = 300  # æ ¡å‡†å‘¨æœŸï¼ˆå¸§æ•°ï¼‰
        self.adaptive_mode = True  # æ˜¯å¦å¯ç”¨è‡ªé€‚åº”æ¨¡å¼

        """å§¿æ€"""
        # ä¸–ç•Œåæ ‡ç³»(UVW)ï¼šå¡«å†™3Då‚è€ƒç‚¹ï¼Œè¯¥æ¨¡å‹å‚è€ƒhttp://aifi.isr.uc.pt/Downloads/OpenGL/glAnthropometric3DModel.cpp
        self.object_pts = np.float32([[6.825897, 6.760612, 4.402142],  #33å·¦çœ‰å·¦ä¸Šè§’
                                 [1.330353, 7.122144, 6.903745],  #29å·¦çœ‰å³è§’
                                 [-1.330353, 7.122144, 6.903745], #34å³çœ‰å·¦è§’
                                 [-6.825897, 6.760612, 4.402142], #38å³çœ‰å³ä¸Šè§’
                                 [5.311432, 5.485328, 3.987654],  #13å·¦çœ¼å·¦ä¸Šè§’
                                 [1.789930, 5.393625, 4.413414],  #17å·¦çœ¼å³ä¸Šè§’
                                 [-1.789930, 5.393625, 4.413414], #25å³çœ¼å·¦ä¸Šè§’
                                 [-5.311432, 5.485328, 3.987654], #21å³çœ¼å³ä¸Šè§’
                                 [2.005628, 1.409845, 6.165652],  #55é¼»å­å·¦ä¸Šè§’
                                 [-2.005628, 1.409845, 6.165652], #49é¼»å­å³ä¸Šè§’
                                 [2.774015, -2.080775, 5.048531], #43å˜´å·¦ä¸Šè§’
                                 [-2.774015, -2.080775, 5.048531],#39å˜´å³ä¸Šè§’
                                 [0.000000, -3.116408, 6.097667], #45å˜´ä¸­å¤®ä¸‹è§’
                                 [0.000000, -7.415691, 4.070434]])#6ä¸‹å·´è§’

        # ç›¸æœºåæ ‡ç³»(XYZ)ï¼šæ·»åŠ ç›¸æœºå†…å‚
        self.K = [6.5308391993466671e+002, 0.0, 3.1950000000000000e+002,
                 0.0, 6.5308391993466671e+002, 2.3950000000000000e+002,
                 0.0, 0.0, 1.0]# ç­‰ä»·äºçŸ©é˜µ[fx, 0, cx; 0, fy, cy; 0, 0, 1]
        # å›¾åƒä¸­å¿ƒåæ ‡ç³»(uv)ï¼šç›¸æœºç•¸å˜å‚æ•°[k1, k2, p1, p2, k3]
        self.D = [7.0834633684407095e-002, 6.9140193737175351e-002, 0.0, 0.0, -1.3073460323689292e+000]

        # åƒç´ åæ ‡ç³»(xy)ï¼šå¡«å†™å‡¸è½®çš„æœ¬å¾å’Œç•¸å˜ç³»æ•°
        self.cam_matrix = np.array(self.K).reshape(3, 3).astype(np.float32)
        self.dist_coeffs = np.array(self.D).reshape(5, 1).astype(np.float32)

        # é‡æ–°æŠ•å½±3Dç‚¹çš„ä¸–ç•Œåæ ‡è½´ä»¥éªŒè¯ç»“æœå§¿åŠ¿
        self.reprojectsrc = np.float32([[10.0, 10.0, 10.0],
                                       [10.0, 10.0, -10.0],
                                       [10.0, -10.0, -10.0],
                                       [10.0, -10.0, 10.0],
                                       [-10.0, 10.0, 10.0],
                                       [-10.0, 10.0, -10.0],
                                       [-10.0, -10.0, -10.0],
                                       [-10.0, -10.0, 10.0]])
        # ç»˜åˆ¶æ­£æ–¹ä½“12è½´
        self.line_pairs = [[0, 1], [1, 2], [2, 3], [3, 0],
                          [4, 5], [5, 6], [6, 7], [7, 4],
                          [0, 4], [1, 5], [2, 6], [3, 7]]


        # çº¿ç¨‹
        self.thread = None
        self.sound_thread = None
        self.is_running = True  # æ·»åŠ è¿è¡ŒçŠ¶æ€æ ‡å¿—

    def safe_emit_signal(self, data):
        """å®‰å…¨åœ°å‘é€ä¿¡å·"""
        try:
            if self.is_running and hasattr(self, 'thread_signal'):
                # è¿›ä¸€æ­¥å‡å°‘è°ƒè¯•è¾“å‡ºï¼Œåªåœ¨é‡è¦äº‹ä»¶æ—¶æ‰“å°
                if data['type'] == 'msg' and ('ç–²åŠ³' in str(data.get('value', '')) or 'CNNæ£€æµ‹' in str(data.get('value', ''))):
                    print(f"å‘é€é‡è¦ä¿¡å·: {data['type']} - {data.get('value', '')}")
                elif data['type'] == 'res' and hasattr(self, 'frame_count') and self.frame_count % 300 == 0:  # æ¯10ç§’æ‰“å°ä¸€æ¬¡çŠ¶æ€
                    print(f"ç³»ç»ŸçŠ¶æ€æ­£å¸¸: FPS={data.get('value', ['', '', '0'])[2]}")

                self.thread_signal.emit(data)
                return True
            else:
                # åªåœ¨ç¬¬ä¸€æ¬¡å‡ºç°é”™è¯¯æ—¶è®°å½•ï¼Œé¿å…é‡å¤æ—¥å¿—
                if not hasattr(self, '_signal_error_logged'):
                    print(f"æ£€æµ‹å·²åœæ­¢ï¼Œåœæ­¢ä¿¡å·å‘é€")
                    self._signal_error_logged = True
                return False
        except RuntimeError as e:
            # Qtå¯¹è±¡å·²è¢«é”€æ¯çš„æƒ…å†µ
            if not hasattr(self, '_runtime_error_logged'):
                print(f"UIå·²å…³é—­ï¼Œåœæ­¢ä¿¡å·å‘é€")
                self._runtime_error_logged = True
            return False
        except Exception as e:
            if not hasattr(self, '_unknown_error_logged'):
                print(f"ä¿¡å·å‘é€å¼‚å¸¸: {e}")
                self._unknown_error_logged = True
            return False

    def _optimize_camera_brightness(self):
        """ä¼˜åŒ–æ‘„åƒå¤´è®¾ç½® - ä½¿ç”¨é…ç½®æ–‡ä»¶å‚æ•°"""
        if self.cap is None or not self.cap.isOpened():
            return

        print("æ­£åœ¨ä¼˜åŒ–æ‘„åƒå¤´è®¾ç½®...")

        try:
            # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æ‘„åƒå¤´å±æ€§
            camera_props = camera_config.get_camera_properties()

            for prop, value in camera_props.items():
                try:
                    self.cap.set(prop, value)
                except Exception as e:
                    # æŸäº›å±æ€§å¯èƒ½ä¸è¢«æ”¯æŒï¼Œç»§ç»­è®¾ç½®å…¶ä»–å±æ€§
                    pass

            print(f"æ‘„åƒå¤´ä¼˜åŒ–è®¾ç½®å®Œæˆ:")
            print(f"  åˆ†è¾¨ç‡: {camera_config.CAMERA_WIDTH}x{camera_config.CAMERA_HEIGHT}")
            print(f"  FPS: {camera_config.CAMERA_FPS}")
            print(f"  äº®åº¦: {camera_config.BRIGHTNESS}")
            print(f"  å¯¹æ¯”åº¦: {camera_config.CONTRAST}")
            print(f"  å¢ç›Š: {camera_config.GAIN}")
            print("  å·²é’ˆå¯¹æš—ç¯å¢ƒå’Œå¸§ç‡è¿›è¡Œä¼˜åŒ–")

        except Exception as e:
            print(f"æ‘„åƒå¤´å‚æ•°è®¾ç½®å¤±è´¥: {e}")
            # å³ä½¿è®¾ç½®å¤±è´¥ä¹Ÿç»§ç»­ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°

    def _enhance_dark_frame(self, frame):
        """å¢å¼ºæš—å›¾åƒ - ä½¿ç”¨é…ç½®æ–‡ä»¶å‚æ•°"""
        if frame is None:
            return frame

        # æ£€æŸ¥å›¾åƒäº®åº¦
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        mean_brightness = np.mean(gray)

        # å¦‚æœå›¾åƒå¤ªæš—ï¼Œåº”ç”¨å¤šæ­¥éª¤å¢å¼º
        if mean_brightness < camera_config.DARK_THRESHOLD:
            enhanced_frame = frame.copy()

            # æ­¥éª¤1: åŸºç¡€äº®åº¦å’Œå¯¹æ¯”åº¦è°ƒæ•´
            enhanced_frame = cv2.convertScaleAbs(
                enhanced_frame,
                alpha=camera_config.BRIGHTNESS_ALPHA,
                beta=camera_config.BRIGHTNESS_BETA
            )

            # æ­¥éª¤2: åº”ç”¨CLAHEï¼ˆå¯¹æ¯”åº¦é™åˆ¶è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡åŒ–ï¼‰
            clahe = cv2.createCLAHE(
                clipLimit=camera_config.CLAHE_CLIP_LIMIT,
                tileGridSize=camera_config.CLAHE_TILE_SIZE
            )

            # åœ¨LABè‰²å½©ç©ºé—´ä¸­å¤„ç†äº®åº¦é€šé“
            lab = cv2.cvtColor(enhanced_frame, cv2.COLOR_BGR2LAB)
            lab[:,:,0] = clahe.apply(lab[:,:,0])
            enhanced_frame = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)

            # æ­¥éª¤3: Gammaæ ¡æ­£è¿›ä¸€æ­¥æäº®æš—éƒ¨
            gamma = camera_config.GAMMA_CORRECTION
            inv_gamma = 1.0 / gamma
            table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype("uint8")
            enhanced_frame = cv2.LUT(enhanced_frame, table)

            # æ­¥éª¤4: ä¸åŸå›¾æ··åˆï¼Œä¿æŒè‡ªç„¶æ•ˆæœ
            enhanced_frame = cv2.addWeighted(
                frame,
                1.0 - camera_config.ENHANCEMENT_WEIGHT,
                enhanced_frame,
                camera_config.ENHANCEMENT_WEIGHT,
                0
            )

            return enhanced_frame

        return frame

    def init_camera_list(self):
        """åˆå§‹åŒ–æ‘„åƒå¤´åˆ—è¡¨"""
        self.cameras = []
        print("æ­£åœ¨æ‰«æå¯ç”¨æ‘„åƒå¤´...")

        for i in range(5):
            print(f"  æµ‹è¯•æ‘„åƒå¤´ç´¢å¼• {i}...")
            cap = None
            try:
                cap = cv2.VideoCapture(i, cv2.CAP_DSHOW)
                if cap.isOpened():
                    # æµ‹è¯•æ˜¯å¦èƒ½è¯»å–å¸§
                    ret, frame = cap.read()
                    if ret and frame is not None and frame.size > 0:
                        self.cameras.append(i)
                        print(f"    âœ“ æ‘„åƒå¤´ {i} å¯ç”¨")
                    else:
                        print(f"    âœ— æ‘„åƒå¤´ {i} æ— æ³•è¯»å–å¸§")
                else:
                    print(f"    âœ— æ‘„åƒå¤´ {i} æ— æ³•æ‰“å¼€")

                if cap is not None:
                    cap.release()

            except Exception as e:
                print(f"    âœ— æ‘„åƒå¤´ {i} æµ‹è¯•å¼‚å¸¸: {e}")
                if cap is not None:
                    try:
                        cap.release()
                    except:
                        pass

        print(f"æ‰¾åˆ° {len(self.cameras)} ä¸ªæ‘„åƒå¤´è®¾å¤‡")
        if self.cameras:
            print("å¯ç”¨æ‘„åƒå¤´:", self.cameras)
        else:
            print("æœªæ‰¾åˆ°æ‘„åƒå¤´è®¾å¤‡")

    def _init_camera_robust(self, camera_index):
        """ç®€åŒ–çš„æ‘„åƒå¤´åˆå§‹åŒ–æ–¹æ³• - é¿å…é—ªçƒ"""
        print(f"æ­£åœ¨åˆå§‹åŒ–æ‘„åƒå¤´ {camera_index}...")

        # ä¼˜å…ˆä½¿ç”¨DirectShowåç«¯ï¼ˆWindowsæœ€ç¨³å®šï¼‰
        try:
            print(f"  ä½¿ç”¨DirectShowåç«¯...")
            cap = cv2.VideoCapture(camera_index, cv2.CAP_DSHOW)

            if cap.isOpened():
                # ç«‹å³æµ‹è¯•è¯»å–
                ret, frame = cap.read()
                if ret and frame is not None and frame.size > 0:
                    print(f"æ‘„åƒå¤´ {camera_index} åˆå§‹åŒ–æˆåŠŸ")
                    return cap
                else:
                    print(f"  æ— æ³•è¯»å–å¸§")
                    cap.release()
            else:
                print(f"  æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
                if cap is not None:
                    cap.release()

        except Exception as e:
            print(f"  DirectShowåˆå§‹åŒ–å¤±è´¥: {e}")
            if 'cap' in locals() and cap is not None:
                try:
                    cap.release()
                except:
                    pass

        # å¦‚æœDirectShowå¤±è´¥ï¼Œå°è¯•é»˜è®¤åç«¯
        try:
            print(f"  å°è¯•é»˜è®¤åç«¯...")
            cap = cv2.VideoCapture(camera_index)

            if cap.isOpened():
                ret, frame = cap.read()
                if ret and frame is not None and frame.size > 0:
                    print(f"æ‘„åƒå¤´ {camera_index} åˆå§‹åŒ–æˆåŠŸï¼ˆé»˜è®¤åç«¯ï¼‰")
                    return cap
                else:
                    cap.release()
            else:
                if cap is not None:
                    cap.release()

        except Exception as e:
            print(f"  é»˜è®¤åç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            if 'cap' in locals() and cap is not None:
                try:
                    cap.release()
                except:
                    pass

        print(f"æ‘„åƒå¤´ {camera_index} åˆå§‹åŒ–å¤±è´¥")
        return None

    def _check_camera_health(self):
        """æ£€æŸ¥æ‘„åƒå¤´å¥åº·çŠ¶æ€"""
        if not hasattr(self, 'cap') or self.cap is None:
            return False

        try:
            # æ£€æŸ¥æ‘„åƒå¤´æ˜¯å¦ä»ç„¶æ‰“å¼€
            if not self.cap.isOpened():
                return False

            # å°è¯•è·å–ä¸€äº›åŸºæœ¬å±æ€§
            width = self.cap.get(cv2.CAP_PROP_FRAME_WIDTH)
            height = self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT)

            if width <= 0 or height <= 0:
                return False

            return True

        except Exception as e:
            print(f"æ‘„åƒå¤´å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False

    def _reconnect_camera(self):
        """é‡æ–°è¿æ¥æ‘„åƒå¤´"""
        print("å°è¯•é‡æ–°è¿æ¥æ‘„åƒå¤´...")

        # é‡Šæ”¾å½“å‰æ‘„åƒå¤´
        if hasattr(self, 'cap') and self.cap is not None:
            try:
                self.cap.release()
            except:
                pass
            self.cap = None

        # çŸ­æš‚ç­‰å¾…
        time.sleep(0.2)

        # å°è¯•é‡æ–°æ‰“å¼€
        try:
            self.cap = cv2.VideoCapture(self.VIDEO_STREAM, cv2.CAP_DSHOW)
            if self.cap.isOpened():
                # é‡æ–°è®¾ç½®åŸºæœ¬å‚æ•°
                self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
                self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
                self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)

                # æµ‹è¯•è¯»å–
                ret, test_frame = self.cap.read()
                if ret and test_frame is not None:
                    print("æ‘„åƒå¤´é‡è¿æˆåŠŸ")
                    return True
                else:
                    print("æ‘„åƒå¤´é‡è¿åæ— æ³•è¯»å–å¸§")
                    self.cap.release()
                    self.cap = None
                    return False
            else:
                print("æ‘„åƒå¤´é‡è¿å¤±è´¥")
                return False

        except Exception as e:
            print(f"æ‘„åƒå¤´é‡è¿å¼‚å¸¸: {e}")
            return False



    def init_models(self):
        """åˆå§‹åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹"""
        self.resnet = self._init_resnet()
        self.cnn_model = self._init_cnn_model()
        self.lstm_model = self._init_lstm_model()

        if all([self.resnet, self.cnn_model, self.lstm_model]):
            print("æ‰€æœ‰æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        else:
            print("éƒ¨åˆ†æ¨¡å‹åˆå§‹åŒ–å¤±è´¥")

    def init_cnn_detector(self):
        """åˆå§‹åŒ–CNNç–²åŠ³æ£€æµ‹å™¨"""
        try:
            # å°è¯•å¯¼å…¥CNNæ£€æµ‹å™¨
            from simple_cnn_detector import CNNFatigueDetector

            # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹
            model_path = './model/fatigue_model_mobilenet.h5'
            if os.path.exists(model_path):
                self.cnn_detector = CNNFatigueDetector(model_path)
                if self.cnn_detector.is_available():
                    print("âœ… CNNç–²åŠ³æ£€æµ‹å™¨å·²åŠ è½½")
                else:
                    print("âš ï¸ CNNæ£€æµ‹å™¨åŠ è½½å¤±è´¥")
                    self.cnn_detector = None
            else:
                print("âš ï¸ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„CNNæ¨¡å‹ï¼Œå°†åˆ›å»ºç®€åŒ–ç‰ˆæœ¬")
                # åˆ›å»ºä¸€ä¸ªä½¿ç”¨é¢„è®­ç»ƒResNetçš„ç®€åŒ–æ£€æµ‹å™¨
                self.cnn_detector = self._create_simple_detector()

        except ImportError as e:
            print(f"CNNæ£€æµ‹å™¨å¯¼å…¥å¤±è´¥: {e}")
            self.cnn_detector = None
        except Exception as e:
            print(f"CNNæ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.cnn_detector = None

    def _create_simple_detector(self):
        """åˆ›å»ºç®€åŒ–çš„ç–²åŠ³æ£€æµ‹å™¨"""
        class SimpleFatigueDetector:
            def __init__(self, resnet_model):
                self.resnet = resnet_model
                self.available = resnet_model is not None

            def is_available(self):
                return self.available

            def predict_fatigue(self, face_image):
                """ä½¿ç”¨ResNetç‰¹å¾è¿›è¡Œç®€å•çš„ç–²åŠ³æ£€æµ‹"""
                if not self.available or face_image is None or face_image.size == 0:
                    return None

                try:
                    # é¢„å¤„ç†å›¾åƒ
                    img = cv2.resize(face_image, (224, 224))
                    img = tf.keras.applications.resnet50.preprocess_input(img)

                    # æå–ç‰¹å¾
                    features = self.resnet.predict(np.expand_dims(img, axis=0), verbose=0)

                    # ç®€å•çš„ç–²åŠ³åˆ¤æ–­ï¼ˆåŸºäºç‰¹å¾çš„ç»Ÿè®¡ç‰¹æ€§ï¼‰
                    feature_mean = np.mean(features)
                    feature_std = np.std(features)

                    # ç®€åŒ–çš„ç–²åŠ³åˆ¤æ–­é€»è¾‘
                    fatigue_score = feature_mean * feature_std
                    confidence = min(abs(fatigue_score) * 100, 1.0)

                    is_fatigue = fatigue_score < -0.1  # é˜ˆå€¼å¯è°ƒæ•´

                    return {
                        'predicted_class': 'drowsy' if is_fatigue else 'alert',
                        'confidence': confidence,
                        'fatigue_level': 'ç–²åŠ³' if is_fatigue else 'æ­£å¸¸',
                        'fatigue_detected': is_fatigue,
                        'feature_mean': feature_mean,
                        'feature_std': feature_std
                    }

                except Exception as e:
                    print(f"ç®€åŒ–æ£€æµ‹å™¨é¢„æµ‹å¤±è´¥: {e}")
                    return None

        return SimpleFatigueDetector(self.resnet)

    def init_yawn_detector(self):
        """åˆå§‹åŒ–CNN+LSTMæ‰“å“ˆæ¬ æ£€æµ‹å™¨"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹ - ç°åœ¨æ¨¡å‹æ–‡ä»¶åœ¨modelæ–‡ä»¶å¤¹ä¸­
            model_path = './model/best_fatigue_model.pth'
            if not os.path.exists(model_path):
                # å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
                model_path = '../real_pljc/models/best_fatigue_model.pth'
            if not os.path.exists(model_path):
                # å°è¯•ç›¸å¯¹è·¯å¾„
                model_path = './real_pljc/models/best_fatigue_model.pth'
            if not os.path.exists(model_path):
                # å°è¯•ç»å¯¹è·¯å¾„
                model_path = 'D:/code/PythonProject2/real_pljc/models/best_fatigue_model.pth'

            if os.path.exists(model_path):
                self.yawn_detector = YawnDetector(model_path)
                if self.yawn_detector.is_available:
                    print("âœ… CNN+LSTMæ‰“å“ˆæ¬ æ£€æµ‹å™¨å·²åŠ è½½")
                else:
                    print("âš ï¸ CNN+LSTMæ‰“å“ˆæ¬ æ£€æµ‹å™¨åŠ è½½å¤±è´¥")
                    self.yawn_detector = None
            else:
                print("âš ï¸ æœªæ‰¾åˆ°CNN+LSTMæ‰“å“ˆæ¬ æ£€æµ‹æ¨¡å‹")
                self.yawn_detector = None

        except Exception as e:
            print(f"CNN+LSTMæ‰“å“ˆæ¬ æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.yawn_detector = None

    def _init_resnet(self):
        """åˆå§‹åŒ–ResNet50æ¨¡å‹"""
        try:
            print("æ­£åœ¨åŠ è½½ResNet50æ¨¡å‹...")
            resnet = ResNet50(weights='imagenet', include_top=False, pooling='avg')
            print("ResNet50æ¨¡å‹åŠ è½½æˆåŠŸ")
            return resnet
        except Exception as e:
            print(f"ResNet50æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return None

    def _init_cnn_model(self):
        """åˆå§‹åŒ–CNNæ¨¡å‹"""
        try:
            print("æ­£åœ¨åˆ›å»ºCNNæ¨¡å‹...")
            cnn_model = Sequential()

            if TENSORFLOW_AVAILABLE:
                cnn_model.add(Input(shape=(64, 64, 3)))

            cnn_model.add(Conv2D(32, (3, 3), activation='relu'))
            cnn_model.add(MaxPooling2D((2, 2)))
            cnn_model.add(Conv2D(64, (3, 3), activation='relu'))
            cnn_model.add(MaxPooling2D((2, 2)))
            cnn_model.add(Flatten())
            cnn_model.add(Dense(128, activation='relu'))
            cnn_model.add(Dense(1, activation='sigmoid'))
            print("CNNæ¨¡å‹åˆ›å»ºæˆåŠŸ")
            return cnn_model
        except Exception as e:
            print(f"CNNæ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
            return None

    def _init_lstm_model(self):
        """åˆå§‹åŒ–LSTMæ¨¡å‹"""
        try:
            print("æ­£åœ¨åˆ›å»ºLSTMæ¨¡å‹...")
            lstm_model = Sequential()

            if TENSORFLOW_AVAILABLE:
                lstm_model.add(Input(shape=(10, 2048)))

            lstm_model.add(LSTM(64, return_sequences=True))
            lstm_model.add(LSTM(32))
            lstm_model.add(Dense(1, activation='sigmoid'))
            print("LSTMæ¨¡å‹åˆ›å»ºæˆåŠŸ")
            return lstm_model
        except Exception as e:
            print(f"LSTMæ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
            return None

    def extract_resnet_features(self, face_img):
        """ä½¿ç”¨ResNetæå–äººè„¸ç‰¹å¾"""
        if self.resnet is None or face_img.size == 0:
            return None

        # é¢„å¤„ç†å›¾åƒ
        img = cv2.resize(face_img, (224, 224))
        img = tf.keras.applications.resnet50.preprocess_input(img)
        features = self.resnet.predict(np.expand_dims(img, axis=0))
        return features

    def detect_cnn_fatigue(self, face_img):
        """ä½¿ç”¨CNNè¿›è¡Œç–²åŠ³æ£€æµ‹"""
        if self.cnn_model is None or face_img.size == 0:
            return None

        # é¢„å¤„ç†å›¾åƒ
        img = cv2.resize(face_img, (64, 64))
        img = img / 255.0  # å½’ä¸€åŒ–
        prediction = self.cnn_model.predict(np.expand_dims(img, axis=0))
        return 'ç–²åŠ³' if prediction[0][0] > 0.5 else 'æ­£å¸¸'

    def analyze_lstm_fatigue(self, features_sequence):
        """ä½¿ç”¨LSTMåˆ†ææ—¶åºç‰¹å¾"""
        if self.lstm_model is None or not features_sequence:
            return None

        # ç¡®ä¿åºåˆ—é•¿åº¦ä¸€è‡´
        if len(features_sequence) != 10:
            return None

        prediction = self.lstm_model.predict(np.expand_dims(features_sequence, axis=0))
        return 'æ·±åº¦ç–²åŠ³' if prediction[0][0] > 0.7 else 'è½»åº¦ç–²åŠ³'

    def calculate_fatigue_score(self, blink_count, yawn_count, nod_count, time_window=60):
        """è®¡ç®—ç»¼åˆç–²åŠ³è¯„åˆ†"""
        # æƒé‡è®¾ç½®ï¼ˆåŸºäºç–²åŠ³æ£€æµ‹ç ”ç©¶ï¼‰
        blink_weight = 0.3  # çœ¨çœ¼æƒé‡
        yawn_weight = 0.5   # å“ˆæ¬ æƒé‡ï¼ˆæ›´é‡è¦ï¼‰
        nod_weight = 0.4    # ç‚¹å¤´æƒé‡

        # æ ‡å‡†åŒ–åˆ°æ¯åˆ†é’Ÿçš„é¢‘ç‡
        blink_rate = (blink_count / time_window) * 60
        yawn_rate = (yawn_count / time_window) * 60
        nod_rate = (nod_count / time_window) * 60

        # æ­£å¸¸åŸºçº¿å€¼ï¼ˆæ¯åˆ†é’Ÿï¼‰
        normal_blink_rate = 15  # æ­£å¸¸çœ¨çœ¼é¢‘ç‡
        normal_yawn_rate = 0.5  # æ­£å¸¸å“ˆæ¬ é¢‘ç‡
        normal_nod_rate = 1     # æ­£å¸¸ç‚¹å¤´é¢‘ç‡

        # è®¡ç®—åç¦»åº¦
        blink_deviation = max(0, blink_rate - normal_blink_rate) / normal_blink_rate
        yawn_deviation = max(0, yawn_rate - normal_yawn_rate) / normal_yawn_rate
        nod_deviation = max(0, nod_rate - normal_nod_rate) / normal_nod_rate

        # ç»¼åˆè¯„åˆ†
        fatigue_score = (blink_deviation * blink_weight +
                        yawn_deviation * yawn_weight +
                        nod_deviation * nod_weight)

        return min(fatigue_score, 1.0)  # é™åˆ¶åœ¨0-1ä¹‹é—´

    def get_fatigue_level(self, score):
        """æ ¹æ®è¯„åˆ†è·å–ç–²åŠ³ç­‰çº§"""
        if score < 0.2:
            return 'æ­£å¸¸'
        elif score < 0.4:
            return 'è½»å¾®ç–²åŠ³'
        elif score < 0.7:
            return 'ä¸­åº¦ç–²åŠ³'
        else:
            return 'é‡åº¦ç–²åŠ³'

    def adaptive_threshold_adjustment(self, ear, mar):
        """è‡ªé€‚åº”é˜ˆå€¼è°ƒæ•´"""
        if not self.adaptive_mode:
            return

        self.calibration_frames += 1

        # åœ¨æ ¡å‡†æœŸé—´æ”¶é›†æ•°æ®
        if self.calibration_frames <= self.calibration_period:
            # æ›´æ–°åŸºçº¿å€¼
            self.update_baseline_values(ear, mar)

            # æ ¡å‡†å®Œæˆåè°ƒæ•´é˜ˆå€¼
            if self.calibration_frames == self.calibration_period:
                self._adjust_thresholds()
                print(f"è‡ªé€‚åº”æ ¡å‡†å®Œæˆ - EARåŸºçº¿: {self.baseline_ear:.3f}, MARåŸºçº¿: {self.baseline_mar:.3f}")

        # å®šæœŸé‡æ–°æ ¡å‡†ï¼ˆæ¯1000å¸§ï¼‰
        elif self.calibration_frames % 1000 == 0:
            self._adjust_thresholds()

    def _adjust_thresholds(self):
        """æ ¹æ®åŸºçº¿å€¼è°ƒæ•´æ£€æµ‹é˜ˆå€¼"""
        if len(self.last_ear_values) >= 10:
            # åŠ¨æ€è°ƒæ•´çœ¨çœ¼é˜ˆå€¼
            ear_std = np.std(self.last_ear_values)
            self.EYE_AR_THRESH = max(0.2, self.baseline_ear - 2 * ear_std)
            self.EYE_AR_UPPER_THRESH = self.baseline_ear + 2 * ear_std

        if len(self.last_mar_values) >= 10:
            # åŠ¨æ€è°ƒæ•´å“ˆæ¬ é˜ˆå€¼
            mar_std = np.std(self.last_mar_values)
            self.MAR_THRESH = max(0.5, self.baseline_mar + 1.5 * mar_std)

    def get_detection_confidence(self, ear, mar):
        """è®¡ç®—æ£€æµ‹ç½®ä¿¡åº¦"""
        ear_confidence = 1.0
        mar_confidence = 1.0

        if len(self.last_ear_values) >= 10:
            ear_std = np.std(self.last_ear_values)
            ear_z_score = abs(ear - self.baseline_ear) / (ear_std + 1e-6)
            ear_confidence = min(1.0, ear_z_score / 3.0)  # 3-sigmaè§„åˆ™

        if len(self.last_mar_values) >= 10:
            mar_std = np.std(self.last_mar_values)
            mar_z_score = abs(mar - self.baseline_mar) / (mar_std + 1e-6)
            mar_confidence = min(1.0, mar_z_score / 3.0)

        return (ear_confidence + mar_confidence) / 2.0

    def get_head_pose(self,shape):# å¤´éƒ¨å§¿æ€ä¼°è®¡
        # ï¼ˆåƒç´ åæ ‡é›†åˆï¼‰å¡«å†™2Då‚è€ƒç‚¹ï¼Œæ³¨é‡Šéµå¾ªhttps://ibug.doc.ic.ac.uk/resources/300-W/
        # 17å·¦çœ‰å·¦ä¸Šè§’/21å·¦çœ‰å³è§’/22å³çœ‰å·¦ä¸Šè§’/26å³çœ‰å³ä¸Šè§’/36å·¦çœ¼å·¦ä¸Šè§’/39å·¦çœ¼å³ä¸Šè§’/42å³çœ¼å·¦ä¸Šè§’/
        # 45å³çœ¼å³ä¸Šè§’/31é¼»å­å·¦ä¸Šè§’/35é¼»å­å³ä¸Šè§’/48å·¦ä¸Šè§’/54å˜´å³ä¸Šè§’/57å˜´ä¸­å¤®ä¸‹è§’/8ä¸‹å·´è§’
        image_pts = np.float32([shape[17], shape[21], shape[22], shape[26], shape[36],
                                shape[39], shape[42], shape[45], shape[31], shape[35],
                                shape[48], shape[54], shape[57], shape[8]])
        # solvePnPè®¡ç®—å§¿åŠ¿â€”â€”æ±‚è§£æ—‹è½¬å’Œå¹³ç§»çŸ©é˜µï¼š
        # rotation_vecè¡¨ç¤ºæ—‹è½¬çŸ©é˜µï¼Œtranslation_vecè¡¨ç¤ºå¹³ç§»çŸ©é˜µï¼Œcam_matrixä¸KçŸ©é˜µå¯¹åº”ï¼Œdist_coeffsä¸DçŸ©é˜µå¯¹åº”ã€‚
        _, rotation_vec, translation_vec = cv2.solvePnP(self.object_pts, image_pts, self.cam_matrix, self.dist_coeffs)
        # projectPointsé‡æ–°æŠ•å½±è¯¯å·®ï¼šåŸ2dç‚¹å’Œé‡æŠ•å½±2dç‚¹çš„è·ç¦»ï¼ˆè¾“å…¥3dç‚¹ã€ç›¸æœºå†…å‚ã€ç›¸æœºç•¸å˜ã€rã€tï¼Œè¾“å‡ºé‡æŠ•å½±2dç‚¹ï¼‰
        reprojectdst, _ = cv2.projectPoints(self.reprojectsrc, rotation_vec, translation_vec, self.cam_matrix,self.dist_coeffs)
        reprojectdst = tuple(map(tuple, reprojectdst.reshape(8, 2)))# ä»¥8è¡Œ2åˆ—æ˜¾ç¤º

        # è®¡ç®—æ¬§æ‹‰è§’calc euler angle
        # å‚è€ƒhttps://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#decomposeprojectionmatrix
        rotation_mat, _ = cv2.Rodrigues(rotation_vec) #ç½—å¾·é‡Œæ ¼æ–¯å…¬å¼ï¼ˆå°†æ—‹è½¬çŸ©é˜µè½¬æ¢ä¸ºæ—‹è½¬å‘é‡ï¼‰
        pose_mat = cv2.hconcat((rotation_mat, translation_vec)) # æ°´å¹³æ‹¼æ¥ï¼Œvconcatå‚ç›´æ‹¼æ¥
        # decomposeProjectionMatrixå°†æŠ•å½±çŸ©é˜µåˆ†è§£ä¸ºæ—‹è½¬çŸ©é˜µå’Œç›¸æœºçŸ©é˜µ
        _, _, _, _, _, _, euler_angle = cv2.decomposeProjectionMatrix(pose_mat)

        # ç¡®ä¿ä»æ•°ç»„ä¸­æå–æ ‡é‡å€¼
        pitch = math.radians(euler_angle[0].item())
        yaw = math.radians(euler_angle[1].item())
        roll = math.radians(euler_angle[2].item())

        pitch = math.degrees(math.asin(math.sin(pitch)))
        roll = -math.degrees(math.asin(math.sin(roll)))
        yaw = math.degrees(math.asin(math.sin(yaw)))
        #print('pitch:{}, yaw:{}, roll:{}'.format(pitch, yaw, roll))

        return reprojectdst, euler_angle# æŠ•å½±è¯¯å·®ï¼Œæ¬§æ‹‰è§’
    def eye_aspect_ratio(self,eye):
        # å‚ç›´çœ¼æ ‡å¿—ï¼ˆXï¼ŒYï¼‰åæ ‡
        A = dist.euclidean(eye[1], eye[5])# è®¡ç®—ä¸¤ä¸ªé›†åˆä¹‹é—´çš„æ¬§å¼è·ç¦»
        B = dist.euclidean(eye[2], eye[4])
        # è®¡ç®—æ°´å¹³ä¹‹é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦»
        # æ°´å¹³çœ¼æ ‡å¿—ï¼ˆXï¼ŒYï¼‰åæ ‡
        C = dist.euclidean(eye[0], eye[3])
        # çœ¼ç›é•¿å®½æ¯”çš„è®¡ç®—
        ear = (A + B) / (2.0 * C)
        # è¿”å›çœ¼ç›çš„é•¿å®½æ¯”
        return ear

    def mouth_aspect_ratio(self,mouth):# å˜´éƒ¨
        A = np.linalg.norm(mouth[2] - mouth[9])  # 51, 59
        B = np.linalg.norm(mouth[4] - mouth[7])  # 53, 57
        C = np.linalg.norm(mouth[0] - mouth[6])  # 49, 55
        mar = (A + B) / (2.0 * C)
        return mar

    def update_baseline_values(self, ear, mar):
        """åŠ¨æ€æ›´æ–°åŸºçº¿å€¼"""
        # ä¿æŒæœ€è¿‘50ä¸ªå€¼çš„å†å²
        self.last_ear_values.append(ear)
        self.last_mar_values.append(mar)

        if len(self.last_ear_values) > 50:
            self.last_ear_values.pop(0)
        if len(self.last_mar_values) > 50:
            self.last_mar_values.pop(0)

        # æ›´æ–°åŸºçº¿å€¼ï¼ˆä½¿ç”¨ä¸­ä½æ•°ï¼Œæ›´ç¨³å®šï¼‰
        if len(self.last_ear_values) >= 10:
            self.baseline_ear = np.median(self.last_ear_values)
        if len(self.last_mar_values) >= 10:
            self.baseline_mar = np.median(self.last_mar_values)

    def is_valid_blink(self, ear):
        """æ”¹è¿›çš„çœ¨çœ¼æ£€æµ‹ - ç®€åŒ–ç‰ˆæœ¬"""
        # ä½¿ç”¨å›ºå®šé˜ˆå€¼ï¼Œæ›´å®¹æ˜“è§¦å‘
        thresh = self.EYE_AR_THRESH

        # æ£€æŸ¥æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…ï¼ˆè¿‡æ»¤å¼‚å¸¸å€¼ï¼‰
        if ear > self.EYE_AR_UPPER_THRESH or ear < 0.1:  # æ·»åŠ ä¸‹é™æ£€æŸ¥
            return False

        is_blink = ear < thresh

        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        if not hasattr(self, '_blink_detail_counter'):
            self._blink_detail_counter = 0
        self._blink_detail_counter += 1

        if self._blink_detail_counter % 60 == 0:  # æ¯60å¸§æ‰“å°ä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
            print(f"ğŸ” çœ¨çœ¼æ£€æµ‹è¯¦æƒ… - EAR: {ear:.3f}, é˜ˆå€¼: {thresh:.3f}, æ˜¯å¦çœ¨çœ¼: {is_blink}")

        return is_blink

    def is_valid_yawn(self, mar, current_time):
        """æ”¹è¿›çš„æ‰“å“ˆæ¬ æ£€æµ‹ - æ›´å®½æ¾çš„æ¡ä»¶"""
        # ä½¿ç”¨ç®€åŒ–çš„é˜ˆå€¼æ£€æµ‹
        thresh = self.MAR_THRESH

        is_mouth_open = mar > thresh

        if is_mouth_open:
            if self.yawn_start_time is None:
                self.yawn_start_time = current_time
                print(f"ğŸ” å¼€å§‹æ£€æµ‹å“ˆæ¬ ï¼ŒMAR: {mar:.3f}, é˜ˆå€¼: {thresh:.3f}")
            return False  # è¿˜åœ¨å¼ å˜´è¿‡ç¨‹ä¸­
        else:
            if self.yawn_start_time is not None:
                # æ£€æŸ¥æŒç»­æ—¶é—´
                duration = (current_time - self.yawn_start_time).total_seconds()
                self.yawn_start_time = None

                print(f"ğŸ” å“ˆæ¬ æŒç»­æ—¶é—´: {duration:.2f}ç§’")
                # è¿›ä¸€æ­¥æ”¾å®½å“ˆæ¬ æŒç»­æ—¶é—´è¦æ±‚ï¼š0.3-3.0ç§’
                is_valid = 0.3 <= duration <= 3.0
                if is_valid:
                    print(f"âœ… æœ‰æ•ˆå“ˆæ¬ ï¼ŒæŒç»­æ—¶é—´: {duration:.2f}ç§’")
                else:
                    print(f"âŒ æ— æ•ˆå“ˆæ¬ ï¼ŒæŒç»­æ—¶é—´: {duration:.2f}ç§’ï¼ˆè¦æ±‚0.3-3.0ç§’ï¼‰")
                return is_valid
            return False


    def _learning_face(self):
        """dlibçš„åˆå§‹åŒ–è°ƒç”¨ - å¢å¼ºç‰ˆæœ¬"""
        try:
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            model_path = "./model/shape_predictor_68_face_landmarks.dat"
            if not os.path.exists(model_path):
                data = {
                    'type':'msg',
                    'value':u"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·ä¸‹è½½ shape_predictor_68_face_landmarks.dat\n"
                }
                self.safe_emit_signal(data)
                return

            # åˆå§‹åŒ–æ£€æµ‹å™¨
            print("ğŸ” æ­£åœ¨åˆå§‹åŒ–dlibäººè„¸æ£€æµ‹å™¨...")
            self.detector = dlib.get_frontal_face_detector()
            print("ğŸ” æ­£åœ¨åŠ è½½ç‰¹å¾ç‚¹é¢„æµ‹å™¨...")
            self.predictor = dlib.shape_predictor(model_path)

            # éªŒè¯æ¨¡å‹åŠ è½½
            if self.detector is None or self.predictor is None:
                print("âŒ æ£€æµ‹å™¨æˆ–é¢„æµ‹å™¨ä¸ºNone")
                data = {
                    'type':'msg',
                    'value':u"âŒ äººè„¸æ£€æµ‹æ¨¡å‹åŠ è½½å¤±è´¥\n"
                }
                self.safe_emit_signal(data)
                return

            print("âœ… æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
            print(f"   æ£€æµ‹å™¨ç±»å‹: {type(self.detector)}")
            print(f"   é¢„æµ‹å™¨ç±»å‹: {type(self.predictor)}")

            data = {
                'type':'msg',
                'value':u"âœ… äººè„¸æ£€æµ‹æ¨¡å‹åŠ è½½æˆåŠŸ!!!\n"
            }
            self.safe_emit_signal(data)

        except Exception as e:
            data = {
                'type':'msg',
                'value':f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}\n"
            }
            self.safe_emit_signal(data)
            return

        (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS["left_eye"]
        (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS["right_eye"]
        (mStart, mEnd) = face_utils.FACIAL_LANDMARKS_IDXS["mouth"]

        self.cap = None

        # ç®€åŒ–çš„æ‘„åƒå¤´åˆå§‹åŒ–
        print(f"å°è¯•æ‰“å¼€æ‘„åƒå¤´ï¼Œç´¢å¼•: {self.VIDEO_STREAM}")

        # ç›´æ¥å°è¯•æ‰“å¼€æ‘„åƒå¤´ï¼Œä¸è¿‡åº¦å¤æ‚åŒ–
        success = False
        for camera_index in [self.VIDEO_STREAM, 0, 1]:  # å°è¯•å½“å‰ç´¢å¼•ã€0ã€1
            try:
                print(f"  å°è¯•æ‘„åƒå¤´ç´¢å¼• {camera_index}...")
                self.cap = cv2.VideoCapture(camera_index, cv2.CAP_DSHOW)

                if self.cap.isOpened():
                    # åŸºæœ¬è®¾ç½®
                    self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
                    self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
                    self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)

                    # æµ‹è¯•è¯»å–
                    ret, test_frame = self.cap.read()
                    if ret and test_frame is not None:
                        print(f"  âœ“ æ‘„åƒå¤´ {camera_index} å·¥ä½œæ­£å¸¸")
                        self.VIDEO_STREAM = camera_index
                        self.CAMERA_STYLE = True
                        success = True
                        data['value'] = f"æ‰“å¼€æ‘„åƒå¤´æˆåŠŸ(ç´¢å¼•{camera_index})!!!"
                        break
                    else:
                        print(f"  âœ— æ‘„åƒå¤´ {camera_index} æ— æ³•è¯»å–")
                        self.cap.release()
                        self.cap = None
                else:
                    print(f"  âœ— æ‘„åƒå¤´ {camera_index} æ— æ³•æ‰“å¼€")
                    if self.cap is not None:
                        self.cap.release()
                        self.cap = None

            except Exception as e:
                print(f"  âœ— æ‘„åƒå¤´ {camera_index} å¼‚å¸¸: {e}")
                if hasattr(self, 'cap') and self.cap is not None:
                    try:
                        self.cap.release()
                    except:
                        pass
                    self.cap = None

        if not success:
            data['value'] = u"æ‘„åƒå¤´æ‰“å¼€å¤±è´¥!!!"
            print("æœªæ‰¾åˆ°å¯ç”¨çš„æ‘„åƒå¤´è®¾å¤‡")
        else:
            # ç®€åŒ–çš„å‚æ•°è®¾ç½®
            self._optimize_camera_brightness()
        self.safe_emit_signal(data)

        # æ‰“å°æœ€ç»ˆæ‘„åƒå¤´çŠ¶æ€
        if hasattr(self, 'cap') and self.cap is not None and self.cap.isOpened():
            width = self.cap.get(cv2.CAP_PROP_FRAME_WIDTH)
            height = self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT)
            fps = self.cap.get(cv2.CAP_PROP_FPS)
            backend = self.cap.getBackendName()
            print(f"æ‘„åƒå¤´çŠ¶æ€: å·²æ‰“å¼€")
            print(f"  åˆ†è¾¨ç‡: {width}x{height}")
            print(f"  è®¾ç½®FPS: {fps}")
            print(f"  åç«¯: {backend}")

            # å¦‚æœFPSä¸º0ï¼Œå°è¯•æ‰‹åŠ¨è®¾ç½®
            if fps == 0:
                print("æ£€æµ‹åˆ°FPSä¸º0ï¼Œå°è¯•æ‰‹åŠ¨è®¾ç½®...")
                self.cap.set(cv2.CAP_PROP_FPS, 30)
                new_fps = self.cap.get(cv2.CAP_PROP_FPS)
                print(f"  é‡æ–°è®¾ç½®åFPS: {new_fps}")
        else:
            print("æœªæ‰¾åˆ°å¯ç”¨çš„è§†é¢‘æº")

        # åˆå§‹åŒ–FPSè®¡ç®—å˜é‡
        self.frame_count = 0
        self.fps_start_time = time.time()
        self.actual_fps = 0

        # åˆå§‹åŒ–æ—¶é—´å˜é‡
        t_time = datetime.datetime.now()
        e_time = datetime.datetime.now()
        h_time = datetime.datetime.now()

        # æˆåŠŸæ‰“å¼€è§†é¢‘ï¼Œå¾ªç¯è¯»å–è§†é¢‘æµ
        print("å¼€å§‹è§†é¢‘æµå¤„ç†...")

        # åˆå§‹åŒ–æ€§èƒ½ç›‘æ§å˜é‡
        frame_count = 0
        error_count = 0
        last_status_time = time.time()

        while self.is_running:
            try:
                # åœ¨å¾ªç¯å¼€å§‹æ—¶ç«‹å³æ£€æŸ¥åœæ­¢æ ‡å¿—
                if not self.is_running:
                    break

                start_time = datetime.datetime.now()
                res = ['-' for _ in range(9)]

                # åˆå§‹åŒ–CNNæ£€æµ‹ç»“æœå˜é‡ï¼Œç¡®ä¿åœ¨æ•´ä¸ªå¾ªç¯ä¸­éƒ½æœ‰å®šä¹‰
                cnn_result = None

                # ç®€å•çš„æ‘„åƒå¤´çŠ¶æ€æ£€æŸ¥
                if not hasattr(self, 'cap') or self.cap is None:
                    print("æ‘„åƒå¤´æœªåˆå§‹åŒ–")
                    break

                # ç®€åŒ–çš„æ‘„åƒå¤´æ£€æŸ¥ - åªåœ¨å¿…è¦æ—¶æ£€æŸ¥
                if not self.cap.isOpened():
                    print("æ‘„åƒå¤´è¿æ¥ä¸¢å¤±ï¼Œå°è¯•é‡æ–°è¿æ¥...")
                    if not self._reconnect_camera():
                        print("æ‘„åƒå¤´é‡è¿å¤±è´¥ï¼Œåœæ­¢æ£€æµ‹")
                        break
                    continue

                # ä¼˜åŒ–çš„å¸§è¯»å– - å‡å°‘å¡é¡¿
                flag, im_rd = self.cap.read()

                if not flag or im_rd is None or im_rd.size == 0:
                    error_count += 1
                    # å‡å°‘é”™è¯¯æŠ¥å‘Šé¢‘ç‡
                    if error_count % 30 == 0:
                        print(f"è¯»å–å¸§å¤±è´¥ï¼Œé”™è¯¯è®¡æ•°: {error_count}")

                    # å¦‚æœè¿ç»­é”™è¯¯å¤ªå¤šï¼Œå°è¯•é‡è¿
                    if error_count > 100:
                        print("é”™è¯¯è¿‡å¤šï¼Œå°è¯•é‡è¿æ‘„åƒå¤´...")
                        if not self._reconnect_camera():
                            break
                        error_count = 0

                    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å»¶è¿Ÿæ—¶é—´
                    time.sleep(camera_config.MAIN_LOOP_DELAY)
                    continue

                # æˆåŠŸè¯»å–å¸§ï¼Œé‡ç½®é”™è¯¯è®¡æ•°
                if error_count > 0:
                    error_count = 0

                frame_count += 1

                # è·³å¸§å¤„ç† - ä½¿ç”¨é…ç½®æ–‡ä»¶å‚æ•°
                skip_detection = (frame_count % camera_config.FRAME_SKIP_DETECTION != 0)

                # æ¯10ç§’æŠ¥å‘Šä¸€æ¬¡çŠ¶æ€ï¼Œå‡å°‘è¾“å‡ºé¢‘ç‡
                current_time_float = time.time()
                if current_time_float - last_status_time >= 10.0:
                    fps = frame_count / (current_time_float - last_status_time)
                    print(f"å¤„ç†çŠ¶æ€: {frame_count} å¸§, FPS: {fps:.1f}, é”™è¯¯: {error_count}")
                    frame_count = 0
                    last_status_time = current_time_float

                # æ”¹è¿›çš„å›¾åƒå¤„ç† - å¢å¼ºäººè„¸æ£€æµ‹
                try:
                    # éªŒè¯å›¾åƒæ ¼å¼å’Œå°ºå¯¸
                    if len(im_rd.shape) != 3 or im_rd.shape[2] != 3:
                        print(f"å›¾åƒæ ¼å¼å¼‚å¸¸: {im_rd.shape}")
                        continue

                    height, width = im_rd.shape[:2]
                    if height < 100 or width < 100:
                        print(f"å›¾åƒå°ºå¯¸è¿‡å°: {width}x{height}")
                        continue

                    # åº”ç”¨æš—å›¾åƒå¢å¼º - è§£å†³æ‘„åƒå¤´å¤ªæš—çš„é—®é¢˜
                    im_rd = self._enhance_dark_frame(im_rd)

                    # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
                    img_gray = cv2.cvtColor(im_rd, cv2.COLOR_BGR2GRAY)

                    # éªŒè¯ç°åº¦å›¾åƒ
                    if img_gray is None or img_gray.size == 0:
                        print("ç°åº¦è½¬æ¢å¤±è´¥")
                        continue

                    # å¢å¼ºå›¾åƒè´¨é‡ä»¥æé«˜äººè„¸æ£€æµ‹ç‡
                    mean_brightness = np.mean(img_gray)

                    # åˆ›å»ºå¢å¼ºç‰ˆæœ¬çš„å›¾åƒ
                    enhanced_gray = img_gray.copy()

                    # å¢å¼ºçš„äº®åº¦è°ƒæ•´ - é’ˆå¯¹æš—ç¯å¢ƒä¼˜åŒ–
                    if mean_brightness < 120:  # æé«˜é˜ˆå€¼ï¼Œæ›´ç§¯æåœ°å¢å¼ºæš—å›¾åƒ
                        # å›¾åƒå¤ªæš—ï¼Œå¤§å¹…å¢åŠ äº®åº¦å’Œå¯¹æ¯”åº¦
                        enhanced_gray = cv2.convertScaleAbs(enhanced_gray, alpha=1.6, beta=40)

                        # ä½¿ç”¨CLAHEè¿›è¡Œå±€éƒ¨å¯¹æ¯”åº¦å¢å¼º
                        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
                        enhanced_gray = clahe.apply(enhanced_gray)

                        # é¢å¤–çš„Gammaæ ¡æ­£æ¥æäº®æš—éƒ¨
                        gamma = 0.7  # å°äº1çš„gammaå€¼ä¼šæäº®å›¾åƒ
                        inv_gamma = 1.0 / gamma
                        table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype("uint8")
                        enhanced_gray = cv2.LUT(enhanced_gray, table)

                    elif mean_brightness > 200:
                        # å›¾åƒå¤ªäº®ï¼Œé™ä½äº®åº¦
                        enhanced_gray = cv2.convertScaleAbs(enhanced_gray, alpha=0.8, beta=-15)

                    # è½»å¾®é™å™ªï¼Œä¿æŒè¾¹ç¼˜æ¸…æ™°
                    enhanced_gray = cv2.bilateralFilter(enhanced_gray, 5, 50, 50)

                except Exception as e:
                    print(f"å›¾åƒå¤„ç†å¤±è´¥: {e}")
                    continue

                # ä¼˜åŒ–çš„äººè„¸æ£€æµ‹ - è·³å¸§å¤„ç†å‡å°‘å¡é¡¿
                faces = []

                # ä½¿ç”¨è·³å¸§ç­–ç•¥ï¼šåªåœ¨ç‰¹å®šå¸§è¿›è¡Œäººè„¸æ£€æµ‹ï¼Œå…¶ä»–å¸§ä½¿ç”¨ç¼“å­˜ç»“æœ
                if not skip_detection:
                    try:
                        # æ£€æŸ¥æ£€æµ‹å™¨æ˜¯å¦å·²åˆå§‹åŒ–
                        if not hasattr(self, 'detector') or self.detector is None:
                            if frame_count % 60 == 0:  # å‡å°‘é”™è¯¯ä¿¡æ¯é¢‘ç‡
                                print("âŒ äººè„¸æ£€æµ‹å™¨æœªåˆå§‹åŒ–")
                            faces = []
                        else:
                            # å¤šç­–ç•¥äººè„¸æ£€æµ‹ï¼Œæé«˜æ£€æµ‹æˆåŠŸç‡
                            faces = []

                            # ç­–ç•¥1: ä½¿ç”¨å¢å¼ºåçš„å›¾åƒï¼Œä¸åŒä¸Šé‡‡æ ·çº§åˆ«
                            for upsample in [0, 1]:  # å…ˆå°è¯•0ï¼ˆæ›´å¿«ï¼‰ï¼Œå†å°è¯•1
                                if len(faces) == 0:
                                    try:
                                        faces = self.detector(enhanced_gray, upsample)
                                        if len(faces) > 0:
                                            break
                                    except:
                                        continue

                            # ç­–ç•¥2: å¦‚æœå¢å¼ºå›¾åƒå¤±è´¥ï¼Œå°è¯•åŸå§‹ç°åº¦å›¾åƒ
                            if len(faces) == 0:
                                try:
                                    faces = self.detector(img_gray, 0)
                                except:
                                    pass

                            # ç­–ç•¥3: å°è¯•ç›´æ–¹å›¾å‡è¡¡åŒ–
                            if len(faces) == 0:
                                try:
                                    equalized = cv2.equalizeHist(img_gray)
                                    faces = self.detector(equalized, 0)
                                except:
                                    pass

                            # ç­–ç•¥4: å°è¯•ç¼©æ”¾å›¾åƒï¼ˆæœ‰æ—¶å°å›¾åƒæ£€æµ‹æ•ˆæœæ›´å¥½ï¼‰
                            if len(faces) == 0:
                                try:
                                    small_gray = cv2.resize(img_gray, (320, 240))
                                    small_faces = self.detector(small_gray, 0)
                                    # å°†åæ ‡ç¼©æ”¾å›åŸå›¾
                                    faces = []
                                    for face in small_faces:
                                        scaled_face = dlib.rectangle(
                                            int(face.left() * 2),
                                            int(face.top() * 2),
                                            int(face.right() * 2),
                                            int(face.bottom() * 2)
                                        )
                                        faces.append(scaled_face)
                                except:
                                    pass

                            # è¿‡æ»¤æ£€æµ‹ç»“æœ - æ›´å®½æ¾çš„è¿‡æ»¤æ¡ä»¶
                            if len(faces) > 0:
                                filtered_faces = []
                                for face in faces:
                                    width = face.right() - face.left()
                                    height = face.bottom() - face.top()
                                    # æ”¾å®½å°ºå¯¸é™åˆ¶ï¼Œæé«˜æ£€æµ‹æˆåŠŸç‡
                                    if 30 <= width <= 600 and 30 <= height <= 600:
                                        # ç¡®ä¿äººè„¸åœ¨å›¾åƒèŒƒå›´å†…
                                        if (face.left() >= 0 and face.top() >= 0 and
                                            face.right() < im_rd.shape[1] and face.bottom() < im_rd.shape[0]):
                                            filtered_faces.append(face)
                                faces = filtered_faces

                            # ç¼“å­˜æ£€æµ‹ç»“æœä¾›è·³å¸§ä½¿ç”¨
                            if hasattr(self, 'last_faces'):
                                self.last_faces = faces
                            else:
                                self.last_faces = faces

                    except Exception as e:
                        if frame_count % 60 == 0:  # å‡å°‘é”™è¯¯ä¿¡æ¯é¢‘ç‡
                            print(f"äººè„¸æ£€æµ‹å¤±è´¥: {e}")
                        faces = []
                else:
                    # è·³å¸§æ—¶ä½¿ç”¨ä¸Šæ¬¡çš„æ£€æµ‹ç»“æœ
                    if hasattr(self, 'last_faces'):
                        faces = self.last_faces
                    else:
                        faces = []
                # å¦‚æœæ£€æµ‹åˆ°äººè„¸
                if (len(faces) != 0):
                    res[0] = 'è¯†åˆ«åˆ°äººè„¸'
                    # enumerateæ–¹æ³•åŒæ—¶è¿”å›æ•°æ®å¯¹è±¡çš„ç´¢å¼•å’Œæ•°æ®ï¼Œkä¸ºç´¢å¼•ï¼Œdä¸ºfacesä¸­çš„å¯¹è±¡
                    for _, d in enumerate(faces):
                        # ç”¨çº¢è‰²çŸ©å½¢æ¡†å‡ºäººè„¸
                        cv2.rectangle(im_rd, (d.left(), d.top()), (d.right(), d.bottom()), (0, 0, 255), 1)
                        # ä½¿ç”¨é¢„æµ‹å™¨å¾—åˆ°68ç‚¹æ•°æ®çš„åæ ‡
                        shape = self.predictor(im_rd, d)
                        # åœ†åœˆæ˜¾ç¤ºæ¯ä¸ªç‰¹å¾ç‚¹
                        for i in range(68):
                            cv2.circle(im_rd, (shape.part(i).x, shape.part(i).y), 2, (0, 255, 0), -1, 8)
                        # å°†è„¸éƒ¨ç‰¹å¾ä¿¡æ¯è½¬æ¢ä¸ºæ•°ç»„arrayçš„æ ¼å¼
                        shape = face_utils.shape_to_np(shape)

                        # æå–äººè„¸åŒºåŸŸç”¨äºCNNåˆ†æ
                        face_img = im_rd[d.top():d.bottom(), d.left():d.right()]

                        # ä½¿ç”¨CNNè¿›è¡Œç–²åŠ³æ£€æµ‹ - é™ä½è°ƒç”¨é¢‘ç‡ä»¥æé«˜æ€§èƒ½
                        # åªåœ¨æ¯10å¸§è°ƒç”¨ä¸€æ¬¡CNNæ£€æµ‹ï¼Œå‡å°‘è®¡ç®—è´Ÿæ‹…
                        if (self.cnn_detector and self.cnn_detector.is_available() and
                            frame_count % 10 == 0):
                            try:
                                cnn_result = self.cnn_detector.predict_fatigue(face_img)
                                if cnn_result:
                                    # åœ¨ç•Œé¢ä¸Šæ˜¾ç¤ºCNNæ£€æµ‹ç»“æœ
                                    cnn_text = f"CNN: {cnn_result['fatigue_level']} ({cnn_result['confidence']:.2f})"
                                    cv2.putText(im_rd, cnn_text, (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
                            except Exception as e:
                                if frame_count % 60 == 0:  # å‡å°‘é”™è¯¯ä¿¡æ¯é¢‘ç‡
                                    print(f"CNNæ£€æµ‹å¤±è´¥: {e}")
                                cnn_result = None  # ç¡®ä¿åœ¨å¼‚å¸¸æƒ…å†µä¸‹é‡ç½®ä¸ºNone

                        # é¢„å…ˆè®¡ç®—EARå’ŒMARï¼Œå› ä¸ºCNN+LSTMæ¨¡å‹éœ€è¦è¿™äº›å€¼
                        # æå–å·¦çœ¼å’Œå³çœ¼åæ ‡
                        leftEye = shape[lStart:lEnd]
                        rightEye = shape[rStart:rEnd]
                        # æ„é€ å‡½æ•°è®¡ç®—å·¦å³çœ¼çš„EARå€¼ï¼Œä½¿ç”¨å¹³å‡å€¼ä½œä¸ºæœ€ç»ˆçš„EAR
                        leftEAR = self.eye_aspect_ratio(leftEye)
                        rightEAR = self.eye_aspect_ratio(rightEye)
                        ear = (leftEAR + rightEAR) / 2.0

                        # å˜´å·´åæ ‡å’ŒMARè®¡ç®—
                        mouth = shape[mStart:mEnd]
                        mar = self.mouth_aspect_ratio(mouth)

                        # è·å–å½“å‰æ—¶é—´ï¼ˆç¡®ä¿åœ¨æ‰€æœ‰åœ°æ–¹éƒ½å¯ç”¨ï¼‰
                        current_time = datetime.datetime.now()

                        """
                        æ‰“å“ˆæ¬  - é›†æˆCNN+LSTMå’Œå¯å‘å¼æ£€æµ‹ï¼ˆå¸¦å†·å´æœºåˆ¶ï¼‰
                        """
                        if self.fun[1]:
                            # ä½¿ç”¨cv2.convexHullè·å¾—å‡¸åŒ…ä½ç½®ï¼Œä½¿ç”¨drawContoursç”»å‡ºè½®å»“ä½ç½®è¿›è¡Œç”»å›¾æ“ä½œ
                            mouthHull = cv2.convexHull(mouth)
                            cv2.drawContours(im_rd, [mouthHull], -1, (0, 255, 0), 1)

                            # æ£€æŸ¥æ‰“å“ˆæ¬ å†·å´çŠ¶æ€
                            if self.last_yawn_time is not None:
                                time_since_last_yawn = (current_time - self.last_yawn_time).total_seconds()
                                if time_since_last_yawn < self.yawn_cooldown_seconds:
                                    self.yawn_detection_enabled = False
                                    # æ˜¾ç¤ºå†·å´çŠ¶æ€
                                    cooldown_remaining = self.yawn_cooldown_seconds - time_since_last_yawn
                                    cv2.putText(im_rd, f"Yawn Cooldown: {cooldown_remaining:.1f}s", (10, 160),
                                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                                else:
                                    self.yawn_detection_enabled = True

                            # CNN+LSTMæ‰“å“ˆæ¬ æ£€æµ‹ï¼ˆåªåœ¨å¯ç”¨æ—¶è¿›è¡Œï¼‰
                            cnn_lstm_yawn_detected = False
                            cnn_lstm_confidence = 0.0

                            if self.yawn_detector and self.yawn_detector.is_available:
                                try:
                                    # æå–ç‰¹å¾å¹¶æ›´æ–°ç¼“å†²åŒºï¼ˆä¼ é€’å¸§é«˜åº¦ç”¨äºå½’ä¸€åŒ–ï¼‰
                                    frame_height = im_rd.shape[0]
                                    features = self.yawn_detector.extract_features(shape, ear, mar, frame_height)
                                    if features is not None:
                                        self.yawn_detector.update_buffer(features)

                                        # è¿›è¡ŒCNN+LSTMé¢„æµ‹ï¼ˆä¼ é€’æ£€æµ‹å¯ç”¨çŠ¶æ€ï¼‰
                                        cnn_lstm_yawn_detected, cnn_lstm_confidence = self.yawn_detector.predict_yawn(self.yawn_detection_enabled)

                                        # åœ¨ç•Œé¢ä¸Šæ˜¾ç¤ºCNN+LSTMæ£€æµ‹ç»“æœ
                                        if frame_count % 10 == 0:  # æ¯10å¸§æ˜¾ç¤ºä¸€æ¬¡
                                            status = "ENABLED" if self.yawn_detection_enabled else "COOLDOWN"
                                            cnn_lstm_text = f"CNN+LSTM: {cnn_lstm_confidence:.2f} (è¿ç»­:{self.yawn_detector.fatigue_frames}) [{status}]"
                                            color = (0, 0, 255) if cnn_lstm_yawn_detected else (0, 255, 0)
                                            cv2.putText(im_rd, cnn_lstm_text, (10, 140), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

                                except Exception as e:
                                    if frame_count % 60 == 0:  # å‡å°‘é”™è¯¯ä¿¡æ¯é¢‘ç‡
                                        print(f"CNN+LSTMæ‰“å“ˆæ¬ æ£€æµ‹å¤±è´¥: {e}")

                            # åªä½¿ç”¨CNN+LSTMæ£€æµ‹æ‰“å“ˆæ¬ ï¼ˆç§»é™¤å¯å‘å¼æ£€æµ‹ï¼‰
                            yawn_detected = False
                            detection_method = ""

                            # æ³¨æ„ï¼šcnn_lstm_yawn_detectedå·²ç»åŒ…å«äº†è¿ç»­å¸§åˆ¤æ–­
                            if cnn_lstm_yawn_detected:
                                # CNN+LSTMæ£€æµ‹åˆ°æ‰“å“ˆæ¬ ï¼ˆå·²ç»é€šè¿‡è¿ç»­å¸§éªŒè¯ï¼‰
                                yawn_detected = True
                                detection_method = "CNN+LSTM"
                                print(f"ğŸ” CNN+LSTMæ£€æµ‹åˆ°æ‰“å“ˆæ¬ : ç½®ä¿¡åº¦={cnn_lstm_confidence:.3f}, è¿ç»­å¸§={self.yawn_detector.fatigue_frames}")

                            # å¦‚æœCNN+LSTMä¸å¯ç”¨ï¼Œæ˜¾ç¤ºæç¤ºä¿¡æ¯
                            if not (self.yawn_detector and self.yawn_detector.is_available):
                                # åœ¨ç•Œé¢ä¸Šæ˜¾ç¤ºCNN+LSTMä¸å¯ç”¨çš„æç¤º
                                cv2.putText(im_rd, "CNN+LSTM Yawn Detection: UNAVAILABLE", (10, 140),
                                          cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
                                if self._yawn_debug_counter % 300 == 0:  # æ¯300å¸§æç¤ºä¸€æ¬¡
                                    print("âš ï¸ CNN+LSTMæ‰“å“ˆæ¬ æ£€æµ‹ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å’ŒPyTorchå®‰è£…")

                            # æ·»åŠ å“ˆæ¬ æ£€æµ‹è°ƒè¯•ä¿¡æ¯
                            if not hasattr(self, '_yawn_debug_counter'):
                                self._yawn_debug_counter = 0
                            self._yawn_debug_counter += 1
                            if self._yawn_debug_counter % 60 == 0:  # æ¯60å¸§æ‰“å°ä¸€æ¬¡
                                if self.yawn_detector and self.yawn_detector.is_available:
                                    print(f"ğŸ” å“ˆæ¬ æ£€æµ‹ - MAR: {mar:.3f}, CNN+LSTM: {cnn_lstm_confidence:.3f}, è¿ç»­å¸§: {self.yawn_detector.fatigue_frames}")
                                else:
                                    print(f"ğŸ” å“ˆæ¬ æ£€æµ‹ - CNN+LSTMä¸å¯ç”¨ï¼Œè·³è¿‡æ£€æµ‹")

                            # åªåœ¨æ£€æµ‹å¯ç”¨ä¸”æ£€æµ‹åˆ°æ‰“å“ˆæ¬ æ—¶æ‰è®¡æ•°å’Œè®°å½•
                            if yawn_detected and self.yawn_detection_enabled:
                                self.mTOTAL += 1
                                print(f"ğŸ¥± æ£€æµ‹åˆ°å“ˆæ¬ ï¼æ–¹æ³•: {detection_method}, æ€»è®¡: {self.mTOTAL}, MAR: {mar:.3f}, CNN+LSTMç½®ä¿¡åº¦: {cnn_lstm_confidence:.3f}")
                                self.safe_emit_signal({'type':'msg','value':time.strftime('%Y-%m-%d %H:%M ', time.localtime()) + f"æ‰“å“ˆæ¬ ({detection_method})"})
                                res[4] = 'å“ˆæ¬ '

                                # è®¾ç½®å†·å´æ—¶é—´
                                self.last_yawn_time = current_time
                                self.yawn_detection_enabled = False
                                print(f"ğŸ”’ æ‰“å“ˆæ¬ æ£€æµ‹è¿›å…¥å†·å´æœŸ {self.yawn_cooldown_seconds} ç§’")

                                # è®°å½•å“ˆæ¬ äº‹ä»¶åˆ°ç»Ÿè®¡æ•°æ®åº“
                                if self.fatigue_stats:
                                    try:
                                        from fatigue_statistics import FatigueEvent
                                        # ä½¿ç”¨CNN+LSTMçš„ç½®ä¿¡åº¦
                                        confidence = cnn_lstm_confidence
                                        event = FatigueEvent(
                                            timestamp=current_time,
                                            event_type='yawn',
                                            value=1.0,
                                            confidence=confidence,
                                            additional_data={
                                                'mouth_aspect_ratio': mar,
                                                'detection_method': detection_method,
                                                'cnn_lstm_confidence': cnn_lstm_confidence,
                                                'consecutive_frames': self.yawn_detector.fatigue_frames if self.yawn_detector else 0
                                            }
                                        )
                                        self.fatigue_stats.record_event(event)
                                        print(f"ğŸ“Š è®°å½•å“ˆæ¬ äº‹ä»¶åˆ°æ•°æ®åº“ (æ–¹æ³•: {detection_method})")
                                    except Exception as e:
                                        print(f"è®°å½•å“ˆæ¬ äº‹ä»¶å¤±è´¥: {e}")
                            elif mar > self.MAR_THRESH:
                                res[4] = 'å¼ å˜´'
                                # æ·»åŠ å¼ å˜´çŠ¶æ€çš„è°ƒè¯•ä¿¡æ¯
                                if self._yawn_debug_counter % 120 == 0:  # æ¯120å¸§æ‰“å°ä¸€æ¬¡
                                    print(f"ğŸ” æ£€æµ‹åˆ°å¼ å˜´ä½†æœªè¾¾åˆ°å“ˆæ¬ æ¡ä»¶ - MAR: {mar:.3f}")
                            else:
                                res[4] = 'é—­å˜´'
                            # cv2.putText(im_rd, "COUNTER: {}".format(self.mCOUNTER), (150, 60), cv2.FONT_HERSHEY_SIMPLEX,
                            #             0.7, (0, 0, 255), 2)
                            # cv2.putText(im_rd, "MAR: {:.2f}".format(mar), (300, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7,
                            #             (0, 0, 255), 2)
                            # cv2.putText(im_rd, "Yawning: {}".format(self.mTOTAL), (450, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7,
                            #             (255, 255, 0), 2)
                        else:
                            pass
                        """
                        çœ¨çœ¼ - æ”¹è¿›ç‰ˆæœ¬
                        """
                        if self.fun[0]:
                            # æ‰§è¡Œè‡ªé€‚åº”é˜ˆå€¼è°ƒæ•´
                            self.adaptive_threshold_adjustment(ear, mar)

                            leftEyeHull = cv2.convexHull(leftEye)
                            rightEyeHull = cv2.convexHull(rightEye)
                            # ä½¿ç”¨cv2.convexHullè·å¾—å‡¸åŒ…ä½ç½®ï¼Œä½¿ç”¨drawContoursç”»å‡ºè½®å»“ä½ç½®è¿›è¡Œç”»å›¾æ“ä½œ
                            cv2.drawContours(im_rd, [leftEyeHull], -1, (0, 255, 0), 1)
                            cv2.drawContours(im_rd, [rightEyeHull], -1, (0, 255, 0), 1)

                        # ä½¿ç”¨æ”¹è¿›çš„çœ¨çœ¼æ£€æµ‹
                        if self.is_valid_blink(ear):
                            self.COUNTER += 1
                            res[5] = 'é—­çœ¼'
                            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                            if not hasattr(self, '_blink_debug_counter'):
                                self._blink_debug_counter = 0
                            self._blink_debug_counter += 1
                            if self._blink_debug_counter % 30 == 0:  # æ¯30å¸§æ‰“å°ä¸€æ¬¡
                                print(f"ğŸ” çœ¨çœ¼æ£€æµ‹ä¸­ - COUNTER: {self.COUNTER}, EAR: {ear:.3f}, é˜ˆå€¼: {self.EYE_AR_THRESH}")
                        else:
                            # å¦‚æœè¿ç»­å¸§æ•°è¾¾åˆ°é˜ˆå€¼ï¼Œåˆ™è¡¨ç¤ºè¿›è¡Œäº†ä¸€æ¬¡çœ¨çœ¼æ´»åŠ¨
                            if self.COUNTER >= self.EYE_AR_CONSEC_FRAMES:
                                self.TOTAL += 1
                                print(f"ğŸ‘ï¸ æ£€æµ‹åˆ°çœ¨çœ¼ï¼æ€»è®¡: {self.TOTAL}, EAR: {ear:.3f}, COUNTER: {self.COUNTER}")
                                self.safe_emit_signal({'type':'msg','value':time.strftime('%Y-%m-%d %H:%M ', time.localtime()) + u"çœ¨çœ¼"})

                                # è®°å½•çœ¨çœ¼äº‹ä»¶åˆ°ç»Ÿè®¡æ•°æ®åº“
                                if self.fatigue_stats:
                                    try:
                                        from fatigue_statistics import FatigueEvent
                                        event = FatigueEvent(
                                            timestamp=datetime.datetime.now(),
                                            event_type='blink',
                                            value=1.0,
                                            confidence=0.8,
                                            additional_data={'eye_aspect_ratio': ear}
                                        )
                                        self.fatigue_stats.record_event(event)
                                        print(f"ğŸ“Š è®°å½•çœ¨çœ¼äº‹ä»¶åˆ°æ•°æ®åº“")
                                    except Exception as e:
                                        print(f"è®°å½•çœ¨çœ¼äº‹ä»¶å¤±è´¥: {e}")
                            # é‡ç½®çœ¼å¸§è®¡æ•°å™¨
                            self.COUNTER = 0
                            res[5] = 'ççœ¼'
                        # ç¬¬åå››æ­¥ï¼šè¿›è¡Œç”»å›¾æ“ä½œï¼ŒåŒæ—¶ä½¿ç”¨cv2.putTextå°†çœ¨çœ¼æ¬¡æ•°è¿›è¡Œæ˜¾ç¤º
                        # cv2.putText(im_rd, "Faces: {}".format(len(faces)), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7,
                        #             (0, 0, 255), 2)
                        # cv2.putText(im_rd, "COUNTER: {}".format(self.COUNTER), (150, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7,
                        #             (0, 0, 255), 2)
                        # cv2.putText(im_rd, "EAR: {:.2f}".format(ear), (300, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7,
                        #             (0, 0, 255), 2)
                        # cv2.putText(im_rd, "Blinks: {}".format(self.TOTAL), (450, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7,
                        #             (255, 255, 0), 2)
                    else:
                        pass
                    """
                    çŒç¡ç‚¹å¤´ - æ”¹è¿›ç‰ˆæœ¬
                    """
                    if self.fun[2]:
                        # è·å–å¤´éƒ¨å§¿æ€
                        _, euler_angle = self.get_head_pose(shape)
                        pitch = abs(euler_angle[0, 0])  # å–pitchæ—‹è½¬è§’åº¦çš„ç»å¯¹å€¼
                        yaw = abs(euler_angle[1, 0])    # å–yawæ—‹è½¬è§’åº¦çš„ç»å¯¹å€¼

                        # æ”¹è¿›çš„ç‚¹å¤´æ£€æµ‹ï¼šè€ƒè™‘å¤šä¸ªè§’åº¦
                        nod_level = 'æ­£'

                        # è½»å¾®ç‚¹å¤´
                        if self.HAR_THRESH_LOW <= pitch <= self.HAR_THRESH_HIGH and yaw < 20:
                            self.hCOUNTER += 1
                            nod_level = 'è½»å¾®å€¾æ–œ'
                        # æ˜æ˜¾ç‚¹å¤´
                        elif pitch > self.HAR_THRESH_HIGH and yaw < 25:
                            self.hCOUNTER += 2  # æ˜æ˜¾ç‚¹å¤´è®¡æ•°æ›´å¤š
                            nod_level = 'æ˜æ˜¾å€¾æ–œ'
                        else:
                            # å¦‚æœè¿ç»­å¸§æ•°è¾¾åˆ°é˜ˆå€¼ï¼Œåˆ™è¡¨ç¤ºçŒç¡ç‚¹å¤´ä¸€æ¬¡
                            if self.hCOUNTER >= self.NOD_AR_CONSEC_FRAMES:
                                self.hTOTAL += 1
                                self.safe_emit_signal({'type': 'msg', 'value': time.strftime('%Y-%m-%d %H:%M ',
                                                                                                   time.localtime()) + u"çŒç¡ç‚¹å¤´"})

                                # è®°å½•ç‚¹å¤´äº‹ä»¶åˆ°ç»Ÿè®¡æ•°æ®åº“
                                if self.fatigue_stats:
                                    try:
                                        from fatigue_statistics import FatigueEvent
                                        event = FatigueEvent(
                                            timestamp=datetime.datetime.now(),
                                            event_type='nod',
                                            value=1.0,
                                            confidence=0.7,
                                            additional_data={'pitch': float(pitch), 'yaw': float(yaw)}
                                        )
                                        self.fatigue_stats.record_event(event)
                                        print(f"ğŸ“Š è®°å½•ç‚¹å¤´äº‹ä»¶åˆ°æ•°æ®åº“")
                                    except Exception as e:
                                        print(f"è®°å½•ç‚¹å¤´äº‹ä»¶å¤±è´¥: {e}")
                            # é‡ç½®ç‚¹å¤´å¸§è®¡æ•°å™¨
                            self.hCOUNTER = 0
                            nod_level = 'æ­£'

                        res[3] = nod_level
                        # ç»˜åˆ¶æ­£æ–¹ä½“12è½´(è§†é¢‘æµå°ºå¯¸è¿‡å¤§æ—¶ï¼Œreprojectdstä¼šè¶…å‡ºintèŒƒå›´ï¼Œå»ºè®®å‹ç¼©æ£€æµ‹è§†é¢‘å°ºå¯¸)
                        # for start, end in self.line_pairs:
                        #     x1, y1 = reprojectdst[start]
                        #     x2, y2 = reprojectdst[end]
                            #cv2.line(im_rd, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255))
                        # æ˜¾ç¤ºè§’åº¦ç»“æœ
                        # cv2.putText(im_rd, "X: " + "{:7.2f}".format(euler_angle[0, 0]), (10, 90),
                        #             cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), thickness=2)  # GREEN
                        # cv2.putText(im_rd, "Y: " + "{:7.2f}".format(euler_angle[1, 0]), (150, 90),
                        #             cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 0, 0), thickness=2)  # BLUE
                        # cv2.putText(im_rd, "Z: " + "{:7.2f}".format(euler_angle[2, 0]), (300, 90),
                        #             cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), thickness=2)  # RED
                        # cv2.putText(im_rd, "Nod: {}".format(self.hTOTAL), (450, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7,
                        #             (255, 255, 0), 2)
                    else:
                        pass

                # æ”¹è¿›çš„ç–²åŠ³çŠ¶æ€åˆ¤æ–­
                res[6] = str(self.TOTAL)   # çœ¨çœ¼æ¬¡æ•°
                res[7] = str(self.mTOTAL)  # å“ˆæ¬ æ¬¡æ•°
                res[8] = str(self.hTOTAL)  # ç‚¹å¤´æ¬¡æ•°

                # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼Œæ¯30ç§’æ‰“å°ä¸€æ¬¡è®¡æ•°çŠ¶æ€
                if not hasattr(self, '_debug_counter_time'):
                    self._debug_counter_time = current_time

                if (current_time - self._debug_counter_time).total_seconds() >= 30:
                    print(f"ğŸ” å½“å‰è®¡æ•°çŠ¶æ€ - çœ¨çœ¼: {self.TOTAL}, å“ˆæ¬ : {self.mTOTAL}, ç‚¹å¤´: {self.hTOTAL}")
                    print(f"ğŸ” resæ•°ç»„: {res}")
                    self._debug_counter_time = current_time

                # è®¡ç®—ç»¼åˆç–²åŠ³è¯„åˆ†
                current_time = datetime.datetime.now()
                time_window = 60  # 60ç§’æ—¶é—´çª—å£

                self.fatigue_score = self.calculate_fatigue_score(
                    self.TOTAL, self.mTOTAL, self.hTOTAL, time_window
                )

                # æ ¹æ®è¯„åˆ†ç¡®å®šç–²åŠ³ç­‰çº§
                traditional_fatigue_level = self.get_fatigue_level(self.fatigue_score)

                # é›†æˆCNNæ£€æµ‹ç»“æœ
                if cnn_result and cnn_result.get('confidence', 0) > 0.6:
                    # æ£€æŸ¥æ˜¯å¦æ£€æµ‹åˆ°ç–²åŠ³
                    is_fatigue = (cnn_result.get('predicted_class') == 'drowsy' or
                                 cnn_result.get('fatigue_level') == 'ç–²åŠ³')

                    if is_fatigue:
                        # å¦‚æœCNNæ£€æµ‹åˆ°ç–²åŠ³ä¸”ç½®ä¿¡åº¦é«˜ï¼Œä¼˜å…ˆä½¿ç”¨CNNç»“æœ
                        res[1] = cnn_result.get('fatigue_level', 'ç–²åŠ³')
                        # å‘é€CNNæ£€æµ‹æ¶ˆæ¯
                        self.safe_emit_signal({
                            'type': 'msg',
                            'value': f"CNNæ£€æµ‹åˆ°ç–²åŠ³: {cnn_result.get('fatigue_level', 'ç–²åŠ³')} (ç½®ä¿¡åº¦: {cnn_result.get('confidence', 0):.2f})"
                        })

                        # è®°å½•CNNç–²åŠ³æ£€æµ‹äº‹ä»¶
                        if self.fatigue_stats:
                            try:
                                from fatigue_statistics import FatigueEvent
                                event = FatigueEvent(
                                    timestamp=current_time,
                                    event_type='fatigue_state',
                                    value=2.0,  # CNNæ£€æµ‹åˆ°çš„ç–²åŠ³
                                    confidence=cnn_result.get('confidence', 0),
                                    additional_data={'source': 'CNN', 'level': cnn_result.get('fatigue_level', 'ç–²åŠ³')}
                                )
                                self.fatigue_stats.record_event(event)
                                print(f"ğŸ“Š è®°å½•CNNç–²åŠ³äº‹ä»¶åˆ°æ•°æ®åº“")
                            except Exception as e:
                                print(f"è®°å½•CNNç–²åŠ³äº‹ä»¶å¤±è´¥: {e}")
                    else:
                        # CNNæ£€æµ‹ä¸ºæ­£å¸¸ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•ç»“æœ
                        res[1] = traditional_fatigue_level
                else:
                    # CNNç½®ä¿¡åº¦ä¸å¤Ÿæˆ–æ— ç»“æœï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•ç»“æœ
                    res[1] = traditional_fatigue_level

                # è®°å½•ç–²åŠ³çŠ¶æ€åˆ°æ•°æ®åº“ï¼ˆæ¯10æ¬¡æ›´æ–°è®°å½•ä¸€æ¬¡ï¼Œé¿å…è¿‡äºé¢‘ç¹ï¼‰
                if self.fatigue_stats and not hasattr(self, '_fatigue_record_counter'):
                    self._fatigue_record_counter = 0

                if self.fatigue_stats:
                    self._fatigue_record_counter += 1
                    if self._fatigue_record_counter >= 10:  # æ¯10æ¬¡è®°å½•ä¸€æ¬¡
                        try:
                            fatigue_level_map = {
                                'æ­£å¸¸': 0,
                                'è½»å¾®ç–²åŠ³': 1,
                                'ä¸­åº¦ç–²åŠ³': 2,
                                'é‡åº¦ç–²åŠ³': 3
                            }
                            fatigue_level = fatigue_level_map.get(res[1], 0)
                            attention_level = max(0.0, 1.0 - fatigue_level * 0.25)

                            self.fatigue_stats.record_fatigue_state(
                                fatigue_level=fatigue_level,
                                drowsiness_prob=fatigue_level * 0.25,
                                attention_level=attention_level,
                                confidence=0.8
                            )
                            self._fatigue_record_counter = 0

                            # å¦‚æœæ£€æµ‹åˆ°ç–²åŠ³ï¼Œé¢å¤–è®°å½•ç–²åŠ³äº‹ä»¶
                            if fatigue_level > 0:
                                from fatigue_statistics import FatigueEvent
                                event = FatigueEvent(
                                    timestamp=current_time,
                                    event_type='fatigue_state',
                                    value=float(fatigue_level),
                                    confidence=0.8,
                                    additional_data={'source': 'traditional', 'level': res[1]}
                                )
                                self.fatigue_stats.record_event(event)
                                print(f"ğŸ“Š è®°å½•ç–²åŠ³çŠ¶æ€åˆ°æ•°æ®åº“: {res[1]}")
                        except Exception as e:
                            print(f"è®°å½•ç–²åŠ³çŠ¶æ€å¤±è´¥: {e}")

                # ç‰¹æ®Šæƒ…å†µæ£€æµ‹ï¼šé•¿æ—¶é—´é—­çœ¼æˆ–å¤´éƒ¨å€¾æ–œ
                if res[3] in ['è½»å¾®å€¾æ–œ', 'æ˜æ˜¾å€¾æ–œ'] and res[5] == 'é—­çœ¼':
                    if (current_time - h_time).total_seconds() >= self.values[3]:
                        res[1] = 'é‡åº¦ç–²åŠ³'
                else:
                    h_time = current_time

                if res[5] == 'é—­çœ¼':
                    if (current_time - e_time).total_seconds() >= self.values[1]:
                        res[1] = 'é‡åº¦ç–²åŠ³'
                else:
                    e_time = current_time

                # æ¯30åˆ†é’Ÿé‡ç½®è®¡æ•°å™¨ï¼ˆå»¶é•¿é‡ç½®é—´éš”ï¼Œé¿å…é¢‘ç¹æ¸…é›¶ï¼‰
                reset_interval = 1800  # 30åˆ†é’Ÿ = 1800ç§’
                if (current_time - t_time).total_seconds() >= reset_interval:
                    print(f"ğŸ“Š é‡ç½®è®¡æ•°å™¨ - çœ¨çœ¼: {self.TOTAL}, å“ˆæ¬ : {self.mTOTAL}, ç‚¹å¤´: {self.hTOTAL}")
                    # ä¿å­˜å½“å‰ç»Ÿè®¡åˆ°å†å²è®°å½•
                    if self.fatigue_stats:
                        try:
                            self.fatigue_stats.save_session_summary()
                            print("ğŸ“Š ä¼šè¯ç»Ÿè®¡å·²ä¿å­˜")
                        except Exception as e:
                            print(f"ä¿å­˜ä¼šè¯ç»Ÿè®¡å¤±è´¥: {e}")

                    # é‡ç½®è®¡æ•°å™¨
                    self.TOTAL = 0
                    self.mTOTAL = 0
                    self.hTOTAL = 0
                    t_time = current_time

                # æ²¡æœ‰æ£€æµ‹åˆ°äººè„¸çš„å¤„ç†
                if len(faces) == 0:
                    res[0] = 'æœªè¯†åˆ«åˆ°'
                    # æ²¡æœ‰æ£€æµ‹åˆ°äººè„¸
                    self.oCOUNTER += 1

                    # æ·»åŠ æ›´å‹å¥½çš„æç¤ºä¿¡æ¯
                    cv2.putText(im_rd, "No Face Detected", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)
                    cv2.putText(im_rd, "Please face the camera", (20, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2, cv2.LINE_AA)

                    # å‡å°‘"æ²¡æœ‰è¯†åˆ«åˆ°äººè„¸"æ¶ˆæ¯çš„é¢‘ç‡
                    if self.oCOUNTER >= self.OUT_AR_CONSEC_FRAMES_check * 2:  # å¢åŠ é˜ˆå€¼
                        self.safe_emit_signal({'type': 'msg', 'value': time.strftime('%Y-%m-%d %H:%M ',
                                                                                           time.localtime()) + u"æ²¡æœ‰è¯†åˆ«åˆ°äººè„¸ï¼Œè¯·è°ƒæ•´ä½ç½®"})
                        self.oCOUNTER = 0

                    # ä¸è¦é‡ç½®è®¡æ•°å™¨ï¼Œä¿æŒå†å²æ•°æ®
                    # self.TOTAL = 0
                    # self.mTOTAL = 0
                    # self.hTOTAL = 0

            # ç¡®å®šç–²åŠ³æç¤º:çœ¨çœ¼50æ¬¡ï¼Œæ‰“å“ˆæ¬ 15æ¬¡ï¼ŒçŒç¡ç‚¹å¤´30æ¬¡
            # if self.TOTAL >= 50 or self.mTOTAL >= 15 or self.hTOTAL >= 30:
            #     cv2.putText(im_rd, "SLEEP!!!", (100, 200), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 3)
            #     self.m_textCtrl3.AppendText(u"ç–²åŠ³")

                # ä¼˜åŒ–çš„å›¾åƒæ˜¾ç¤ºå¤„ç† - å‡å°‘å¡é¡¿
                try:
                    # éªŒè¯å›¾åƒæœ‰æ•ˆæ€§
                    if im_rd is None or im_rd.size == 0:
                        if frame_count % 60 == 0:  # å‡å°‘é”™è¯¯ä¿¡æ¯é¢‘ç‡
                            print("å›¾åƒæ•°æ®æ— æ•ˆï¼Œè·³è¿‡æ˜¾ç¤º")
                        continue

                    height, width = im_rd.shape[:2]

                    # ç¡®ä¿å›¾åƒæœ‰æ•ˆ
                    if height > 0 and width > 0:
                        # è½¬æ¢é¢œè‰²ç©ºé—´ä»BGRåˆ°RGB
                        RGBImg = cv2.cvtColor(im_rd, cv2.COLOR_BGR2RGB)

                        # éªŒè¯è½¬æ¢åçš„å›¾åƒ
                        if RGBImg is None or RGBImg.size == 0:
                            if frame_count % 60 == 0:  # å‡å°‘é”™è¯¯ä¿¡æ¯é¢‘ç‡
                                print("RGBè½¬æ¢å¤±è´¥")
                            continue

                        # å‡å°‘è°ƒè¯•ä¿¡æ¯çš„é¢‘ç‡ï¼Œé¿å…æ§åˆ¶å°åˆ·å±
                        if self.frame_count % 300 == 0:  # æ¯300å¸§æ‰“å°ä¸€æ¬¡ï¼ˆçº¦æ¯10ç§’ï¼‰
                            print(f"å‘é€å›¾åƒæ•°æ®: {width}x{height}, äººè„¸æ•°: {len(faces)}")

                        # å‘é€å›¾åƒæ•°æ®åˆ°UIçº¿ç¨‹ - ç¡®ä¿è§†é¢‘æµç•…æ˜¾ç¤º
                        # æ¯å¸§éƒ½å‘é€å›¾åƒæ•°æ®ä»¥ä¿è¯è§†é¢‘è¿ç»­æ€§
                        data = {'type':'img','value':RGBImg}
                        self.safe_emit_signal(data)
                    else:
                        if frame_count % 60 == 0:  # å‡å°‘é”™è¯¯ä¿¡æ¯é¢‘ç‡
                            print("å›¾åƒå°ºå¯¸æ— æ•ˆï¼Œè·³è¿‡æ˜¾ç¤º")

                except Exception as e:
                    if frame_count % 60 == 0:  # å‡å°‘é”™è¯¯ä¿¡æ¯é¢‘ç‡
                        print(f"å›¾åƒæ˜¾ç¤ºå¤„ç†å¤±è´¥: {e}")
                    # ä¸è¦continueï¼Œè®©ç¨‹åºç»§ç»­å¤„ç†å…¶ä»–æ•°æ®
                    pass

                end_time = datetime.datetime.now()

                # è®¡ç®—å®é™…FPS
                self.frame_count += 1
                current_time_float = time.time()
                if current_time_float - self.fps_start_time >= 3.0:  # æ¯3ç§’æ›´æ–°ä¸€æ¬¡
                    self.actual_fps = self.frame_count / (current_time_float - self.fps_start_time)
                    print(f"å®é™…FPS: {self.actual_fps:.1f}")
                    self.frame_count = 0
                    self.fps_start_time = current_time_float

                # å¸§æ•° - ä½¿ç”¨å®é™…FPSæˆ–è®¡ç®—çš„ç¬æ—¶FPS
                frame_time_ms = (end_time - start_time).total_seconds() * 1000
                if frame_time_ms > 0:
                    instant_fps = 1000 / frame_time_ms
                    res[2] = str(int(instant_fps))
                else:
                    res[2] = str(int(self.actual_fps)) if hasattr(self, 'actual_fps') else "0"

                data = {'type': 'res', 'value': res}
                self.safe_emit_signal(data)

            except Exception as e:
                print(f"ä¸»å¾ªç¯å¼‚å¸¸: {e}")
                error_count += 1
                if error_count > 100:
                    print("é”™è¯¯è¿‡å¤šï¼Œåœæ­¢æ£€æµ‹")
                    break
                time.sleep(0.1)  # å¼‚å¸¸åç¨ä½œç­‰å¾…
                continue

        # é‡Šæ”¾æ‘„åƒå¤´
        if hasattr(self, 'cap') and self.cap is not None:
            try:
                self.cap.release()
            except Exception as e:
                print(f"æ‘„åƒå¤´é‡Šæ”¾å¤±è´¥: {e}")
            finally:
                self.cap = None  # æ— è®ºå¦‚ä½•éƒ½è®¾ä¸ºNone

    def value_changed(self):
        try:
            if hasattr(self, 'spinBox_1'):
                self.values[0] = self.spinBox_1.value()
            if hasattr(self, 'spinBox_2'):
                self.values[1] = self.spinBox_2.value()
            if hasattr(self, 'spinBox_3'):
                self.values[2] = self.spinBox_3.value()
            if hasattr(self, 'spinBox_4'):
                self.values[3] = self.spinBox_4.value()
            if hasattr(self, 'spinBox_5'):
                self.values[4] = self.spinBox_5.value()
        except AttributeError:
            pass

    def select_changed(self):
        try:
            if hasattr(self, 'checkBox_11'):
                self.fun[0] = self.checkBox_11.isChecked()
            if hasattr(self, 'checkBox_12'):
                self.fun[1] = self.checkBox_12.isChecked()
            if hasattr(self, 'checkBox_21'):
                self.fun[2] = self.checkBox_21.isChecked()
            if hasattr(self, 'checkBox_22'):
                self.fun[3] = self.checkBox_22.isChecked()
            if hasattr(self, 'checkBox_31'):
                self.fun[4] = self.checkBox_31.isChecked()
            if hasattr(self, 'checkBox_32'):
                self.fun[5] = self.checkBox_32.isChecked()
        except AttributeError:
            pass
        pass

    def button_clicked(self):
        """å¤„ç†æŒ‰é’®ç‚¹å‡»äº‹ä»¶ - åˆ‡æ¢å¼€å§‹/åœæ­¢æ£€æµ‹"""
        if self.thread is not None and self.thread.is_alive():
            # å½“å‰æ­£åœ¨æ£€æµ‹ï¼Œç‚¹å‡»åœæ­¢
            self.stop_detection()
            if hasattr(self, 'plainTextEdit_tip'):
                self.plainTextEdit_tip.appendPlainText('æ£€æµ‹å·²åœæ­¢')
        else:
            # å½“å‰æœªæ£€æµ‹ï¼Œç‚¹å‡»å¼€å§‹
            self.start_detection()
            if hasattr(self, 'plainTextEdit_tip'):
                self.plainTextEdit_tip.appendPlainText('å¼€å§‹æ£€æµ‹')

    def start_detection(self):
        """å¼€å§‹æ£€æµ‹"""
        # å¯åŠ¨æ£€æµ‹
        self.is_running = True
        self.thread = threading.Thread(target=self._learning_face, daemon=True)
        self.thread.start()

        # æ›´æ–°æŒ‰é’®çŠ¶æ€ä¸ºæ£€æµ‹ä¸­
        self.pushButton.setText("â¹ï¸ åœæ­¢æ£€æµ‹")
        self.pushButton.setStyleSheet("""
            QPushButton {
                background: qlineargradient(x1:0, y1:0, x2:0, y2:1,
                    stop:0 #FF9800, stop:1 #F57C00);
                border: none;
                border-radius: 10px;
                color: white;
                font-weight: bold;
                padding: 15px 30px;
                font-size: 16pt;
                min-height: 40px;
            }
            QPushButton:hover {
                background: qlineargradient(x1:0, y1:0, x2:0, y2:1,
                    stop:0 #FFB74D, stop:1 #FF9800);
            }
        """)



    def stop_detection(self):
        """åœæ­¢æ£€æµ‹"""
        print("æ­£åœ¨åœæ­¢æ£€æµ‹...")

        # é¦–å…ˆè®¾ç½®åœæ­¢æ ‡å¿—
        self.is_running = False

        # ç»“æŸç»Ÿè®¡ä¼šè¯
        if hasattr(self, 'fatigue_stats') and self.fatigue_stats:
            try:
                self.fatigue_stats.end_session()
                print("âœ… ç–²åŠ³ç»Ÿè®¡ä¼šè¯å·²ç»“æŸ")
            except Exception as e:
                print(f"ç»“æŸç»Ÿè®¡ä¼šè¯å¤±è´¥: {e}")

        # é‡ç½®é”™è¯¯æ—¥å¿—æ ‡å¿—ï¼Œå…è®¸ä¸‹æ¬¡å¯åŠ¨æ—¶é‡æ–°è®°å½•
        if hasattr(self, '_signal_error_logged'):
            delattr(self, '_signal_error_logged')
        if hasattr(self, '_runtime_error_logged'):
            delattr(self, '_runtime_error_logged')
        if hasattr(self, '_unknown_error_logged'):
            delattr(self, '_unknown_error_logged')

        # å…³é—­æ‘„åƒå¤´
        if hasattr(self, 'cap') and self.cap is not None:
            try:
                self.cap.release()
                print("æ‘„åƒå¤´å·²é‡Šæ”¾")
            except Exception as e:
                print(f"æ‘„åƒå¤´é‡Šæ”¾å¤±è´¥: {e}")
            finally:
                self.cap = None
                self.CAMERA_STYLE = False

        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        if self.thread is not None and self.thread.is_alive():
            try:
                print("ç­‰å¾…æ£€æµ‹çº¿ç¨‹åœæ­¢...")
                self.thread.join(timeout=3)  # ç­‰å¾…æœ€å¤š3ç§’
                if self.thread.is_alive():
                    print("âš ï¸ æ£€æµ‹çº¿ç¨‹æœªèƒ½åœ¨3ç§’å†…åœæ­¢ï¼Œä½†ç³»ç»Ÿå·²è®¾ç½®åœæ­¢æ ‡å¿—")
                else:
                    print("âœ… æ£€æµ‹çº¿ç¨‹å·²æ­£å¸¸åœæ­¢")
            except Exception as e:
                print(f"ç­‰å¾…çº¿ç¨‹åœæ­¢æ—¶å‡ºé”™: {e}")

        # é‡ç½®æŒ‰é’®çŠ¶æ€ä¸ºå¼€å§‹æ£€æµ‹
        self.pushButton.setText("ğŸš€ å¼€å§‹æ£€æµ‹")
        self.pushButton.setStyleSheet("""
            QPushButton {
                background: qlineargradient(x1:0, y1:0, x2:0, y2:1,
                    stop:0 #4CAF50, stop:1 #45a049);
                border: none;
                border-radius: 10px;
                color: white;
                font-weight: bold;
                padding: 15px 30px;
                font-size: 16pt;
                min-height: 40px;
            }
            QPushButton:hover {
                background: qlineargradient(x1:0, y1:0, x2:0, y2:1,
                    stop:0 #5CBF60, stop:1 #4CAF50);
            }
        """)

        # æ¸…ç©ºè§†é¢‘æ˜¾ç¤º
        if hasattr(self, 'label_img'):
            self.label_img.clear()
            self.label_img.setText("ç­‰å¾…è§†é¢‘æµ...")
            self.label_img.setAlignment(Qt.AlignCenter)

        print("âœ… æ£€æµ‹å·²åœæ­¢")

    def thread_sound(self):
        """æ’­æ”¾è­¦æŠ¥å£°éŸ³"""
        try:
            # å°è¯•æ’­æ”¾éŸ³é¢‘æ–‡ä»¶
            audio_file = '1.mp3'
            if os.path.exists(audio_file):
                pygame.mixer.init()
                pygame.mixer.music.load(audio_file)
                pygame.mixer.music.play()
                time.sleep(15)
                pygame.mixer.music.stop()
            else:
                # å¦‚æœéŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç³»ç»Ÿæç¤ºéŸ³
                self.play_system_beep()
        except Exception as e:
            print(f"éŸ³é¢‘æ’­æ”¾å¤±è´¥: {e}")
            # é™çº§åˆ°ç³»ç»Ÿæç¤ºéŸ³
            self.play_system_beep()

    def play_system_beep(self):
        """æ’­æ”¾ç³»ç»Ÿæç¤ºéŸ³ä½œä¸ºæ›¿ä»£"""
        try:
            # åœ¨Windowsä¸Šä½¿ç”¨ç³»ç»Ÿæç¤ºéŸ³
            import winsound
            # æ’­æ”¾ç³»ç»Ÿè­¦å‘Šå£°
            for _ in range(3):  # æ’­æ”¾3æ¬¡
                winsound.Beep(1000, 500)  # 1000Hz, 500ms
                time.sleep(0.2)
        except ImportError:
            # å¦‚æœwinsoundä¸å¯ç”¨ï¼Œæ‰“å°è­¦å‘Š
            print("âš ï¸ ç–²åŠ³è­¦æŠ¥ï¼šè¯·æ³¨æ„ä¼‘æ¯ï¼")
        except Exception as e:
            print(f"ç³»ç»Ÿæç¤ºéŸ³æ’­æ”¾å¤±è´¥: {e}")
            print("âš ï¸ ç–²åŠ³è­¦æŠ¥ï¼šè¯·æ³¨æ„ä¼‘æ¯ï¼")

    def paly_sound(self):
        if self.sound_thread is not None and self.sound_thread.is_alive():
            # self.plainTextEdit_tip('æ’­æ”¾å£°éŸ³ä¸­')
            pass
        else:
            self.plainTextEdit_tip.appendPlainText('ç–²åŠ³é©¾é©¶ æ’­æ”¾å£°éŸ³')
            self.sound_thread = threading.Thread(target=self.thread_sound,daemon=True)
            self.sound_thread.start()
        pass

    def thread_singnal_slot(self, d):
        if d['type']=='img':
            try:
                RGBImg = d['value']

                # å‡å°‘è°ƒè¯•è¾“å‡ºé¢‘ç‡
                if not hasattr(self, '_img_debug_counter'):
                    self._img_debug_counter = 0
                self._img_debug_counter += 1

                if self._img_debug_counter % 120 == 0:  # æ¯120å¸§æ‰“å°ä¸€æ¬¡
                    print(f"æ¥æ”¶åˆ°å›¾åƒä¿¡å·: {type(RGBImg)}")

                # éªŒè¯å›¾åƒæ•°æ®
                if RGBImg is None or RGBImg.size == 0:
                    if self._img_debug_counter % 30 == 0:  # é”™è¯¯ä¿¡æ¯å‡å°‘é¢‘ç‡
                        print("æ¥æ”¶åˆ°æ— æ•ˆå›¾åƒæ•°æ®")
                    return

                # ç¡®ä¿å›¾åƒæ˜¯è¿ç»­çš„å†…å­˜å¸ƒå±€
                if not RGBImg.flags['C_CONTIGUOUS']:
                    RGBImg = np.ascontiguousarray(RGBImg)

                height, width, _ = RGBImg.shape
                bytes_per_line = 3 * width

                if self._img_debug_counter % 120 == 0:  # æ¯120å¸§æ‰“å°ä¸€æ¬¡
                    print(f"å¤„ç†å›¾åƒ: {width}x{height}")

                # å°†å›¾ç‰‡è½¬åŒ–æˆQtå¯è¯»æ ¼å¼
                qimage = QImage(RGBImg.data, width, height, bytes_per_line, QImage.Format_RGB888)

                if qimage.isNull():
                    print("QImageåˆ›å»ºå¤±è´¥")
                    return

                piximage = QtGui.QPixmap.fromImage(qimage)

                if piximage.isNull():
                    print("QPixmapåˆ›å»ºå¤±è´¥")
                    return

                # æ£€æŸ¥label_imgæ˜¯å¦å­˜åœ¨
                if not hasattr(self, 'label_img') or self.label_img is None:
                    if self._img_debug_counter % 60 == 0:  # å‡å°‘é”™è¯¯ä¿¡æ¯é¢‘ç‡
                        print("label_imgä¸å­˜åœ¨ï¼Œæ— æ³•æ˜¾ç¤ºå›¾åƒ")
                    return

                if self._img_debug_counter % 120 == 0:  # å‡å°‘è°ƒè¯•ä¿¡æ¯é¢‘ç‡
                    print(f"label_imgå­˜åœ¨ï¼Œå°ºå¯¸: {self.label_img.size()}")

                # ç›´æ¥è®¾ç½®å›¾åƒåˆ°æ ‡ç­¾ï¼Œè®©setScaledContentså¤„ç†ç¼©æ”¾
                self.label_img.setPixmap(piximage)
                if self._img_debug_counter % 120 == 0:  # å‡å°‘è°ƒè¯•ä¿¡æ¯é¢‘ç‡
                    print(f"å›¾åƒå·²è®¾ç½®åˆ°label_img: {width}x{height}")

            except Exception as e:
                print(f"å›¾åƒæ˜¾ç¤ºå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
            self.label_img.setAlignment(Qt.AlignCenter)
        elif d['type'] == 'msg':
            if hasattr(self, 'plainTextEdit_tip'):
                self.plainTextEdit_tip.appendPlainText(d['value'])
            else:
                print(f"ç³»ç»Ÿæ¶ˆæ¯: {d['value']}")
        elif d['type'] == 'res':
            # å…¼å®¹æ–°æ—§ç•Œé¢çš„æ ‡ç­¾æ›´æ–°
            try:
                if hasattr(self, 'label_11'):
                    self.label_11.setText(d['value'][0])
                if hasattr(self, 'label_12'):
                    self.label_12.setText(d['value'][1])
                if hasattr(self, 'label_13'):
                    self.label_13.setText(d['value'][2])
                if hasattr(self, 'label_21'):
                    self.label_21.setText(d['value'][3])
                if hasattr(self, 'label_22'):
                    self.label_22.setText(d['value'][4])
                if hasattr(self, 'label_23'):
                    self.label_23.setText(d['value'][5])
                if hasattr(self, 'label_31') and len(d['value']) > 6:
                    self.label_31.setText(d['value'][6])
                if hasattr(self, 'label_32') and len(d['value']) > 7:
                    self.label_32.setText(d['value'][7])
                if hasattr(self, 'label_33') and len(d['value']) > 8:
                    self.label_33.setText(d['value'][8])
            except (IndexError, AttributeError) as e:
                print(f"æ ‡ç­¾æ›´æ–°è­¦å‘Š: {e}")
            # è®¾ç½®ç–²åŠ³çŠ¶æ€é¢œè‰²
            try:
                if len(d['value']) > 1 and hasattr(self, 'label_12'):
                    if d['value'][1] == 'è½»å¾®ç–²åŠ³':
                        self.label_12.setStyleSheet("color:orange;")
                    elif d['value'][1] == 'ä¸­åº¦ç–²åŠ³':
                        self.label_12.setStyleSheet("color:yellow;")
                    elif d['value'][1] == 'é‡åº¦ç–²åŠ³':
                        self.label_12.setStyleSheet("color:red;")
                        self.paly_sound()
                    else:
                        self.label_12.setStyleSheet("color:green;")
            except (IndexError, AttributeError):
                pass

        pass

    def close(self) -> bool:
        # è®¾ç½®è¿è¡ŒçŠ¶æ€ä¸ºFalseï¼Œåœæ­¢çº¿ç¨‹
        self.is_running = False

        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        if self.thread is not None and self.thread.is_alive():
            self.thread.join(timeout=2)  # ç­‰å¾…æœ€å¤š2ç§’

        # é‡Šæ”¾OpenCVæ‘„åƒå¤´
        if hasattr(self, 'cap') and self.cap is not None:
            self.cap.release()
            print("æ‘„åƒå¤´å·²é‡Šæ”¾")

        return super(MainUI, self).close()

    def closeEvent(self, event):
        """çª—å£å…³é—­äº‹ä»¶"""
        self.is_running = False

        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        if self.thread is not None and self.thread.is_alive():
            self.thread.join(timeout=2)

        if hasattr(self, 'cap') and self.cap is not None:
            self.cap.release()

        event.accept()



if __name__ == '__main__':
    app = QtWidgets.QApplication(sys.argv)
    window = MainUI()
    window.show()
    sys.exit(app.exec())