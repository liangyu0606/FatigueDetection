<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="AugmentWebviewStateStore">
    <option name="stateMap">
      <map>
        <entry key="CHAT_STATE" value="{&quot;currentConversationId&quot;:&quot;5f99132e-e82a-4c7a-83b8-a26e78e42387&quot;,&quot;conversations&quot;:{&quot;5f99132e-e82a-4c7a-83b8-a26e78e42387&quot;:{&quot;id&quot;:&quot;5f99132e-e82a-4c7a-83b8-a26e78e42387&quot;,&quot;createdAtIso&quot;:&quot;2025-07-08T06:46:57.711Z&quot;,&quot;lastInteractedAtIso&quot;:&quot;2025-07-08T07:14:49.709Z&quot;,&quot;chatHistory&quot;:[{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;eb97ae73-945b-4c90-a518-9a279174851b&quot;,&quot;request_message&quot;:&quot;现在我要把这个项目部署到docker，使用的python版本为3.10，启动fatigue_web_ffmpeg.py，在帮我书写一个requirements.txt文档来下载所依赖的python包，同时使用linux/arm64架构来打包&quot;,&quot;rich_text_json_repr&quot;:{&quot;type&quot;:&quot;doc&quot;,&quot;content&quot;:[{&quot;type&quot;:&quot;paragraph&quot;,&quot;content&quot;:[{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;现在我要把这个项目部署到docker，使用的python版本为3.10，启动fatigue_web_ffmpeg.py，在帮我书写一个requirements.txt文档来下载所依赖的python包，同时使用linux/arm64架构来打包&quot;}]}]},&quot;mentioned_items&quot;:[],&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_01N4mzXrfMevV9ceESi1xJEC&quot;,&quot;tool_name&quot;:&quot;view&quot;,&quot;input_json&quot;:&quot;{\&quot;path\&quot;: \&quot;.\&quot;, \&quot;type\&quot;: \&quot;directory\&quot;}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;我来帮您为这个项目创建Docker部署所需的文件。首先让我了解一下项目的结构和依赖关系。\n&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:0,&quot;type&quot;:0,&quot;text_node&quot;:{&quot;content&quot;:&quot;现在我要把这个项目部署到docker，使用的python版本为3.10，启动fatigue_web_ffmpeg.py，在帮我书写一个requirements.txt文档来下载所依赖的python包，同时使用linux/arm64架构来打包&quot;}},{&quot;id&quot;:1,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;,&quot;folderRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-08T06:49:20.985Z&quot;,&quot;response_text&quot;:&quot;我来帮您为这个项目创建Docker部署所需的文件。首先让我了解一下项目的结构和依赖关系。\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;7136fb3b-b082-4877-8680-8493f4d26d81&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_01SeJKDiP8RGjhQGDFQY4KEd&quot;,&quot;tool_name&quot;:&quot;view&quot;,&quot;input_json&quot;:&quot;{\&quot;path\&quot;: \&quot;fatigue_web_ffmpeg.py\&quot;, \&quot;type\&quot;: \&quot;file\&quot;}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;现在让我查看主要的启动文件和其他Python文件来了解依赖关系：\n&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;Here's the files and directories up to 2 levels deep in ., excluding hidden items:\n./__pycache__\n./__pycache__\\config.cpython-312.pyc\n./__pycache__\\database_config.cpython-312.pyc\n./__pycache__\\model.cpython-312.pyc\n./__pycache__\\utils.cpython-312.pyc\n./config.py\n./database_config.py\n./fatigue_web_ffmpeg.py\n./model.py\n./output\n./output\\logs\n./output\\models\n./output\\processed_data\n./static\n./static\\warning.mp3\n./templates\n./templates\\dashboard_backend_ffmpeg.html\n./templates\\login.html\n./templates\\register.html\n./utils.py\n&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_01N4mzXrfMevV9ceESi1xJEC&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;,&quot;folderRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-08T06:49:28.239Z&quot;,&quot;response_text&quot;:&quot;现在让我查看主要的启动文件和其他Python文件来了解依赖关系：\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;0a0fbeac-6293-44cd-85a0-fdaa870478c4&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_01Rgv7RZZiWHvGPAvuL9C4wG&quot;,&quot;tool_name&quot;:&quot;view&quot;,&quot;input_json&quot;:&quot;{\&quot;path\&quot;: \&quot;config.py\&quot;, \&quot;type\&quot;: \&quot;file\&quot;}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;现在让我查看其他相关的Python文件来了解完整的依赖关系：\n&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;Here's the result of running `cat -n` on fatigue_web_ffmpeg.py:\n     1\t\&quot;\&quot;\&quot;\n     2\t疲劳检测Web应用 - FFmpeg后端优化版本\n     3\t使用FFmpeg进行视频压缩和优化，大幅减少网络传输数据量\n     4\t集成完整的AI疲劳检测功能\n     5\t\&quot;\&quot;\&quot;\n     6\timport cv2\n     7\timport torch\n     8\timport numpy as np\n     9\timport dlib\n    10\tfrom collections import deque\n    11\timport base64\n    12\timport json\n    13\timport subprocess\n    14\timport tempfile\n    15\timport os\n    16\timport sys\n    17\timport pygame\n    18\timport asyncio\n    19\timport time\n    20\timport threading\n    21\tfrom queue import Queue, Empty\n    22\tfrom datetime import datetime\n    23\tfrom typing import Dict, Any\n    24\t\n    25\tfrom fastapi import FastAPI, WebSocket, WebSocketDisconnect, Request, Form, HTTPException\n    26\tfrom fastapi.responses import HTMLResponse, JSONResponse, RedirectResponse\n    27\tfrom fastapi.staticfiles import StaticFiles\n    28\tfrom fastapi.templating import Jinja2Templates\n    29\timport uvicorn\n    30\t\n    31\t# 导入AI检测相关模块\n    32\ttry:\n    33\t    from config import *\n    34\t    from model import create_model\n    35\t    from utils import extract_face_landmarks, normalize_landmarks\n    36\t    from database_config import get_db_connection, init_database\n    37\t\n    38\t    # 定义模型路径\n    39\t    MODEL_PATH = os.path.join(MODEL_SAVE_PATH, \&quot;best_model.pth\&quot;)\n    40\t    AI_MODULES_AVAILABLE = True\n    41\t    print(\&quot;✅ AI模块导入成功\&quot;)\n    42\texcept ImportError as e:\n    43\t    print(f\&quot;⚠️  AI模块导入失败: {e}\&quot;)\n    44\t    print(\&quot;系统将在基础模式下运行\&quot;)\n    45\t    AI_MODULES_AVAILABLE = False\n    46\t\n    47\t    # 定义基础配置\n    48\t    MODEL_PATH = \&quot;output/models/best_model.pth\&quot;\n    49\t    DLIB_PREDICTOR_PATH = \&quot;output/models/shape_predictor_68_face_landmarks.dat\&quot;\n    50\t    SEQUENCE_LENGTH = 30\n    51\t    FACE_SIZE = (64, 64)\n    52\t\n    53\t    # 创建模拟函数\n    54\t    def create_model():\n    55\t        return None\n    56\t\n    57\t    def extract_face_landmarks(frame, detector, predictor):\n    58\t        return None, None\n    59\t\n    60\t    def normalize_landmarks(landmarks, shape):\n    61\t        return None\n    62\t\n    63\t    def init_database():\n    64\t        pass\n    65\t\n    66\tapp = FastAPI(title=\&quot;疲劳检测系统 - FFmpeg优化版\&quot;)\n    67\t\n    68\t# 静态文件和模板\n    69\tapp.mount(\&quot;/static\&quot;, StaticFiles(directory=\&quot;static\&quot;), name=\&quot;static\&quot;)\n    70\ttemplates = Jinja2Templates(directory=\&quot;templates\&quot;)\n    71\t\n    72\tclass FFmpegVideoProcessor:\n    73\t    \&quot;\&quot;\&quot;FFmpeg视频处理器\&quot;\&quot;\&quot;\n    74\t    \n    75\t    def __init__(self):\n    76\t        self.temp_dir = tempfile.mkdtemp()\n    77\t        self.frame_counter = 0\n    78\t        self.ffmpeg_path = None  # 将存储FFmpeg的路径\n    79\t\n    80\t        # FFmpeg压缩参数\n    81\t        self.compression_settings = {\n    82\t            'preset': 'veryfast',\n    83\t            'crf': '28',  # 恒定质量因子 (18-28为合理范围)\n    84\t            'scale': '320:240',\n    85\t            'fps': '5',\n    86\t            'format': 'webm'\n    87\t        }\n    88\t        \n    89\t    def check_ffmpeg(self):\n    90\t        \&quot;\&quot;\&quot;检查FFmpeg是否可用\&quot;\&quot;\&quot;\n    91\t        # 首先尝试PATH中的ffmpeg\n    92\t        try:\n    93\t            result = subprocess.run(['ffmpeg', '-version'],\n    94\t                                  capture_output=True, text=True, timeout=5)\n    95\t            if result.returncode == 0:\n    96\t                self.ffmpeg_path = 'ffmpeg'  # 使用PATH中的ffmpeg\n    97\t                return True\n    98\t        except:\n    99\t            pass\n   100\t\n   101\t        # 如果PATH中没有，尝试常见的Windows安装路径\n   102\t        common_paths = [\n   103\t            r\&quot;C:\\ffmpeg\\bin\\ffmpeg.exe\&quot;,\n   104\t            r\&quot;C:\\Program Files\\ffmpeg\\bin\\ffmpeg.exe\&quot;,\n   105\t            r\&quot;C:\\Program Files (x86)\\ffmpeg\\bin\\ffmpeg.exe\&quot;,\n   106\t            r\&quot;D:\\ffmpeg\\bin\\ffmpeg.exe\&quot;\n   107\t        ]\n   108\t\n   109\t        for path in common_paths:\n   110\t            if os.path.exists(path):\n   111\t                try:\n   112\t                    result = subprocess.run([path, '-version'],\n   113\t                                          capture_output=True, text=True, timeout=5)\n   114\t                    if result.returncode == 0:\n   115\t                        self.ffmpeg_path = path  # 使用找到的完整路径\n   116\t                        print(f\&quot;✅ 找到FFmpeg: {path}\&quot;)\n   117\t                        return True\n   118\t                except:\n   119\t                    continue\n   120\t\n   121\t        self.ffmpeg_path = None\n   122\t        return False\n   123\t    \n   124\t    def compress_frame(self, frame_data):\n   125\t        \&quot;\&quot;\&quot;使用FFmpeg压缩单帧\&quot;\&quot;\&quot;\n   126\t        try:\n   127\t            # 解码base64图像\n   128\t            if frame_data.startswith('data:image'):\n   129\t                frame_data = frame_data.split(',')[1]\n   130\t            \n   131\t            img_data = base64.b64decode(frame_data)\n   132\t            \n   133\t            # 创建临时文件\n   134\t            self.frame_counter += 1\n   135\t            input_file = os.path.join(self.temp_dir, f'input_{self.frame_counter}.png')\n   136\t            output_file = os.path.join(self.temp_dir, f'output_{self.frame_counter}.webm')\n   137\t            \n   138\t            # 写入输入文件\n   139\t            with open(input_file, 'wb') as f:\n   140\t                f.write(img_data)\n   141\t            \n   142\t            # FFmpeg压缩命令\n   143\t            cmd = [\n   144\t                self.ffmpeg_path or 'ffmpeg',  # 使用找到的FFmpeg路径\n   145\t                '-y',  # 覆盖输出文件\n   146\t                '-i', input_file,\n   147\t                '-c:v', 'libvpx-vp9',  # VP9编码器\n   148\t                '-preset', self.compression_settings['preset'],\n   149\t                '-crf', self.compression_settings['crf'],\n   150\t                '-vf', f\&quot;scale={self.compression_settings['scale']},fps={self.compression_settings['fps']}\&quot;,\n   151\t                '-f', self.compression_settings['format'],\n   152\t                '-loglevel', 'quiet',  # 静默模式\n   153\t                output_file\n   154\t            ]\n   155\t            \n   156\t            # 执行压缩\n   157\t            result = subprocess.run(cmd, capture_output=True, timeout=10)\n   158\t            \n   159\t            if result.returncode == 0 and os.path.exists(output_file):\n   160\t                # 读取压缩后的文件\n   161\t                with open(output_file, 'rb') as f:\n   162\t                    compressed_data = f.read()\n   163\t                \n   164\t                # 清理临时文件\n   165\t                self._cleanup_files([input_file, output_file])\n   166\t                \n   167\t                # 计算压缩比\n   168\t                original_size = len(img_data)\n   169\t                compressed_size = len(compressed_data)\n   170\t                compression_ratio = (1 - compressed_size / original_size) * 100\n   171\t                \n   172\t                return {\n   173\t                    'data': base64.b64encode(compressed_data).decode('utf-8'),\n   174\t                    'original_size': original_size,\n   175\t                    'compressed_size': compressed_size,\n   176\t                    'compression_ratio': compression_ratio\n   177\t                }\n   178\t            else:\n   179\t                self._cleanup_files([input_file, output_file])\n   180\t                return None\n   181\t                \n   182\t        except Exception as e:\n   183\t            print(f\&quot;FFmpeg压缩失败: {e}\&quot;)\n   184\t            return None\n   185\t    \n   186\t    def compress_frame_fast(self, frame_data):\n   187\t        \&quot;\&quot;\&quot;快速压缩模式 - 使用更激进的压缩参数\&quot;\&quot;\&quot;\n   188\t        try:\n   189\t            # 解码并转换为OpenCV格式\n   190\t            if frame_data.startswith('data:image'):\n   191\t                frame_data = frame_data.split(',')[1]\n   192\t\n   193\t            img_data = base64.b64decode(frame_data)\n   194\t            nparr = np.frombuffer(img_data, np.uint8)\n   195\t            frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n   196\t\n   197\t            if frame is None:\n   198\t                return None\n   199\t\n   200\t            # 获取目标分辨率\n   201\t            scale_parts = self.compression_settings['scale'].split(':')\n   202\t            new_width, new_height = int(scale_parts[0]), int(scale_parts[1])\n   203\t\n   204\t            # 1. 降低分辨率\n   205\t            frame_resized = cv2.resize(frame, (new_width, new_height))\n   206\t\n   207\t            # 2. 可选的图像预处理\n   208\t            if self.compression_settings.get('denoise', False):\n   209\t                frame_resized = cv2.fastNlMeansDenoisingColored(frame_resized)\n   210\t\n   211\t            # 3. 根据CRF值调整JPEG质量\n   212\t            crf = int(self.compression_settings.get('crf', '28'))\n   213\t            jpeg_quality = max(10, min(95, 100 - crf * 2))  # CRF转JPEG质量\n   214\t\n   215\t            # 4. 高压缩编码\n   216\t            encode_params = [cv2.IMWRITE_JPEG_QUALITY, jpeg_quality]\n   217\t            _, buffer = cv2.imencode('.jpg', frame_resized, encode_params)\n   218\t\n   219\t            compressed_data = buffer.tobytes()\n   220\t            original_size = len(img_data)\n   221\t            compressed_size = len(compressed_data)\n   222\t            compression_ratio = (1 - compressed_size / original_size) * 100\n   223\t\n   224\t            return {\n   225\t                'data': base64.b64encode(compressed_data).decode('utf-8'),\n   226\t                'original_size': original_size,\n   227\t                'compressed_size': compressed_size,\n   228\t                'compression_ratio': compression_ratio,\n   229\t                'method': 'opencv_fast'\n   230\t            }\n   231\t\n   232\t        except Exception as e:\n   233\t            print(f\&quot;快速压缩失败: {e}\&quot;)\n   234\t            return None\n   235\t\n   236\t    def compress_frame_webp(self, frame_data):\n   237\t        \&quot;\&quot;\&quot;使用WebP格式进行压缩 - 更好的压缩比\&quot;\&quot;\&quot;\n   238\t        try:\n   239\t            if frame_data.startswith('data:image'):\n   240\t                frame_data = frame_data.split(',')[1]\n   241\t\n   242\t            img_data = base64.b64decode(frame_data)\n   243\t            nparr = np.frombuffer(img_data, np.uint8)\n   244\t            frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n   245\t\n   246\t            if frame is None:\n   247\t                return None\n   248\t\n   249\t            # 获取目标分辨率\n   250\t            scale_parts = self.compression_settings['scale'].split(':')\n   251\t            new_width, new_height = int(scale_parts[0]), int(scale_parts[1])\n   252\t            frame_resized = cv2.resize(frame, (new_width, new_height))\n   253\t\n   254\t            # WebP压缩参数\n   255\t            crf = int(self.compression_settings.get('crf', '28'))\n   256\t            webp_quality = max(10, min(100, 100 - crf))\n   257\t\n   258\t            encode_params = [cv2.IMWRITE_WEBP_QUALITY, webp_quality]\n   259\t            _, buffer = cv2.imencode('.webp', frame_resized, encode_params)\n   260\t\n   261\t            compressed_data = buffer.tobytes()\n   262\t            original_size = len(img_data)\n   263\t            compressed_size = len(compressed_data)\n   264\t            compression_ratio = (1 - compressed_size / original_size) * 100\n   265\t\n   266\t            return {\n   267\t                'data': base64.b64encode(compressed_data).decode('utf-8'),\n   268\t                'original_size': original_size,\n   269\t                'compressed_size': compressed_size,\n   270\t                'compression_ratio': compression_ratio,\n   271\t                'method': 'webp'\n   272\t            }\n   273\t\n   274\t        except Exception as e:\n   275\t            print(f\&quot;WebP压缩失败: {e}\&quot;)\n   276\t            return None\n   277\t\n   278\t    def compress_frame_adaptive(self, frame_data):\n   279\t        \&quot;\&quot;\&quot;自适应压缩 - 根据内容复杂度选择压缩策略\&quot;\&quot;\&quot;\n   280\t        try:\n   281\t            if frame_data.startswith('data:image'):\n   282\t                frame_data = frame_data.split(',')[1]\n   283\t\n   284\t            img_data = base64.b64decode(frame_data)\n   285\t            nparr = np.frombuffer(img_data, np.uint8)\n   286\t            frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n   287\t\n   288\t            if frame is None:\n   289\t                return None\n   290\t\n   291\t            # 分析图像复杂度\n   292\t            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n   293\t            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n   294\t\n   295\t            # 根据复杂度调整压缩参数\n   296\t            if laplacian_var &gt; 1000:  # 高复杂度图像\n   297\t                quality_factor = 0.8\n   298\t            elif laplacian_var &gt; 500:  # 中等复杂度\n   299\t                quality_factor = 0.6\n   300\t            else:  # 低复杂度图像\n   301\t                quality_factor = 0.4\n   302\t\n   303\t            # 获取目标分辨率\n   304\t            scale_parts = self.compression_settings['scale'].split(':')\n   305\t            new_width, new_height = int(scale_parts[0]), int(scale_parts[1])\n   306\t            frame_resized = cv2.resize(frame, (new_width, new_height))\n   307\t\n   308\t            # 自适应质量\n   309\t            base_crf = int(self.compression_settings.get('crf', '28'))\n   310\t            adaptive_quality = max(10, min(95, int((100 - base_crf) * quality_factor)))\n   311\t\n   312\t            encode_params = [cv2.IMWRITE_JPEG_QUALITY, adaptive_quality]\n   313\t            _, buffer = cv2.imencode('.jpg', frame_resized, encode_params)\n   314\t\n   315\t            compressed_data = buffer.tobytes()\n   316\t            original_size = len(img_data)\n   317\t            compressed_size = len(compressed_data)\n   318\t            compression_ratio = (1 - compressed_size / original_size) * 100\n   319\t\n   320\t            return {\n   321\t                'data': base64.b64encode(compressed_data).decode('utf-8'),\n   322\t                'original_size': original_size,\n   323\t                'compressed_size': compressed_size,\n   324\t                'compression_ratio': compression_ratio,\n   325\t                'method': 'adaptive',\n   326\t                'complexity': laplacian_var,\n   327\t                'quality_used': adaptive_quality\n   328\t            }\n   329\t\n   330\t        except Exception as e:\n   331\t            print(f\&quot;自适应压缩失败: {e}\&quot;)\n   332\t            return None\n   333\t    \n   334\t    def _cleanup_files(self, files):\n   335\t        \&quot;\&quot;\&quot;清理临时文件\&quot;\&quot;\&quot;\n   336\t        for file in files:\n   337\t            try:\n   338\t                if os.path.exists(file):\n   339\t                    os.remove(file)\n   340\t            except:\n   341\t                pass\n   342\t    \n   343\t    def update_settings(self, settings):\n   344\t        \&quot;\&quot;\&quot;更新压缩设置\&quot;\&quot;\&quot;\n   345\t        self.compression_settings.update(settings)\n   346\t\n   347\tclass OptimizedFatigueDetectionSystem:\n   348\t    \&quot;\&quot;\&quot;优化的疲劳检测系统 - 集成完整AI检测功能\&quot;\&quot;\&quot;\n   349\t\n   350\t    def __init__(self, model_path: str):\n   351\t        self.model_path = model_path\n   352\t        self.ai_available = AI_MODULES_AVAILABLE\n   353\t\n   354\t        if self.ai_available:\n   355\t            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n   356\t            print(f\&quot; 使用设备: {self.device}\&quot;)\n   357\t\n   358\t            # 加载AI模型\n   359\t            self.model = self._load_model()\n   360\t\n   361\t            # 初始化dlib\n   362\t            self.detector = dlib.get_frontal_face_detector()\n   363\t            # 检查两个可能的dlib模型路径\n   364\t            dlib_paths = [\n   365\t                DLIB_PREDICTOR_PATH,  # config.py中定义的路径\n   366\t                \&quot;output/models/shape_predictor_68_face_landmarks.dat\&quot;  # 备用路径\n   367\t            ]\n   368\t\n   369\t            self.predictor = None\n   370\t            for dlib_path in dlib_paths:\n   371\t                if os.path.exists(dlib_path):\n   372\t                    self.predictor = dlib.shape_predictor(dlib_path)\n   373\t                    print(f\&quot;✅ dlib模型加载成功: {dlib_path}\&quot;)\n   374\t                    break\n   375\t\n   376\t            if self.predictor is None:\n   377\t                print(f\&quot;⚠️  dlib模型文件不存在，检查路径: {dlib_paths}\&quot;)\n   378\t                print(\&quot;系统将在模拟模式下运行\&quot;)\n   379\t                self.ai_available = False\n   380\t        else:\n   381\t            print(\&quot;⚠️  AI模块不可用，系统将在模拟模式下运行\&quot;)\n   382\t            self.device = None\n   383\t            self.model = None\n   384\t            self.detector = None\n   385\t            self.predictor = None\n   386\t\n   387\t        # 检测参数（保持原有逻辑）\n   388\t        class SimpleVar:\n   389\t            def __init__(self, value):\n   390\t                self._value = value\n   391\t            def get(self):\n   392\t                return self._value\n   393\t            def set(self, value):\n   394\t                self._value = value\n   395\t\n   396\t        self.yawn_threshold = SimpleVar(0.6)\n   397\t        self.mar_threshold = SimpleVar(0.5)  # 默认使用平衡模式的MAR阈值\n   398\t        self.ear_threshold = SimpleVar(0.18)  # 统一EAR阈值，适应所有眼型用户\n   399\t        self.alert_cooldown = SimpleVar(5.0)\n   400\t\n   401\t        # 检测状态\n   402\t        self.is_detecting = False\n   403\t        self.current_user = None\n   404\t        self.current_mode = \&quot;平衡模式\&quot;\n   405\t\n   406\t        # 疲劳状态评估相关（与PyQt版本保持一致）\n   407\t        self.recent_yawns = []\n   408\t        self.recent_blinks = []\n   409\t        self.fatigue_window = 30  # 30秒窗口\n   410\t        self.last_fatigue_status = \&quot;正常\&quot;  # 记录上一次的疲劳状态\n   411\t        self.last_blink_time = 0\n   412\t        self.eye_closed_frames = 0\n   413\t        self.eye_closed_threshold = 10\n   414\t        self.long_eye_closed_threshold = 60\n   415\t        self.eye_closed_start_time = None\n   416\t\n   417\t        # 缓冲区（保持原有逻辑）\n   418\t        self.face_buffer = deque(maxlen=SEQUENCE_LENGTH)\n   419\t        self.landmark_buffer = deque(maxlen=SEQUENCE_LENGTH)\n   420\t\n   421\t        # 统计变量（保持原有逻辑）\n   422\t        self.session_start_time = None\n   423\t        self.yawn_count = 0\n   424\t        self.blink_count = 0\n   425\t        self.total_predictions = 0\n   426\t        self.consecutive_yawns = 0\n   427\t        self.consecutive_threshold = 15  # 默认使用平衡模式的连续检测阈值\n   428\t        self.last_yawn_time = 0\n   429\t        self.last_detection_time = 0\n   430\t        self.no_detection_frames = 0\n   431\t        self.decay_rate = 2.0\n   432\t\n   433\t        # 眨眼检测变量（保持原有逻辑）\n   434\t        self.eye_closed_frames = 0\n   435\t        self.eye_closed_threshold = 3\n   436\t        self.long_eye_closed_threshold = 30\n   437\t        self.eye_closed_start_time = None\n   438\t        self.last_blink_time = 0\n   439\t        self.recent_blinks = []\n   440\t        self.recent_yawns = []\n   441\t        self.fatigue_window = 30.0\n   442\t        self.last_fatigue_status = \&quot;正常\&quot;\n   443\t\n   444\t        # 内部状态变量\n   445\t        self._last_mar = 0.0\n   446\t        self._last_ear = 0.3\n   447\t\n   448\t        # 音频系统\n   449\t        self.audio_path = \&quot;static/warning.mp3\&quot;\n   450\t        self.audio_initialized = False\n   451\t        self.warning_sound = None\n   452\t        self._init_audio()\n   453\t\n   454\t        # FFmpeg处理器\n   455\t        self.video_processor = FFmpegVideoProcessor()\n   456\t        self.use_ffmpeg = self.video_processor.check_ffmpeg()\n   457\t\n   458\t        if self.use_ffmpeg:\n   459\t            print(\&quot;✅ FFmpeg可用，将使用FFmpeg进行视频压缩\&quot;)\n   460\t        else:\n   461\t            print(\&quot;⚠️ FFmpeg不可用，将使用快速压缩模式\&quot;)\n   462\t\n   463\t        # 处理队列\n   464\t        self.frame_queue = Queue(maxsize=5)\n   465\t        self.processing_thread = None\n   466\t\n   467\t        # 压缩统计\n   468\t        self.compression_stats = {\n   469\t            'total_original_size': 0,\n   470\t            'total_compressed_size': 0,\n   471\t            'frames_processed': 0,\n   472\t            'avg_compression_ratio': 0\n   473\t        }\n   474\t\n   475\t        # 最新结果\n   476\t        self.latest_results = {\n   477\t            'frame': None,\n   478\t            'face_detected': False,\n   479\t            'yawn_prob': 0.0,\n   480\t            'prediction': 0,\n   481\t            'mar': 0.0,\n   482\t            'ear': 0.0,\n   483\t            'fatigue_status': '正常',\n   484\t            'consecutive_yawns': 0,\n   485\t            'session_time': '00:00',\n   486\t            'buffer_status': '0/30',\n   487\t            'total_predictions': 0,\n   488\t            'yawn_count': 0,\n   489\t            'blink_count': 0,\n   490\t            'progress': 0,\n   491\t            'compression_stats': self.compression_stats.copy()\n   492\t        }\n   493\t\n   494\t    def _load_model(self):\n   495\t        \&quot;\&quot;\&quot;加载AI模型（保持原有逻辑）\&quot;\&quot;\&quot;\n   496\t        if not self.ai_available:\n   497\t            return None\n   498\t\n   499\t        try:\n   500\t            if not os.path.exists(self.model_path):\n   501\t                print(f\&quot;⚠️  模型文件不存在: {self.model_path}\&quot;)\n   502\t                print(\&quot;系统将在模拟模式下运行\&quot;)\n   503\t                return None\n   504\t\n   505\t            model = create_model().to(self.device)\n   506\t            checkpoint = torch.load(self.model_path, map_location=self.device)\n   507\t            model.load_state_dict(checkpoint['model_state_dict'])\n   508\t            model.eval()\n   509\t            print(\&quot;✅ AI模型加载成功\&quot;)\n   510\t            return model\n   511\t        except Exception as e:\n   512\t            print(f\&quot;❌ AI模型加载失败: {e}\&quot;)\n   513\t            print(\&quot;系统将在模拟模式下运行\&quot;)\n   514\t            return None\n   515\t\n   516\t    def _init_audio(self):\n   517\t        \&quot;\&quot;\&quot;初始化音频系统\&quot;\&quot;\&quot;\n   518\t        try:\n   519\t            pygame.mixer.init()\n   520\t            if os.path.exists(self.audio_path):\n   521\t                self.warning_sound = pygame.mixer.Sound(self.audio_path)\n   522\t                self.audio_initialized = True\n   523\t                print(\&quot;✅ 音频系统初始化成功\&quot;)\n   524\t            else:\n   525\t                print(f\&quot;❌ 警告音频文件不存在: {self.audio_path}\&quot;)\n   526\t                self.audio_initialized = False\n   527\t        except Exception as e:\n   528\t            print(f\&quot;❌ 音频系统初始化失败: {e}\&quot;)\n   529\t            self.audio_initialized = False\n   530\t\n   531\t    def _play_warning_sound(self):\n   532\t        \&quot;\&quot;\&quot;播放警告音频\&quot;\&quot;\&quot;\n   533\t        if self.audio_initialized and self.warning_sound:\n   534\t            try:\n   535\t                self.warning_sound.play()\n   536\t                print(\&quot; 播放警告音频\&quot;)\n   537\t            except Exception as e:\n   538\t                print(f\&quot;❌ 播放音频失败: {e}\&quot;)\n   539\t\n   540\t    def _preprocess_frame(self, frame):\n   541\t        \&quot;\&quot;\&quot;预处理帧（保持原有逻辑）\&quot;\&quot;\&quot;\n   542\t        if self.predictor is None:\n   543\t            return None, None, None\n   544\t\n   545\t        face_img, landmarks = extract_face_landmarks(frame, self.detector, self.predictor)\n   546\t\n   547\t        if face_img is None or landmarks is None:\n   548\t            return None, None, None\n   549\t\n   550\t        face_resized = cv2.resize(face_img, FACE_SIZE)\n   551\t        landmarks_norm = normalize_landmarks(landmarks, face_img.shape[:2])\n   552\t\n   553\t        # 获取人脸区域\n   554\t        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n   555\t        faces = self.detector(gray)\n   556\t        face_rect = faces[0] if len(faces) &gt; 0 else None\n   557\t\n   558\t        return face_resized, landmarks_norm, face_rect\n   559\t\n   560\t    def _predict_yawn(self):\n   561\t        \&quot;\&quot;\&quot;预测打哈欠（保持原有逻辑）\&quot;\&quot;\&quot;\n   562\t        if self.model is None or len(self.face_buffer) &lt; SEQUENCE_LENGTH:\n   563\t            return 0.0, 0\n   564\t\n   565\t        faces = np.array(list(self.face_buffer))\n   566\t        landmarks = np.array(list(self.landmark_buffer))\n   567\t\n   568\t        faces_tensor = torch.from_numpy(faces).float().unsqueeze(0)\n   569\t        landmarks_tensor = torch.from_numpy(landmarks).float().unsqueeze(0)\n   570\t\n   571\t        faces_tensor = faces_tensor.permute(0, 1, 4, 2, 3)\n   572\t        landmarks_tensor = landmarks_tensor.reshape(1, SEQUENCE_LENGTH, -1)\n   573\t        faces_tensor = faces_tensor / 255.0\n   574\t\n   575\t        faces_tensor = faces_tensor.to(self.device)\n   576\t        landmarks_tensor = landmarks_tensor.to(self.device)\n   577\t\n   578\t        with torch.no_grad():\n   579\t            outputs = self.model(faces_tensor, landmarks_tensor)\n   580\t            probabilities = torch.softmax(outputs, dim=1)\n   581\t            yawn_prob = probabilities[0, 1].item()\n   582\t            prediction = 1 if yawn_prob &gt; self.yawn_threshold.get() else 0\n   583\t\n   584\t        return yawn_prob, prediction\n   585\t\n   586\t    def _calculate_mouth_aspect_ratio(self, landmarks):\n   587\t        \&quot;\&quot;\&quot;计算嘴部长宽比(MAR)（保持原有逻辑）\&quot;\&quot;\&quot;\n   588\t        try:\n   589\t            mouth_points = landmarks[48:68]\n   590\t            A = np.linalg.norm(mouth_points[13] - mouth_points[19])\n   591\t            B = np.linalg.norm(mouth_points[14] - mouth_points[18])\n   592\t            C = np.linalg.norm(mouth_points[15] - mouth_points[17])\n   593\t            D = np.linalg.norm(mouth_points[0] - mouth_points[6])\n   594\t            mar = (A + B + C) / (3.0 * D)\n   595\t            self._last_mar = mar\n   596\t            return mar\n   597\t        except:\n   598\t            return 0.0\n   599\t\n   600\t    def _calculate_eye_aspect_ratio(self, landmarks):\n   601\t        \&quot;\&quot;\&quot;计算眼部长宽比(EAR)（保持原有逻辑）\&quot;\&quot;\&quot;\n   602\t        try:\n   603\t            left_eye = landmarks[36:42]\n   604\t            right_eye = landmarks[42:48]\n   605\t\n   606\t            def eye_aspect_ratio(eye_points):\n   607\t                A = np.linalg.norm(eye_points[1] - eye_points[5])\n   608\t                B = np.linalg.norm(eye_points[2] - eye_points[4])\n   609\t                C = np.linalg.norm(eye_points[0] - eye_points[3])\n   610\t                ear = (A + B) / (2.0 * C)\n   611\t                return ear\n   612\t\n   613\t            left_ear = eye_aspect_ratio(left_eye)\n   614\t            right_ear = eye_aspect_ratio(right_eye)\n   615\t            avg_ear = (left_ear + right_ear) / 2.0\n   616\t            self._last_ear = avg_ear\n   617\t\n   618\t            return avg_ear\n   619\t        except:\n   620\t            return 0.3\n   621\t\n   622\t    def _detect_blink(self, ear):\n   623\t        \&quot;\&quot;\&quot;检测眨眼和长时间闭眼（使用可配置的EAR阈值）\&quot;\&quot;\&quot;\n   624\t        ear_threshold = self.ear_threshold.get()\n   625\t        current_time = time.time()\n   626\t\n   627\t        if ear &lt; ear_threshold:\n   628\t            if self.eye_closed_frames == 0:\n   629\t                self.eye_closed_start_time = current_time\n   630\t            self.eye_closed_frames += 1\n   631\t        else:\n   632\t            if self.eye_closed_frames &gt;= self.eye_closed_threshold:\n   633\t                if current_time - self.last_blink_time &gt; 0.3:\n   634\t                    self.blink_count += 1\n   635\t                    self.last_blink_time = current_time\n   636\t                    self.recent_blinks.append(current_time)\n   637\t                    return True\n   638\t\n   639\t            self.eye_closed_frames = 0\n   640\t            self.eye_closed_start_time = None\n   641\t\n   642\t        return False\n   643\t\n   644\t    def _evaluate_fatigue_status(self):\n   645\t        \&quot;\&quot;\&quot;评估疲劳状态（与PyQt版本保持一致）\&quot;\&quot;\&quot;\n   646\t        current_time = time.time()\n   647\t\n   648\t        # 清理过期的记录（30秒窗口）\n   649\t        self.recent_yawns = [t for t in self.recent_yawns if current_time - t &lt;= self.fatigue_window]\n   650\t        self.recent_blinks = [t for t in self.recent_blinks if current_time - t &lt;= self.fatigue_window]\n   651\t\n   652\t        yawn_count_30s = len(self.recent_yawns)  # 30秒窗口内的打哈欠次数\n   653\t        long_eye_closed = self.eye_closed_frames &gt;= self.long_eye_closed_threshold\n   654\t\n   655\t        # 疲劳状态判断逻辑（与PyQt版本完全一致）\n   656\t        if yawn_count_30s &gt;= 3 or long_eye_closed:\n   657\t            return \&quot;重度疲劳\&quot;\n   658\t        elif yawn_count_30s &gt;= 2:\n   659\t            return \&quot;中度疲劳\&quot;\n   660\t        elif yawn_count_30s &gt;= 1:\n   661\t            return \&quot;轻度疲劳\&quot;\n   662\t        else:\n   663\t            return \&quot;正常\&quot;\n   664\t\n   665\t    def _draw_face_landmarks(self, frame, face_rect, landmarks_norm):\n   666\t        \&quot;\&quot;\&quot;在人脸上绘制特征点和人脸框（增强显示效果）\&quot;\&quot;\&quot;\n   667\t        if face_rect is None:\n   668\t            print(\&quot;⚠️  face_rect为None，跳过绘制\&quot;)\n   669\t            return frame\n   670\t\n   671\t        # 绘制人脸框 - 使用更粗的线条和更亮的颜色\n   672\t        x, y, w, h = face_rect.left(), face_rect.top(), face_rect.width(), face_rect.height()\n   673\t        print(f\&quot; 开始绘制人脸框: ({x}, {y}, {w}, {h})\&quot;)\n   674\t\n   675\t        # 绘制多层人脸框以确保可见性 - 使用更粗的线条\n   676\t        cv2.rectangle(frame, (x-3, y-3), (x + w + 3, y + h + 3), (0, 255, 0), 6)  # 外层绿框（更粗）\n   677\t        cv2.rectangle(frame, (x-1, y-1), (x + w + 1, y + h + 1), (255, 255, 255), 4)  # 中层白框\n   678\t        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # 内层绿框\n   679\t        print(f\&quot;✅ 人脸框绘制完成\&quot;)\n   680\t\n   681\t        # 如果有归一化的landmarks，需要转换回原始坐标\n   682\t        if landmarks_norm is not None and self.ai_available:\n   683\t            print(f\&quot; 开始绘制特征点，AI可用: {self.ai_available}\&quot;)\n   684\t            # 重新获取原始landmarks来绘制\n   685\t            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n   686\t            faces = self.detector(gray)\n   687\t            if len(faces) &gt; 0:\n   688\t                landmarks = self.predictor(gray, faces[0])\n   689\t                print(f\&quot; 获取到landmarks，开始绘制68个特征点\&quot;)\n   690\t\n   691\t                # 绘制68个特征点 - 使用更大更明显的点\n   692\t                for i in range(68):\n   693\t                    x_point = landmarks.part(i).x\n   694\t                    y_point = landmarks.part(i).y\n   695\t\n   696\t                    # 根据不同区域使用不同颜色和大小\n   697\t                    if i &lt; 17:  # 下巴轮廓\n   698\t                        color = (255, 255, 0)  # 青色\n   699\t                        radius = 3\n   700\t                    elif i &lt; 22:  # 右眉毛\n   701\t                        color = (0, 255, 255)  # 黄色\n   702\t                        radius = 3\n   703\t                    elif i &lt; 27:  # 左眉毛\n   704\t                        color = (0, 255, 255)  # 黄色\n   705\t                        radius = 3\n   706\t                    elif i &lt; 36:  # 鼻子\n   707\t                        color = (255, 0, 255)  # 紫色\n   708\t                        radius = 3\n   709\t                    elif i &lt; 42:  # 右眼\n   710\t                        color = (255, 0, 0)    # 蓝色\n   711\t                        radius = 4  # 眼部特征点稍大\n   712\t                    elif i &lt; 48:  # 左眼\n   713\t                        color = (255, 0, 0)    # 蓝色\n   714\t                        radius = 4  # 眼部特征点稍大\n   715\t                    else:  # 嘴部\n   716\t                        color = (0, 0, 255)    # 红色\n   717\t                        radius = 4  # 嘴部特征点稍大\n   718\t\n   719\t                    # 绘制更大的特征点，带黑色边框增强对比度\n   720\t                    cv2.circle(frame, (x_point, y_point), radius + 2, (0, 0, 0), -1)  # 黑色底\n   721\t                    cv2.circle(frame, (x_point, y_point), radius + 1, (255, 255, 255), -1)  # 白色中层\n   722\t                    cv2.circle(frame, (x_point, y_point), radius, color, -1)  # 彩色点\n   723\t\n   724\t                # 绘制关键区域的连线 - 使用更粗的线条和更好的对比度\n   725\t                # 眼部轮廓\n   726\t                for eye_start, eye_end in [(36, 42), (42, 48)]:\n   727\t                    eye_points = []\n   728\t                    for i in range(eye_start, eye_end):\n   729\t                        eye_points.append((landmarks.part(i).x, landmarks.part(i).y))\n   730\t                    eye_points = np.array(eye_points, np.int32)\n   731\t                    cv2.polylines(frame, [eye_points], True, (0, 0, 0), 5)  # 黑色底线\n   732\t                    cv2.polylines(frame, [eye_points], True, (255, 255, 255), 3)  # 白色中线\n   733\t                    cv2.polylines(frame, [eye_points], True, (255, 0, 0), 2)  # 蓝色线\n   734\t\n   735\t                # 嘴部轮廓\n   736\t                mouth_points = []\n   737\t                for i in range(48, 68):\n   738\t                    mouth_points.append((landmarks.part(i).x, landmarks.part(i).y))\n   739\t                mouth_points = np.array(mouth_points, np.int32)\n   740\t                cv2.polylines(frame, [mouth_points], True, (0, 0, 0), 5)  # 黑色底线\n   741\t                cv2.polylines(frame, [mouth_points], True, (255, 255, 255), 3)  # 白色中线\n   742\t                cv2.polylines(frame, [mouth_points], True, (0, 0, 255), 2)  # 红色线\n   743\t        else:\n   744\t            # 如果AI不可用，只绘制人脸框，不添加文字标识\n   745\t            print(f\&quot;⚠️  AI不可用或landmarks为None，只绘制人脸框\&quot;)\n   746\t\n   747\t        print(f\&quot;✅ 人脸框和特征点绘制完成\&quot;)\n   748\t\n   749\t        return frame\n   750\t\n   751\t    def _record_fatigue_status(self, fatigue_status):\n   752\t        \&quot;\&quot;\&quot;记录疲劳状态到数据库（与PyQt版本保持一致）\&quot;\&quot;\&quot;\n   753\t        # 只在疲劳状态发生变化时记录（与PyQt版本逻辑一致）\n   754\t        if fatigue_status != self.last_fatigue_status:\n   755\t            if fatigue_status == \&quot;轻度疲劳\&quot; and self.last_fatigue_status == \&quot;正常\&quot;:\n   756\t                self._save_fatigue_record(\&quot;轻度疲劳\&quot;)\n   757\t                print(\&quot;⚠️ 轻度疲劳警告\&quot;)\n   758\t            elif fatigue_status == \&quot;中度疲劳\&quot; and self.last_fatigue_status in [\&quot;正常\&quot;, \&quot;轻度疲劳\&quot;]:\n   759\t                self._save_fatigue_record(\&quot;中度疲劳\&quot;)\n   760\t                print(\&quot;⚠️⚠️ 中度疲劳警告\&quot;)\n   761\t            elif fatigue_status == \&quot;重度疲劳\&quot; and self.last_fatigue_status in [\&quot;正常\&quot;, \&quot;轻度疲劳\&quot;, \&quot;中度疲劳\&quot;]:\n   762\t                self._save_fatigue_record(\&quot;重度疲劳\&quot;)\n   763\t                print(\&quot; 重度疲劳警告\&quot;)\n   764\t            elif fatigue_status == \&quot;正常\&quot;:\n   765\t                print(\&quot;✅ 疲劳状态恢复正常\&quot;)\n   766\t\n   767\t            # 播放警告音频（如果状态变化且非正常）\n   768\t            if fatigue_status != \&quot;正常\&quot;:\n   769\t                self._play_warning_sound()\n   770\t\n   771\t            # 更新上一次疲劳状态\n   772\t            self.last_fatigue_status = fatigue_status\n   773\t\n   774\t    def _save_fatigue_record(self, fatigue_level):\n   775\t        \&quot;\&quot;\&quot;保存疲劳记录到数据库（与PyQt版本保持一致）\&quot;\&quot;\&quot;\n   776\t        if not self.current_user:\n   777\t            return\n   778\t\n   779\t        try:\n   780\t            if AI_MODULES_AVAILABLE:\n   781\t                with get_db_connection() as conn:\n   782\t                    cursor = conn.cursor()\n   783\t                    cursor.execute('''\n   784\t                        INSERT INTO fatigue_records\n   785\t                        (username, timestamp, fatigue_level)\n   786\t                        VALUES (%s, %s, %s)\n   787\t                    ''', (\n   788\t                        self.current_user['username'],\n   789\t                        datetime.now(),\n   790\t                        fatigue_level\n   791\t                    ))\n   792\t                    conn.commit()\n   793\t\n   794\t                print(f\&quot; 疲劳记录已保存: {self.current_user['username']} - {fatigue_level}\&quot;)\n   795\t            else:\n   796\t                print(f\&quot; 疲劳记录（模拟）: {self.current_user['username']} - {fatigue_level}\&quot;)\n   797\t\n   798\t        except Exception as e:\n   799\t            print(f\&quot;❌ 保存疲劳记录失败: {e}\&quot;)\n   800\t\n   801\t    def apply_preset(self, mode):\n   802\t        \&quot;\&quot;\&quot;应用预设模式（EAR阈值保持不变）\&quot;\&quot;\&quot;\n   803\t        if mode == 'sensitive':\n   804\t            self.yawn_threshold.set(0.6)  # 保持模型阈值不变\n   805\t            self.mar_threshold.set(0.45)  # MAR阈值调整为0.45\n   806\t            # EAR阈值保持不变，统一为0.18\n   807\t            self.consecutive_threshold = 10  # 连续检测阈值10帧\n   808\t            self.alert_cooldown.set(3.0)\n   809\t            self.current_mode = \&quot;敏感模式\&quot;\n   810\t        elif mode == 'balanced':\n   811\t            self.yawn_threshold.set(0.6)  # 保持模型阈值不变\n   812\t            self.mar_threshold.set(0.5)   # MAR阈值调整为0.5\n   813\t            # EAR阈值保持不变，统一为0.18\n   814\t            self.consecutive_threshold = 15  # 连续检测阈值15帧\n   815\t            self.alert_cooldown.set(5.0)\n   816\t            self.current_mode = \&quot;平衡模式\&quot;\n   817\t        elif mode == 'conservative':\n   818\t            self.yawn_threshold.set(0.6)  # 保持模型阈值不变\n   819\t            self.mar_threshold.set(0.55)  # MAR阈值调整为0.55\n   820\t            # EAR阈值保持不变，统一为0.18\n   821\t            self.consecutive_threshold = 20  # 连续检测阈值20帧\n   822\t            self.alert_cooldown.set(8.0)\n   823\t            self.current_mode = \&quot;保守模式\&quot;\n   824\t\n   825\t    def start_detection(self):\n   826\t        \&quot;\&quot;\&quot;开始检测\&quot;\&quot;\&quot;\n   827\t        if self.is_detecting:\n   828\t            return False\n   829\t\n   830\t        self.is_detecting = True\n   831\t        self.session_start_time = time.time()\n   832\t\n   833\t        # 重置AI检测统计数据\n   834\t        self.yawn_count = 0\n   835\t        self.blink_count = 0\n   836\t        self.total_predictions = 0\n   837\t        self.consecutive_yawns = 0\n   838\t        self.last_yawn_time = 0\n   839\t        self.last_detection_time = 0\n   840\t        self.no_detection_frames = 0\n   841\t\n   842\t        # 重置缓冲区\n   843\t        self.face_buffer.clear()\n   844\t        self.landmark_buffer.clear()\n   845\t\n   846\t        # 重置眨眼检测\n   847\t        self.eye_closed_frames = 0\n   848\t        self.eye_closed_start_time = None\n   849\t        self.last_blink_time = 0\n   850\t        self.recent_blinks = []\n   851\t        self.recent_yawns = []\n   852\t\n   853\t        # 重置压缩统计\n   854\t        self.compression_stats = {\n   855\t            'total_original_size': 0,\n   856\t            'total_compressed_size': 0,\n   857\t            'frames_processed': 0,\n   858\t            'avg_compression_ratio': 0\n   859\t        }\n   860\t\n   861\t        # 启动处理线程\n   862\t        self.processing_thread = threading.Thread(target=self._processing_loop, daemon=True)\n   863\t        self.processing_thread.start()\n   864\t\n   865\t        print(\&quot;✅ 检测已开始 - AI + FFmpeg优化模式\&quot;)\n   866\t        return True\n   867\t\n   868\t    def stop_detection(self):\n   869\t        \&quot;\&quot;\&quot;停止检测\&quot;\&quot;\&quot;\n   870\t        self.is_detecting = False\n   871\t        \n   872\t        # 清空队列\n   873\t        while not self.frame_queue.empty():\n   874\t            try:\n   875\t                self.frame_queue.get_nowait()\n   876\t            except Empty:\n   877\t                break\n   878\t                \n   879\t        print(\&quot;✅ 检测已停止\&quot;)\n   880\t\n   881\t    def add_frame(self, frame_data):\n   882\t        \&quot;\&quot;\&quot;添加帧到处理队列\&quot;\&quot;\&quot;\n   883\t        if not self.is_detecting:\n   884\t            return False\n   885\t            \n   886\t        try:\n   887\t            if self.frame_queue.full():\n   888\t                try:\n   889\t                    self.frame_queue.get_nowait()  # 移除最老的帧\n   890\t                except Empty:\n   891\t                    pass\n   892\t            \n   893\t            self.frame_queue.put_nowait(frame_data)\n   894\t            return True\n   895\t        except:\n   896\t            return False\n   897\t\n   898\t    def _processing_loop(self):\n   899\t        \&quot;\&quot;\&quot;处理循环 - 集成完整AI检测\&quot;\&quot;\&quot;\n   900\t        compression_methods = ['fast', 'webp', 'adaptive']\n   901\t        current_method_index = 0\n   902\t\n   903\t        while self.is_detecting:\n   904\t            try:\n   905\t                # 获取帧数据\n   906\t                frame_data = None\n   907\t                try:\n   908\t                    frame_data = self.frame_queue.get(timeout=0.5)\n   909\t                except Empty:\n   910\t                    continue\n   911\t\n   912\t                # 执行完整的AI检测流程\n   913\t                detection_result = self.process_frame_with_ai(frame_data)\n   914\t\n   915\t                if detection_result:\n   916\t                    # 更新最新结果\n   917\t                    self.latest_results.update(detection_result)\n   918\t                    self.latest_results['compression_stats'] = self.compression_stats.copy()\n   919\t\n   920\t                    # 动态调整压缩方法（基于性能）\n   921\t                    if self.compression_stats['frames_processed'] % 50 == 0:\n   922\t                        self._optimize_compression_method()\n   923\t\n   924\t            except Exception as e:\n   925\t                print(f\&quot;处理循环错误: {e}\&quot;)\n   926\t                time.sleep(0.1)\n   927\t\n   928\t    def process_frame_with_ai(self, frame_data):\n   929\t        \&quot;\&quot;\&quot;处理视频帧并进行AI检测（完整检测逻辑）\&quot;\&quot;\&quot;\n   930\t        try:\n   931\t            # 将base64数据转换为OpenCV图像\n   932\t            if frame_data.startswith('data:image'):\n   933\t                frame_data = frame_data.split(',')[1]\n   934\t\n   935\t            # 解码base64数据\n   936\t            img_data = base64.b64decode(frame_data)\n   937\t            nparr = np.frombuffer(img_data, np.uint8)\n   938\t            frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n   939\t\n   940\t            if frame is None:\n   941\t                return None\n   942\t\n   943\t            # 先进行FFmpeg压缩\n   944\t            compressed_result = self.video_processor.compress_frame_fast(frame_data)\n   945\t            if compressed_result:\n   946\t                self._update_compression_stats(compressed_result)\n   947\t\n   948\t            # 如果AI不可用，使用模拟检测\n   949\t            if not self.ai_available:\n   950\t                return self._simulate_detection(frame)\n   951\t\n   952\t            # 执行AI检测逻辑（保持原有逻辑）\n   953\t            face_img, landmarks_norm, face_rect = self._preprocess_frame(frame)\n   954\t            face_detected = face_img is not None\n   955\t\n   956\t            # 获取原始landmarks用于MAR/EAR计算\n   957\t            original_landmarks = None\n   958\t            if face_detected:\n   959\t                original_face_img, original_landmarks = extract_face_landmarks(frame, self.detector, self.predictor)\n   960\t                # 立即绘制人脸框和特征点\n   961\t                print(f\&quot; 绘制人脸框和特征点，人脸区域: {face_rect.left()}, {face_rect.top()}, {face_rect.width()}, {face_rect.height()}\&quot;)\n   962\t                frame = self._draw_face_landmarks(frame, face_rect, landmarks_norm)\n   963\t                print(f\&quot;✅ 人脸框和特征点绘制完成\&quot;)\n   964\t\n   965\t            yawn_prob = 0.0\n   966\t            prediction = 0\n   967\t\n   968\t            if face_detected:\n   969\t                self.face_buffer.append(face_img)\n   970\t                self.landmark_buffer.append(landmarks_norm)\n   971\t\n   972\t                # 如果缓冲区满了，进行预测（保持原有逻辑）\n   973\t                if len(self.face_buffer) &gt;= SEQUENCE_LENGTH:\n   974\t                    yawn_prob, model_prediction = self._predict_yawn()\n   975\t                    self.total_predictions += 1\n   976\t\n   977\t                    # 计算当前帧的嘴部长宽比和眼部长宽比（使用原始landmarks）\n   978\t                    current_mar = self._calculate_mouth_aspect_ratio(original_landmarks)\n   979\t                    current_ear = self._calculate_eye_aspect_ratio(original_landmarks)\n   980\t\n   981\t                    # 检测眨眼（保持原有逻辑）\n   982\t                    blink_detected = self._detect_blink(current_ear)\n   983\t\n   984\t                    # 新的检测逻辑：模型预测 + MAR阈值的组合判断（保持原有逻辑）\n   985\t                    model_says_yawn = yawn_prob &gt; self.yawn_threshold.get()\n   986\t                    mar_says_yawn = current_mar &gt; self.mar_threshold.get()\n   987\t\n   988\t                    # 最终判断：两个条件都满足才认为是打哈欠（保持原有逻辑）\n   989\t                    final_prediction = 1 if (model_says_yawn and mar_says_yawn) else 0\n   990\t\n   991\t                    # 更新连续检测计数 - 使用平滑衰减机制（保持原有逻辑）\n   992\t                    current_time = time.time()\n   993\t                    if final_prediction == 1:\n   994\t                        # 检测到打哈欠：增加计数，更新最后检测时间\n   995\t                        self.consecutive_yawns += 1\n   996\t                        self.last_detection_time = current_time\n   997\t                        self.no_detection_frames = 0  # 重置未检测帧数\n   998\t                        print(f\&quot; 打哈欠检测: 模型={yawn_prob:.3f}({'✓' if model_says_yawn else '✗'}), MAR={current_mar:.3f}({'✓' if mar_says_yawn else '✗'}), 连续={self.consecutive_yawns}\&quot;)\n   999\t                    else:\n  1000\t                        # 未检测到打哈欠：使用平滑衰减\n  1001\t                        self.no_detection_frames += 1\n  1002\t\n  1003\t                        # 如果有之前的检测记录，则开始衰减\n  1004\t                        if self.consecutive_yawns &gt; 0:\n  1005\t                            # 计算衰减量：基于时间的衰减\n  1006\t                            if self.last_detection_time &gt; 0:\n  1007\t                                time_since_last = current_time - self.last_detection_time\n  1008\t                                # 每秒衰减decay_rate帧，但至少保持1秒不衰减\n  1009\t                                if time_since_last &gt; 1.0:  # 1秒后开始衰减\n  1010\t                                    decay_amount = int((time_since_last - 1.0) * self.decay_rate)\n  1011\t                                    self.consecutive_yawns = max(0, self.consecutive_yawns - decay_amount)\n  1012\t\n  1013\t                                    if self.consecutive_yawns == 0:\n  1014\t                                        print(f\&quot; 进度条衰减至零（未检测{self.no_detection_frames}帧，时间间隔{time_since_last:.1f}秒）\&quot;)\n  1015\t                                    else:\n  1016\t                                        print(f\&quot; 进度条衰减: {self.consecutive_yawns}（未检测{self.no_detection_frames}帧）\&quot;)\n  1017\t                            else:\n  1018\t                                # 如果没有时间记录，立即开始衰减\n  1019\t                                if self.no_detection_frames &gt; 30:  # 30帧后开始衰减（约1秒）\n  1020\t                                    self.consecutive_yawns = max(0, self.consecutive_yawns - 1)\n  1021\t                        else:\n  1022\t                            # 如果consecutive_yawns已经是0，保持为0\n  1023\t                            self.consecutive_yawns = 0\n  1024\t\n  1025\t                    # 检查是否触发警报（保持原有逻辑）\n  1026\t                    if (self.consecutive_yawns &gt;= self.consecutive_threshold and\n  1027\t                        (current_time - self.last_yawn_time) &gt; self.alert_cooldown.get()):\n  1028\t                        self.yawn_count += 1\n  1029\t                        self.last_yawn_time = current_time\n  1030\t                        self.recent_yawns.append(current_time)\n  1031\t                        print(f\&quot; 触发警报！连续{self.consecutive_yawns}帧检测到打哈欠\&quot;)\n  1032\t                        self._play_warning_sound()\n  1033\t\n  1034\t                    # 更新prediction变量用于GUI显示\n  1035\t                    prediction = final_prediction\n  1036\t\n  1037\t                # 人脸框和特征点已在前面绘制\n  1038\t            else:\n  1039\t                # 未检测到人脸时的衰减逻辑（保持原有逻辑）\n  1040\t                if self.consecutive_yawns &gt; 0:\n  1041\t                    current_time = time.time()\n  1042\t                    self.no_detection_frames += 1\n  1043\t\n  1044\t                    # 如果有之前的检测记录，则开始衰减\n  1045\t                    if self.last_detection_time &gt; 0:\n  1046\t                        time_since_last = current_time - self.last_detection_time\n  1047\t                        # 未检测到人脸时，衰减更快一些\n  1048\t                        if time_since_last &gt; 0.5:  # 0.5秒后开始衰减\n  1049\t                            decay_amount = int((time_since_last - 0.5) * self.decay_rate * 1.5)  # 衰减速度1.5倍\n  1050\t                            old_consecutive = self.consecutive_yawns\n  1051\t                            self.consecutive_yawns = max(0, self.consecutive_yawns - decay_amount)\n  1052\t\n  1053\t                            if old_consecutive != self.consecutive_yawns:\n  1054\t                                if self.consecutive_yawns == 0:\n  1055\t                                    print(f\&quot; 未检测到人脸，进度条衰减至零（未检测{self.no_detection_frames}帧）\&quot;)\n  1056\t                                else:\n  1057\t                                    print(f\&quot; 未检测到人脸，进度条衰减: {self.consecutive_yawns}\&quot;)\n  1058\t                    else:\n  1059\t                        # 如果没有时间记录，较快衰减\n  1060\t                        if self.no_detection_frames &gt; 15:  # 15帧后开始衰减（约0.5秒）\n  1061\t                            self.consecutive_yawns = max(0, self.consecutive_yawns - 1)\n  1062\t\n  1063\t            # 评估疲劳状态\n  1064\t            fatigue_status = self._evaluate_fatigue_status()\n  1065\t\n  1066\t            # 记录疲劳状态到数据库\n  1067\t            self._record_fatigue_status(fatigue_status)\n  1068\t\n  1069\t            # 更新最新结果用于Web显示\n  1070\t            return self._update_latest_results(frame, face_detected, yawn_prob, prediction, fatigue_status)\n  1071\t\n  1072\t        except Exception as e:\n  1073\t            print(f\&quot;❌ AI检测处理错误: {e}\&quot;)\n  1074\t            return None\n  1075\t\n  1076\t    def _update_latest_results(self, frame, face_detected, yawn_prob, prediction, fatigue_status):\n  1077\t        \&quot;\&quot;\&quot;更新最新的检测结果用于Web显示\&quot;\&quot;\&quot;\n  1078\t        # 注意：frame已经包含了人脸框和特征点（在_draw_face_landmarks中绘制）\n  1079\t        # 不再添加额外的文本信息，只保留人脸框和特征点\n  1080\t\n  1081\t        # 将frame转换为base64用于Web显示，使用更高质量\n  1082\t        _, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 90])\n  1083\t        frame_base64 = base64.b64encode(buffer).decode('utf-8')\n  1084\t\n  1085\t        # 计算会话时间\n  1086\t        session_time = \&quot;00:00\&quot;\n  1087\t        if self.session_start_time:\n  1088\t            elapsed = int(time.time() - self.session_start_time)\n  1089\t            minutes = elapsed // 60\n  1090\t            seconds = elapsed % 60\n  1091\t            session_time = f\&quot;{minutes:02d}:{seconds:02d}\&quot;\n  1092\t\n  1093\t        return {\n  1094\t            'frame': frame_base64,\n  1095\t            'face_detected': face_detected,\n  1096\t            'yawn_prob': round(yawn_prob, 3),\n  1097\t            'prediction': prediction,\n  1098\t            'mar': round(self._last_mar, 3),\n  1099\t            'ear': round(self._last_ear, 3),\n  1100\t            'fatigue_status': fatigue_status,\n  1101\t            'consecutive_yawns': self.consecutive_yawns,\n  1102\t            'session_time': session_time,\n  1103\t            'buffer_status': f\&quot;{min(SEQUENCE_LENGTH, len(self.face_buffer))}/{SEQUENCE_LENGTH}\&quot;,\n  1104\t            'total_predictions': self.total_predictions,\n  1105\t            'yawn_count': self.yawn_count,\n  1106\t            'blink_count': self.blink_count,\n  1107\t            'progress': min(100, int((self.consecutive_yawns / self.consecutive_threshold) * 100))\n  1108\t        }\n  1109\t\n  1110\t    def _simulate_detection(self, frame):\n  1111\t        \&quot;\&quot;\&quot;模拟AI检测（当AI模块不可用时）\&quot;\&quot;\&quot;\n  1112\t        import random\n  1113\t\n  1114\t        # 模拟检测结果\n  1115\t        face_detected = random.choice([True, False, True, True])  # 75%概率检测到人脸\n  1116\t        yawn_prob = random.uniform(0.0, 1.0)\n  1117\t        prediction = 1 if yawn_prob &gt; 0.7 else 0\n  1118\t\n  1119\t        # 更新统计\n  1120\t        self.total_predictions += 1\n  1121\t        if prediction == 1:\n  1122\t            self.yawn_count += 1\n  1123\t\n  1124\t        # 模拟眨眼检测\n  1125\t        if random.random() &lt; 0.1:  # 10%概率检测到眨眼\n  1126\t            self.blink_count += 1\n  1127\t\n  1128\t        # 模拟疲劳状态\n  1129\t        if self.yawn_count &gt;= 3:\n  1130\t            fatigue_status = \&quot;中度疲劳\&quot;\n  1131\t        elif self.yawn_count &gt;= 1:\n  1132\t            fatigue_status = \&quot;轻度疲劳\&quot;\n  1133\t        else:\n  1134\t            fatigue_status = \&quot;正常\&quot;\n  1135\t\n  1136\t        # 在frame上绘制模拟检测框和特征点\n  1137\t        if face_detected:\n  1138\t            h, w = frame.shape[:2]\n  1139\t            # 绘制人脸框\n  1140\t            face_x, face_y = w//4, h//4\n  1141\t            face_w, face_h = w//2, h//2\n  1142\t            cv2.rectangle(frame, (face_x, face_y), (face_x + face_w, face_y + face_h), (0, 255, 0), 3)\n  1143\t\n  1144\t            # 绘制模拟特征点\n  1145\t            # 眼部区域\n  1146\t            eye_y = face_y + face_h//3\n  1147\t            left_eye_x = face_x + face_w//4\n  1148\t            right_eye_x = face_x + 3*face_w//4\n  1149\t\n  1150\t            # 左眼\n  1151\t            for i in range(6):\n  1152\t                angle = i * 60 * np.pi / 180\n  1153\t                x = int(left_eye_x + 15 * np.cos(angle))\n  1154\t                y = int(eye_y + 8 * np.sin(angle))\n  1155\t                cv2.circle(frame, (x, y), 2, (255, 0, 0), -1)\n  1156\t\n  1157\t            # 右眼\n  1158\t            for i in range(6):\n  1159\t                angle = i * 60 * np.pi / 180\n  1160\t                x = int(right_eye_x + 15 * np.cos(angle))\n  1161\t                y = int(eye_y + 8 * np.sin(angle))\n  1162\t                cv2.circle(frame, (x, y), 2, (255, 0, 0), -1)\n  1163\t\n  1164\t            # 嘴部区域\n  1165\t            mouth_y = face_y + 2*face_h//3\n  1166\t            mouth_x = face_x + face_w//2\n  1167\t\n  1168\t            # 嘴部轮廓\n  1169\t            for i in range(8):\n  1170\t                angle = i * 45 * np.pi / 180\n  1171\t                x = int(mouth_x + 20 * np.cos(angle))\n  1172\t                y = int(mouth_y + 10 * np.sin(angle))\n  1173\t                cv2.circle(frame, (x, y), 2, (0, 0, 255), -1)\n  1174\t\n  1175\t            # 鼻子\n  1176\t            nose_y = face_y + face_h//2\n  1177\t            nose_x = face_x + face_w//2\n  1178\t            cv2.circle(frame, (nose_x, nose_y), 3, (255, 0, 255), -1)\n  1179\t            cv2.circle(frame, (nose_x-5, nose_y+5), 2, (255, 0, 255), -1)\n  1180\t            cv2.circle(frame, (nose_x+5, nose_y+5), 2, (255, 0, 255), -1)\n  1181\t\n  1182\t        # 记录疲劳状态到数据库\n  1183\t        self._record_fatigue_status(fatigue_status)\n  1184\t\n  1185\t        return self._update_latest_results(frame, face_detected, yawn_prob, prediction, fatigue_status)\n  1186\t\n  1187\t    def _optimize_compression_method(self):\n  1188\t        \&quot;\&quot;\&quot;根据性能动态优化压缩方法\&quot;\&quot;\&quot;\n  1189\t        try:\n  1190\t            avg_ratio = self.compression_stats['avg_compression_ratio']\n  1191\t\n  1192\t            # 如果压缩比不够好，切换到更激进的压缩\n  1193\t            if avg_ratio &lt; 70:  # 压缩比低于70%\n  1194\t                new_crf = min(35, int(self.video_processor.compression_settings['crf']) + 2)\n  1195\t                self.video_processor.compression_settings['crf'] = str(new_crf)\n  1196\t                print(f\&quot; 自动优化: 提高压缩比，CRF调整为 {new_crf}\&quot;)\n  1197\t\n  1198\t            # 如果压缩比太高可能影响质量，适当降低\n  1199\t            elif avg_ratio &gt; 90:  # 压缩比高于90%\n  1200\t                new_crf = max(18, int(self.video_processor.compression_settings['crf']) - 1)\n  1201\t                self.video_processor.compression_settings['crf'] = str(new_crf)\n  1202\t                print(f\&quot; 自动优化: 保持质量，CRF调整为 {new_crf}\&quot;)\n  1203\t\n  1204\t        except Exception as e:\n  1205\t            print(f\&quot;优化压缩方法失败: {e}\&quot;)\n  1206\t\n  1207\t    def _update_compression_stats(self, result):\n  1208\t        \&quot;\&quot;\&quot;更新压缩统计\&quot;\&quot;\&quot;\n  1209\t        self.compression_stats['total_original_size'] += result['original_size']\n  1210\t        self.compression_stats['total_compressed_size'] += result['compressed_size']\n  1211\t        self.compression_stats['frames_processed'] += 1\n  1212\t        \n  1213\t        if self.compression_stats['total_original_size'] &gt; 0:\n  1214\t            self.compression_stats['avg_compression_ratio'] = (\n  1215\t                (1 - self.compression_stats['total_compressed_size'] / \n  1216\t                 self.compression_stats['total_original_size']) * 100\n  1217\t            )\n  1218\t\n  1219\t\n  1220\t\n  1221\t    def get_latest_results(self):\n  1222\t        \&quot;\&quot;\&quot;获取最新结果\&quot;\&quot;\&quot;\n  1223\t        return self.latest_results.copy()\n  1224\t\n  1225\t    def update_compression_settings(self, settings):\n  1226\t        \&quot;\&quot;\&quot;更新压缩设置\&quot;\&quot;\&quot;\n  1227\t        self.video_processor.update_settings(settings)\n  1228\t\n  1229\t# 全局检测系统实例\n  1230\tdetection_system = OptimizedFatigueDetectionSystem(MODEL_PATH)\n  1231\t\n  1232\t# 在应用启动时初始化数据库\n  1233\t@app.on_event(\&quot;startup\&quot;)\n  1234\tasync def startup_event():\n  1235\t    \&quot;\&quot;\&quot;应用启动事件\&quot;\&quot;\&quot;\n  1236\t    if AI_MODULES_AVAILABLE:\n  1237\t        try:\n  1238\t            init_database()\n  1239\t            print(\&quot;✅ 数据库初始化成功\&quot;)\n  1240\t        except Exception as e:\n  1241\t            print(f\&quot;⚠️  数据库初始化失败: {e}\&quot;)\n  1242\t    else:\n  1243\t        print(\&quot;⚠️  跳过数据库初始化（AI模块不可用）\&quot;)\n  1244\t\n  1245\t# 路由定义\n  1246\t@app.get(\&quot;/\&quot;, response_class=HTMLResponse)\n  1247\tasync def login_page(request: Request):\n  1248\t    return templates.TemplateResponse(\&quot;login.html\&quot;, {\&quot;request\&quot;: request})\n  1249\t\n  1250\t@app.post(\&quot;/login\&quot;)\n  1251\tasync def login(username: str = Form(...), password: str = Form(...)):\n  1252\t    \&quot;\&quot;\&quot;用户登录 - 使用数据库认证\&quot;\&quot;\&quot;\n  1253\t    try:\n  1254\t        if AI_MODULES_AVAILABLE:\n  1255\t            # 使用数据库认证\n  1256\t            with get_db_connection() as conn:\n  1257\t                cursor = conn.cursor()\n  1258\t                cursor.execute(\&quot;SELECT username, password FROM users WHERE username = %s\&quot;, (username,))\n  1259\t                user = cursor.fetchone()\n  1260\t\n  1261\t            if user and user[1] == password:\n  1262\t                detection_system.current_user = {\n  1263\t                    'username': user[0],\n  1264\t                    'full_name': user[0]\n  1265\t                }\n  1266\t                return RedirectResponse(url=\&quot;/dashboard\&quot;, status_code=302)\n  1267\t            else:\n  1268\t                return JSONResponse({\&quot;success\&quot;: False, \&quot;message\&quot;: \&quot;用户名或密码错误\&quot;})\n  1269\t        else:\n  1270\t            # 如果数据库不可用，使用简化认证\n  1271\t            simple_users = {\&quot;test\&quot;: \&quot;123456\&quot;, \&quot;admin\&quot;: \&quot;admin\&quot;}\n  1272\t            if username in simple_users and simple_users[username] == password:\n  1273\t                detection_system.current_user = {\n  1274\t                    'username': username,\n  1275\t                    'full_name': username\n  1276\t                }\n  1277\t                return RedirectResponse(url=\&quot;/dashboard\&quot;, status_code=302)\n  1278\t            else:\n  1279\t                return JSONResponse({\&quot;success\&quot;: False, \&quot;message\&quot;: \&quot;用户名或密码错误\&quot;})\n  1280\t\n  1281\t    except Exception as e:\n  1282\t        print(f\&quot;❌ 登录失败: {e}\&quot;)\n  1283\t        return JSONResponse({\&quot;success\&quot;: False, \&quot;message\&quot;: f\&quot;登录失败: {e}\&quot;})\n  1284\t\n  1285\t@app.get(\&quot;/register\&quot;, response_class=HTMLResponse)\n  1286\tasync def register_page(request: Request):\n  1287\t    \&quot;\&quot;\&quot;注册页面\&quot;\&quot;\&quot;\n  1288\t    return templates.TemplateResponse(\&quot;register.html\&quot;, {\&quot;request\&quot;: request})\n  1289\t\n  1290\t@app.post(\&quot;/register\&quot;)\n  1291\tasync def register(username: str = Form(...), password: str = Form(...), confirm_password: str = Form(...)):\n  1292\t    \&quot;\&quot;\&quot;用户注册 - 使用数据库存储\&quot;\&quot;\&quot;\n  1293\t    if len(password) &lt; 6:\n  1294\t        return JSONResponse({\&quot;success\&quot;: False, \&quot;message\&quot;: \&quot;密码长度至少6位\&quot;})\n  1295\t\n  1296\t    if password != confirm_password:\n  1297\t        return JSONResponse({\&quot;success\&quot;: False, \&quot;message\&quot;: \&quot;两次输入的密码不一致\&quot;})\n  1298\t\n  1299\t    try:\n  1300\t        if AI_MODULES_AVAILABLE:\n  1301\t            # 使用数据库存储\n  1302\t            with get_db_connection() as conn:\n  1303\t                cursor = conn.cursor()\n  1304\t                cursor.execute(\&quot;INSERT INTO users (username, password) VALUES (%s, %s)\&quot;, (username, password))\n  1305\t                conn.commit()\n  1306\t\n  1307\t            return JSONResponse({\&quot;success\&quot;: True, \&quot;message\&quot;: f\&quot;用户 {username} 注册成功！\&quot;})\n  1308\t        else:\n  1309\t            # 如果数据库不可用，返回提示\n  1310\t            return JSONResponse({\&quot;success\&quot;: False, \&quot;message\&quot;: \&quot;数据库不可用，无法注册新用户\&quot;})\n  1311\t\n  1312\t    except Exception as e:\n  1313\t        if \&quot;Duplicate entry\&quot; in str(e):\n  1314\t            return JSONResponse({\&quot;success\&quot;: False, \&quot;message\&quot;: \&quot;用户名已存在，请选择其他用户名\&quot;})\n  1315\t        else:\n  1316\t            return JSONResponse({\&quot;success\&quot;: False, \&quot;message\&quot;: f\&quot;注册失败: {e}\&quot;})\n  1317\t\n  1318\t@app.get(\&quot;/dashboard\&quot;, response_class=HTMLResponse)\n  1319\tasync def dashboard(request: Request):\n  1320\t    if not detection_system.current_user:\n  1321\t        return RedirectResponse(url=\&quot;/\&quot;, status_code=302)\n  1322\t    return templates.TemplateResponse(\&quot;dashboard_backend_ffmpeg.html\&quot;, {\n  1323\t        \&quot;request\&quot;: request,\n  1324\t        \&quot;user\&quot;: detection_system.current_user\n  1325\t    })\n  1326\t\n  1327\t@app.post(\&quot;/api/start_detection\&quot;)\n  1328\tasync def start_detection():\n  1329\t    if detection_system.start_detection():\n  1330\t        return JSONResponse({\&quot;success\&quot;: True, \&quot;message\&quot;: \&quot;检测已开始\&quot;})\n  1331\t    else:\n  1332\t        return JSONResponse({\&quot;success\&quot;: False, \&quot;message\&quot;: \&quot;启动检测失败\&quot;})\n  1333\t\n  1334\t@app.post(\&quot;/api/stop_detection\&quot;)\n  1335\tasync def stop_detection():\n  1336\t    detection_system.stop_detection()\n  1337\t    return JSONResponse({\&quot;success\&quot;: True, \&quot;message\&quot;: \&quot;检测已停止\&quot;})\n  1338\t\n  1339\t@app.post(\&quot;/api/update_compression\&quot;)\n  1340\tasync def update_compression(\n  1341\t    preset: str = Form(...),\n  1342\t    crf: str = Form(...),\n  1343\t    scale: str = Form(...),\n  1344\t    fps: str = Form(...)\n  1345\t):\n  1346\t    \&quot;\&quot;\&quot;更新压缩设置\&quot;\&quot;\&quot;\n  1347\t    settings = {\n  1348\t        'preset': preset,\n  1349\t        'crf': crf,\n  1350\t        'scale': scale,\n  1351\t        'fps': fps\n  1352\t    }\n  1353\t    detection_system.update_compression_settings(settings)\n  1354\t    return JSONResponse({\&quot;success\&quot;: True, \&quot;message\&quot;: \&quot;压缩设置已更新\&quot;})\n  1355\t\n  1356\t@app.get(\&quot;/api/compression_stats\&quot;)\n  1357\tasync def get_compression_stats():\n  1358\t    \&quot;\&quot;\&quot;获取压缩统计\&quot;\&quot;\&quot;\n  1359\t    return JSONResponse(detection_system.compression_stats)\n  1360\t\n  1361\t@app.post(\&quot;/api/reset_stats\&quot;)\n  1362\tasync def reset_stats():\n  1363\t    \&quot;\&quot;\&quot;重置统计数据\&quot;\&quot;\&quot;\n  1364\t    detection_system.compression_stats = {\n  1365\t        'total_original_size': 0,\n  1366\t        'total_compressed_size': 0,\n  1367\t        'frames_processed': 0,\n  1368\t        'avg_compression_ratio': 0\n  1369\t    }\n  1370\t    return JSONResponse({\&quot;success\&quot;: True, \&quot;message\&quot;: \&quot;统计数据已重置\&quot;})\n  1371\t\n  1372\t@app.get(\&quot;/api/system_info\&quot;)\n  1373\tasync def get_system_info():\n  1374\t    \&quot;\&quot;\&quot;获取系统信息\&quot;\&quot;\&quot;\n  1375\t    import psutil\n  1376\t    import platform\n  1377\t\n  1378\t    try:\n  1379\t        cpu_percent = psutil.cpu_percent(interval=1)\n  1380\t        memory = psutil.virtual_memory()\n  1381\t        disk = psutil.disk_usage(&lt;response clipped&gt;&lt;NOTE&gt;To save on context only part of this file has been shown to you.&lt;/NOTE&gt;\nTotal lines in file: 1595\n&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_01SeJKDiP8RGjhQGDFQY4KEd&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;,&quot;folderRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-08T06:49:34.608Z&quot;,&quot;response_text&quot;:&quot;现在让我查看其他相关的Python文件来了解完整的依赖关系：\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;4b01cf87-4efb-4295-a588-cb705977b8ce&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_01P1PkvfXtuvEdR77tNHUgYe&quot;,&quot;tool_name&quot;:&quot;view&quot;,&quot;input_json&quot;:&quot;{\&quot;path\&quot;: \&quot;model.py\&quot;, \&quot;type\&quot;: \&quot;file\&quot;}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;Here's the result of running `cat -n` on config.py:\n     1\t\&quot;\&quot;\&quot;\n     2\t配置文件 - 打哈欠检测项目\n     3\t\&quot;\&quot;\&quot;\n     4\timport os\n     5\t\n     6\t# 数据路径配置\n     7\tDATASET_ROOT = \&quot;dataset\&quot;\n     8\tDASH_PATH = os.path.join(DATASET_ROOT, \&quot;Dash\&quot;)\n     9\tMIRROR_PATH = os.path.join(DATASET_ROOT, \&quot;Mirror\&quot;)\n    10\t\n    11\t# dlib模型路径\n    12\tDLIB_PREDICTOR_PATH = \&quot;output/models/shape_predictor_68_face_landmarks.dat\&quot;\n    13\t\n    14\t# 数据预处理参数\n    15\tFACE_SIZE = (128, 128)  # 人脸图像resize尺寸\n    16\tSEQUENCE_LENGTH = 30    # LSTM输入序列长度（帧数）\n    17\tOVERLAP_RATIO = 0.5     # 滑动窗口重叠比例\n    18\t\n    19\t# 模型参数\n    20\tCNN_CHANNELS = [32, 64, 128]  # CNN各层通道数\n    21\tLSTM_HIDDEN_SIZE = 256        # LSTM隐藏层大小\n    22\tLSTM_LAYERS = 2               # LSTM层数\n    23\tDROPOUT_RATE = 0.3            # Dropout比例\n    24\tNUM_CLASSES = 2               # 分类数：0-正常/说话，1-打哈欠\n    25\t\n    26\t# 训练参数\n    27\tBATCH_SIZE = 8  # 减小批次大小以节省内存\n    28\tLEARNING_RATE = 0.001\n    29\tNUM_EPOCHS = 100\n    30\tTRAIN_SPLIT = 0.8\n    31\tVAL_SPLIT = 0.1\n    32\tTEST_SPLIT = 0.1\n    33\t\n    34\t# 调试模式参数（已关闭，进行完整训练）\n    35\tDEBUG_MODE = False  # 设置为False进行完整训练\n    36\tDEBUG_MAX_VIDEOS = 3  # 调试模式下每个文件夹最多处理的视频数\n    37\tDEBUG_MAX_FRAMES = 600  # 调试模式下每个视频最多处理的帧数\n    38\tDEBUG_EPOCHS = 5  # 调试模式下的训练轮数\n    39\tDEBUG_BATCH_SIZE = 4  # 调试模式下的批次大小\n    40\t\n    41\t# 数据增强参数\n    42\tAUGMENTATION = {\n    43\t    'brightness_range': 0.2,\n    44\t    'contrast_range': 0.2,\n    45\t    'rotation_range': 10,\n    46\t    'horizontal_flip': True\n    47\t}\n    48\t\n    49\t# 输出路径\n    50\tOUTPUT_DIR = \&quot;output\&quot;\n    51\tMODEL_SAVE_PATH = os.path.join(OUTPUT_DIR, \&quot;models\&quot;)\n    52\tLOG_PATH = os.path.join(OUTPUT_DIR, \&quot;logs\&quot;)\n    53\tPROCESSED_DATA_PATH = os.path.join(OUTPUT_DIR, \&quot;processed_data\&quot;)\n    54\t\n    55\t# 创建输出目录\n    56\tos.makedirs(OUTPUT_DIR, exist_ok=True)\n    57\tos.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n    58\tos.makedirs(LOG_PATH, exist_ok=True)\n    59\tos.makedirs(PROCESSED_DATA_PATH, exist_ok=True)\n    60\t\nTotal lines in file: 60\n&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_01Rgv7RZZiWHvGPAvuL9C4wG&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;,&quot;folderRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-08T06:49:42.137Z&quot;,&quot;response_text&quot;:&quot;&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;dcee9ada-952e-46df-a578-1570559279d5&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;&lt;antml5:function_calls&gt;\n&lt;invoke&gt;\n&lt;self_reflection&gt;\n&lt;parameter name=\&quot;reflection\&quot;&gt;\n现在我已经了解了项目的结构和依赐关系。让我分析一下主要的依赖：\n\n1. 从 `fatigue_web_self_ffmpeg.py` 中可以看到主要的依赖包括：\n   - cv2 (OpenCV)\n   - torch (PyTorch)\n   - numpy\n   - dlib\n   - pygame\n   - fastapi\n   - uvicorn\n   - jinja2\n   - psutil\n   - subprocess (内置)\n   - base64 (内置)\n   - json (内置)\n   - tempfile (内置)\n   - os (内置)\n   - sys (内置)\n   - asyncio (内置)\n   - time (内置)\n   - threading (内置)\n   - queue (内置)\n   - datetime (内置)\n   - typing (内置)\n   - collections (内置)\n\n2. 从 `model.py` 中可以看到：\n   - torch\n   - torch.nn\n   - torch.nn.functional\n\n3. 从 `config.py` 中可以看到：\n   - os (内置)\n\n4. 还需要查看其他文件来确认完整的依赖关系。\n\n让我继续查看其他文件。\n&lt;/parameter&gt;\n&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;Here's the result of running `cat -n` on model.py:\n     1\t\&quot;\&quot;\&quot;\n     2\tCNN+LSTM模型架构 - 打哈欠检测\n     3\t\&quot;\&quot;\&quot;\n     4\timport torch\n     5\timport torch.nn as nn\n     6\timport torch.nn.functional as F\n     7\tfrom typing import Tuple\n     8\t\n     9\tfrom config import *\n    10\t\n    11\tclass ResidualBlock(nn.Module):\n    12\t    \&quot;\&quot;\&quot;残差块\&quot;\&quot;\&quot;\n    13\t    def __init__(self, in_channels, out_channels, stride=1):\n    14\t        super(ResidualBlock, self).__init__()\n    15\t\n    16\t        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n    17\t        self.bn1 = nn.BatchNorm2d(out_channels)\n    18\t        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n    19\t        self.bn2 = nn.BatchNorm2d(out_channels)\n    20\t\n    21\t        # 跳跃连接\n    22\t        self.shortcut = nn.Sequential()\n    23\t        if stride != 1 or in_channels != out_channels:\n    24\t            self.shortcut = nn.Sequential(\n    25\t                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n    26\t                nn.BatchNorm2d(out_channels)\n    27\t            )\n    28\t\n    29\t    def forward(self, x):\n    30\t        residual = self.shortcut(x)\n    31\t\n    32\t        out = F.relu(self.bn1(self.conv1(x)))\n    33\t        out = self.bn2(self.conv2(out))\n    34\t\n    35\t        out += residual  # 残差连接\n    36\t        out = F.relu(out)\n    37\t\n    38\t        return out\n    39\t\n    40\tclass CNNFeatureExtractor(nn.Module):\n    41\t    \&quot;\&quot;\&quot;基于残差网络的CNN特征提取器\&quot;\&quot;\&quot;\n    42\t\n    43\t    def __init__(self, input_channels: int = 3):\n    44\t        super(CNNFeatureExtractor, self).__init__()\n    45\t\n    46\t        # 初始卷积层\n    47\t        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=7, stride=2, padding=3, bias=False)\n    48\t        self.bn1 = nn.BatchNorm2d(32)\n    49\t        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n    50\t\n    51\t        # 残差层\n    52\t        self.layer1 = self._make_layer(32, 32, 2, stride=1)\n    53\t        self.layer2 = self._make_layer(32, 64, 2, stride=2)\n    54\t        self.layer3 = self._make_layer(64, 128, 2, stride=2)\n    55\t\n    56\t        # 全局平均池化\n    57\t        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n    58\t\n    59\t        # 特征映射层\n    60\t        self.fc = nn.Sequential(\n    61\t            nn.Linear(128, 256),\n    62\t            nn.ReLU(inplace=True),\n    63\t            nn.Dropout(DROPOUT_RATE),\n    64\t            nn.Linear(256, 512),\n    65\t            nn.ReLU(inplace=True),\n    66\t            nn.Dropout(DROPOUT_RATE)\n    67\t        )\n    68\t\n    69\t        # 权重初始化\n    70\t        self._initialize_weights()\n    71\t\n    72\t    def _make_layer(self, in_channels, out_channels, blocks, stride):\n    73\t        layers = []\n    74\t        layers.append(ResidualBlock(in_channels, out_channels, stride))\n    75\t        for _ in range(1, blocks):\n    76\t            layers.append(ResidualBlock(out_channels, out_channels))\n    77\t        return nn.Sequential(*layers)\n    78\t\n    79\t    def _initialize_weights(self):\n    80\t        for m in self.modules():\n    81\t            if isinstance(m, nn.Conv2d):\n    82\t                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n    83\t            elif isinstance(m, nn.BatchNorm2d):\n    84\t                nn.init.constant_(m.weight, 1)\n    85\t                nn.init.constant_(m.bias, 0)\n    86\t            elif isinstance(m, nn.Linear):\n    87\t                nn.init.normal_(m.weight, 0, 0.01)\n    88\t                nn.init.constant_(m.bias, 0)\n    89\t\n    90\t    def forward(self, x):\n    91\t        # x shape: (batch_size, channels, height, width)\n    92\t        x = F.relu(self.bn1(self.conv1(x)))\n    93\t        x = self.maxpool(x)\n    94\t\n    95\t        x = self.layer1(x)\n    96\t        x = self.layer2(x)\n    97\t        x = self.layer3(x)\n    98\t\n    99\t        x = self.avgpool(x)\n   100\t        x = x.reshape(x.size(0), -1)\n   101\t\n   102\t        x = self.fc(x)\n   103\t\n   104\t        return x\n   105\t\n   106\tclass LandmarkProcessor(nn.Module):\n   107\t    \&quot;\&quot;\&quot;改进的特征点处理器 - 使用残差连接\&quot;\&quot;\&quot;\n   108\t\n   109\t    def __init__(self, landmark_dim: int = 68 * 2):\n   110\t        super(LandmarkProcessor, self).__init__()\n   111\t\n   112\t        # 输入投影\n   113\t        self.input_proj = nn.Linear(landmark_dim, 256)\n   114\t\n   115\t        # 残差块\n   116\t        self.res_block1 = nn.Sequential(\n   117\t            nn.Linear(256, 256),\n   118\t            nn.BatchNorm1d(256),\n   119\t            nn.ReLU(inplace=True),\n   120\t            nn.Dropout(DROPOUT_RATE),\n   121\t            nn.Linear(256, 256),\n   122\t            nn.BatchNorm1d(256)\n   123\t        )\n   124\t\n   125\t        self.res_block2 = nn.Sequential(\n   126\t            nn.Linear(256, 128),\n   127\t            nn.BatchNorm1d(128),\n   128\t            nn.ReLU(inplace=True),\n   129\t            nn.Dropout(DROPOUT_RATE),\n   130\t            nn.Linear(128, 128),\n   131\t            nn.BatchNorm1d(128)\n   132\t        )\n   133\t\n   134\t        # 输出层\n   135\t        self.output_proj = nn.Linear(128, 64)\n   136\t\n   137\t        # 跳跃连接的投影层\n   138\t        self.skip_proj1 = nn.Identity()  # 256 -&gt; 256\n   139\t        self.skip_proj2 = nn.Linear(256, 128)  # 256 -&gt; 128\n   140\t\n   141\t        # 权重初始化\n   142\t        self._initialize_weights()\n   143\t\n   144\t    def _initialize_weights(self):\n   145\t        for m in self.modules():\n   146\t            if isinstance(m, nn.Linear):\n   147\t                nn.init.xavier_normal_(m.weight)\n   148\t                if m.bias is not None:\n   149\t                    nn.init.constant_(m.bias, 0)\n   150\t\n   151\t    def forward(self, x):\n   152\t        # x shape: (batch_size, 68*2)\n   153\t        x = F.relu(self.input_proj(x))  # (batch_size, 256)\n   154\t\n   155\t        # 第一个残差块\n   156\t        residual1 = self.skip_proj1(x)\n   157\t        out1 = self.res_block1(x)\n   158\t        x = F.relu(out1 + residual1)\n   159\t\n   160\t        # 第二个残差块\n   161\t        residual2 = self.skip_proj2(x)\n   162\t        out2 = self.res_block2(x)\n   163\t        x = F.relu(out2 + residual2)\n   164\t\n   165\t        # 输出投影\n   166\t        x = F.relu(self.output_proj(x))\n   167\t\n   168\t        return x\n   169\t\n   170\tclass YawnDetectionModel(nn.Module):\n   171\t    \&quot;\&quot;\&quot;改进的打哈欠检测模型 - 残差CNN+LSTM架构\&quot;\&quot;\&quot;\n   172\t\n   173\t    def __init__(self):\n   174\t        super(YawnDetectionModel, self).__init__()\n   175\t\n   176\t        # CNN特征提取器\n   177\t        self.cnn_extractor = CNNFeatureExtractor()\n   178\t\n   179\t        # 特征点处理器\n   180\t        self.landmark_processor = LandmarkProcessor()\n   181\t\n   182\t        # 改进的特征融合层\n   183\t        self.feature_fusion = nn.Sequential(\n   184\t            nn.Linear(512 + 64, 512),\n   185\t            nn.BatchNorm1d(512),\n   186\t            nn.ReLU(inplace=True),\n   187\t            nn.Dropout(DROPOUT_RATE),\n   188\t            nn.Linear(512, 256),\n   189\t            nn.BatchNorm1d(256),\n   190\t            nn.ReLU(inplace=True)\n   191\t        )\n   192\t\n   193\t        # 多层LSTM\n   194\t        self.lstm1 = nn.LSTM(\n   195\t            input_size=256,\n   196\t            hidden_size=LSTM_HIDDEN_SIZE,\n   197\t            num_layers=1,\n   198\t            batch_first=True,\n   199\t            bidirectional=True\n   200\t        )\n   201\t\n   202\t        self.lstm2 = nn.LSTM(\n   203\t            input_size=LSTM_HIDDEN_SIZE * 2,\n   204\t            hidden_size=LSTM_HIDDEN_SIZE,\n   205\t            num_layers=1,\n   206\t            batch_first=True,\n   207\t            bidirectional=True\n   208\t        )\n   209\t\n   210\t        # LSTM层归一化\n   211\t        self.ln1 = nn.LayerNorm(LSTM_HIDDEN_SIZE * 2)\n   212\t        self.ln2 = nn.LayerNorm(LSTM_HIDDEN_SIZE * 2)\n   213\t\n   214\t        # 改进的注意力机制\n   215\t        self.attention = nn.MultiheadAttention(\n   216\t            embed_dim=LSTM_HIDDEN_SIZE * 2,\n   217\t            num_heads=8,\n   218\t            dropout=DROPOUT_RATE,\n   219\t            batch_first=True\n   220\t        )\n   221\t\n   222\t        # 残差分类器\n   223\t        self.classifier = nn.Sequential(\n   224\t            nn.Linear(LSTM_HIDDEN_SIZE * 2, 256),\n   225\t            nn.BatchNorm1d(256),\n   226\t            nn.ReLU(inplace=True),\n   227\t            nn.Dropout(DROPOUT_RATE),\n   228\t            nn.Linear(256, 128),\n   229\t            nn.BatchNorm1d(128),\n   230\t            nn.ReLU(inplace=True),\n   231\t            nn.Dropout(DROPOUT_RATE),\n   232\t            nn.Linear(128, NUM_CLASSES)\n   233\t        )\n   234\t\n   235\t        # 权重初始化\n   236\t        self._initialize_weights()\n   237\t\n   238\t    def _initialize_weights(self):\n   239\t        for m in self.modules():\n   240\t            if isinstance(m, nn.Linear):\n   241\t                nn.init.xavier_normal_(m.weight)\n   242\t                if m.bias is not None:\n   243\t                    nn.init.constant_(m.bias, 0)\n   244\t            elif isinstance(m, nn.LSTM):\n   245\t                for name, param in m.named_parameters():\n   246\t                    if 'weight' in name:\n   247\t                        nn.init.orthogonal_(param)\n   248\t                    elif 'bias' in name:\n   249\t                        nn.init.constant_(param, 0)\n   250\t        \n   251\t    def forward(self, faces, landmarks):\n   252\t        \&quot;\&quot;\&quot;\n   253\t        改进的前向传播\n   254\t\n   255\t        Args:\n   256\t            faces: 人脸图像序列 (batch_size, sequence_length, channels, height, width)\n   257\t            landmarks: 特征点序列 (batch_size, sequence_length, 68*2)\n   258\t\n   259\t        Returns:\n   260\t            output: 分类结果 (batch_size, num_classes)\n   261\t        \&quot;\&quot;\&quot;\n   262\t        batch_size, seq_len = faces.size(0), faces.size(1)\n   263\t\n   264\t        # 重塑输入以便CNN处理\n   265\t        faces = faces.reshape(batch_size * seq_len, *faces.shape[2:])\n   266\t        landmarks = landmarks.reshape(batch_size * seq_len, -1)\n   267\t\n   268\t        # CNN特征提取\n   269\t        cnn_features = self.cnn_extractor(faces)  # (batch_size * seq_len, 512)\n   270\t\n   271\t        # 特征点处理\n   272\t        landmark_features = self.landmark_processor(landmarks)  # (batch_size * seq_len, 64)\n   273\t\n   274\t        # 特征融合\n   275\t        fused_features = torch.cat([cnn_features, landmark_features], dim=1)\n   276\t        fused_features = self.feature_fusion(fused_features)\n   277\t\n   278\t        # 重塑为序列格式\n   279\t        fused_features = fused_features.reshape(batch_size, seq_len, -1)\n   280\t\n   281\t        # 多层LSTM处理\n   282\t        lstm_out1, _ = self.lstm1(fused_features)\n   283\t        lstm_out1 = self.ln1(lstm_out1)  # 层归一化\n   284\t\n   285\t        # 残差连接（如果维度匹配）\n   286\t        if lstm_out1.size(-1) == fused_features.size(-1):\n   287\t            lstm_out1 = lstm_out1 + fused_features\n   288\t\n   289\t        lstm_out2, _ = self.lstm2(lstm_out1)\n   290\t        lstm_out2 = self.ln2(lstm_out2)  # 层归一化\n   291\t\n   292\t        # 残差连接\n   293\t        lstm_out2 = lstm_out2 + lstm_out1\n   294\t\n   295\t        # 注意力机制\n   296\t        attn_out, attn_weights = self.attention(lstm_out2, lstm_out2, lstm_out2)\n   297\t\n   298\t        # 多种池化策略结合\n   299\t        # 1. 全局平均池化\n   300\t        avg_pooled = torch.mean(attn_out, dim=1)\n   301\t        # 2. 全局最大池化\n   302\t        max_pooled, _ = torch.max(attn_out, dim=1)\n   303\t        # 3. 最后时间步\n   304\t        last_step = attn_out[:, -1, :]\n   305\t\n   306\t        # 组合不同的池化结果\n   307\t        pooled_features = (avg_pooled + max_pooled + last_step) / 3\n   308\t\n   309\t        # 分类\n   310\t        output = self.classifier(pooled_features)\n   311\t\n   312\t        return output\n   313\t    \n   314\t    def get_attention_weights(self, faces, landmarks):\n   315\t        \&quot;\&quot;\&quot;获取注意力权重用于可视化\&quot;\&quot;\&quot;\n   316\t        with torch.no_grad():\n   317\t            batch_size, seq_len = faces.size(0), faces.size(1)\n   318\t            \n   319\t            # 重塑输入\n   320\t            faces = faces.reshape(batch_size * seq_len, *faces.shape[2:])\n   321\t            landmarks = landmarks.reshape(batch_size * seq_len, -1)\n   322\t            \n   323\t            # 特征提取和融合\n   324\t            cnn_features = self.cnn_extractor(faces)\n   325\t            landmark_features = self.landmark_processor(landmarks)\n   326\t            fused_features = torch.cat([cnn_features, landmark_features], dim=1)\n   327\t            fused_features = F.relu(self.feature_fusion(fused_features))\n   328\t            fused_features = fused_features.reshape(batch_size, seq_len, -1)\n   329\t            \n   330\t            # LSTM处理\n   331\t            lstm_out, _ = self.lstm(fused_features)\n   332\t            \n   333\t            # 注意力权重\n   334\t            _, attn_weights = self.attention(lstm_out, lstm_out, lstm_out)\n   335\t            \n   336\t            return attn_weights\n   337\t\n   338\tclass ImprovedFocalLoss(nn.Module):\n   339\t    \&quot;\&quot;\&quot;改进的Focal Loss - 处理类别不平衡和标签平滑\&quot;\&quot;\&quot;\n   340\t\n   341\t    def __init__(self, alpha=None, gamma=2, label_smoothing=0.1, reduction='mean'):\n   342\t        super(ImprovedFocalLoss, self).__init__()\n   343\t        self.alpha = alpha\n   344\t        self.gamma = gamma\n   345\t        self.label_smoothing = label_smoothing\n   346\t        self.reduction = reduction\n   347\t\n   348\t    def forward(self, inputs, targets):\n   349\t        # 标签平滑\n   350\t        num_classes = inputs.size(-1)\n   351\t        if self.label_smoothing &gt; 0:\n   352\t            smooth_targets = torch.zeros_like(inputs)\n   353\t            smooth_targets.fill_(self.label_smoothing / (num_classes - 1))\n   354\t            smooth_targets.scatter_(1, targets.unsqueeze(1), 1.0 - self.label_smoothing)\n   355\t\n   356\t            # 使用KL散度计算平滑损失\n   357\t            log_probs = F.log_softmax(inputs, dim=-1)\n   358\t            loss = -torch.sum(smooth_targets * log_probs, dim=-1)\n   359\t        else:\n   360\t            loss = F.cross_entropy(inputs, targets, reduction='none')\n   361\t\n   362\t        # Focal权重\n   363\t        probs = F.softmax(inputs, dim=-1)\n   364\t        pt = probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n   365\t        focal_weight = (1 - pt) ** self.gamma\n   366\t\n   367\t        # 类别权重\n   368\t        if self.alpha is not None:\n   369\t            if isinstance(self.alpha, (float, int)):\n   370\t                alpha_t = self.alpha\n   371\t            else:\n   372\t                alpha_t = self.alpha[targets]\n   373\t            focal_weight = alpha_t * focal_weight\n   374\t\n   375\t        focal_loss = focal_weight * loss\n   376\t\n   377\t        if self.reduction == 'mean':\n   378\t            return focal_loss.mean()\n   379\t        elif self.reduction == 'sum':\n   380\t            return focal_loss.sum()\n   381\t        else:\n   382\t            return focal_loss\n   383\t\n   384\tclass LabelSmoothingCrossEntropy(nn.Module):\n   385\t    \&quot;\&quot;\&quot;标签平滑交叉熵损失\&quot;\&quot;\&quot;\n   386\t\n   387\t    def __init__(self, smoothing=0.1):\n   388\t        super(LabelSmoothingCrossEntropy, self).__init__()\n   389\t        self.smoothing = smoothing\n   390\t\n   391\t    def forward(self, inputs, targets):\n   392\t        log_probs = F.log_softmax(inputs, dim=-1)\n   393\t        num_classes = inputs.size(-1)\n   394\t\n   395\t        # 创建平滑标签\n   396\t        smooth_targets = torch.zeros_like(log_probs)\n   397\t        smooth_targets.fill_(self.smoothing / (num_classes - 1))\n   398\t        smooth_targets.scatter_(1, targets.unsqueeze(1), 1.0 - self.smoothing)\n   399\t\n   400\t        loss = -torch.sum(smooth_targets * log_probs, dim=-1)\n   401\t        return loss.mean()\n   402\t\n   403\tdef create_model():\n   404\t    \&quot;\&quot;\&quot;创建模型实例\&quot;\&quot;\&quot;\n   405\t    model = YawnDetectionModel()\n   406\t    return model\n   407\t\n   408\tdef count_parameters(model):\n   409\t    \&quot;\&quot;\&quot;统计模型参数数量\&quot;\&quot;\&quot;\n   410\t    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n   411\t\n   412\tif __name__ == \&quot;__main__\&quot;:\n   413\t    # 测试模型\n   414\t    model = create_model()\n   415\t    \n   416\t    # 创建测试输入\n   417\t    batch_size = 2\n   418\t    faces = torch.randn(batch_size, SEQUENCE_LENGTH, 3, FACE_SIZE[0], FACE_SIZE[1])\n   419\t    landmarks = torch.randn(batch_size, SEQUENCE_LENGTH, 68 * 2)\n   420\t    \n   421\t    # 前向传播\n   422\t    output = model(faces, landmarks)\n   423\t    \n   424\t    print(f\&quot;模型参数数量: {count_parameters(model):,}\&quot;)\n   425\t    print(f\&quot;输入形状 - 人脸: {faces.shape}, 特征点: {landmarks.shape}\&quot;)\n   426\t    print(f\&quot;输出形状: {output.shape}\&quot;)\n   427\t    print(f\&quot;输出: {output}\&quot;)\n   428\t\nTotal lines in file: 428\n&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_01P1PkvfXtuvEdR77tNHUgYe&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;,&quot;folderRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-08T06:50:21.542Z&quot;,&quot;response_text&quot;:&quot;&lt;antml5:function_calls&gt;\n&lt;invoke&gt;\n&lt;self_reflection&gt;\n&lt;parameter name=\&quot;reflection\&quot;&gt;\n现在我已经了解了项目的结构和依赐关系。让我分析一下主要的依赖：\n\n1. 从 `fatigue_web_self_ffmpeg.py` 中可以看到主要的依赖包括：\n   - cv2 (OpenCV)\n   - torch (PyTorch)\n   - numpy\n   - dlib\n   - pygame\n   - fastapi\n   - uvicorn\n   - jinja2\n   - psutil\n   - subprocess (内置)\n   - base64 (内置)\n   - json (内置)\n   - tempfile (内置)\n   - os (内置)\n   - sys (内置)\n   - asyncio (内置)\n   - time (内置)\n   - threading (内置)\n   - queue (内置)\n   - datetime (内置)\n   - typing (内置)\n   - collections (内置)\n\n2. 从 `model.py` 中可以看到：\n   - torch\n   - torch.nn\n   - torch.nn.functional\n\n3. 从 `config.py` 中可以看到：\n   - os (内置)\n\n4. 还需要查看其他文件来确认完整的依赖关系。\n\n让我继续查看其他文件。\n&lt;/parameter&gt;\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;8d6330df-d97c-4e88-b0df-7ff94ca41ca1&quot;,&quot;request_message&quot;:&quot;现在我要把这个项目部署到docker，使用的python版本为3.10，启动fatigue_web_ffmpeg.py，在帮我书写一个requirements.txt文档来下载所依赖的python包，同时使用linux/arm64架构来打包，需要书写Dockerfile文件和requirements.txt&quot;,&quot;rich_text_json_repr&quot;:{&quot;type&quot;:&quot;doc&quot;,&quot;content&quot;:[{&quot;type&quot;:&quot;paragraph&quot;,&quot;content&quot;:[{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;现在我要把这个项目部署到docker，使用的python版本为3.10，启动fatigue_web_ffmpeg.py，在帮我书写一个requirements.txt文档来下载所依赖的python包，同时使用linux/arm64架构来打包，需要书写Dockerfile文件和requirements.txt&quot;}]}]},&quot;mentioned_items&quot;:[],&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_01FBQGEEnArs6ktauSHS5T93&quot;,&quot;tool_name&quot;:&quot;view&quot;,&quot;input_json&quot;:&quot;{\&quot;path\&quot;: \&quot;utils.py\&quot;, \&quot;type\&quot;: \&quot;file\&quot;}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;我来帮您为这个疲劳检测项目创建Docker部署所需的文件。首先让我查看其他相关文件来了解完整的依赖关系。\n&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:0,&quot;type&quot;:0,&quot;text_node&quot;:{&quot;content&quot;:&quot;现在我要把这个项目部署到docker，使用的python版本为3.10，启动fatigue_web_ffmpeg.py，在帮我书写一个requirements.txt文档来下载所依赖的python包，同时使用linux/arm64架构来打包，需要书写Dockerfile文件和requirements.txt&quot;}},{&quot;id&quot;:1,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;,&quot;folderRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-08T06:51:44.268Z&quot;,&quot;response_text&quot;:&quot;我来帮您为这个疲劳检测项目创建Docker部署所需的文件。首先让我查看其他相关文件来了解完整的依赖关系。\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;8c71ee27-eb8e-4c3b-95f6-13c29e8bd63c&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_01FB94xbBVVsHVo3e7eWbNV3&quot;,&quot;tool_name&quot;:&quot;view&quot;,&quot;input_json&quot;:&quot;{\&quot;path\&quot;: \&quot;database_config.py\&quot;, \&quot;type\&quot;: \&quot;file\&quot;}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;Here's the result of running `cat -n` on utils.py:\n     1\t\&quot;\&quot;\&quot;\n     2\t工具函数\n     3\t\&quot;\&quot;\&quot;\n     4\timport cv2\n     5\timport numpy as np\n     6\timport dlib\n     7\tfrom typing import List, Tuple, Optional\n     8\timport logging\n     9\t\n    10\tdef setup_logging():\n    11\t    \&quot;\&quot;\&quot;设置日志\&quot;\&quot;\&quot;\n    12\t    logging.basicConfig(\n    13\t        level=logging.INFO,\n    14\t        format='%(asctime)s - %(levelname)s - %(message)s',\n    15\t        handlers=[\n    16\t            logging.FileHandler('training.log'),\n    17\t            logging.StreamHandler()\n    18\t        ]\n    19\t    )\n    20\t    return logging.getLogger(__name__)\n    21\t\n    22\tdef parse_annotation_line(line: str) -&gt; Tuple[str, int, List[Tuple[int, int]]]:\n    23\t    \&quot;\&quot;\&quot;\n    24\t    解析标注文件的一行\n    25\t    \n    26\t    Args:\n    27\t        line: 标注行，格式：filename label intervals\n    28\t        \n    29\t    Returns:\n    30\t        filename: 视频文件名\n    31\t        label: 类别标签\n    32\t        intervals: 打哈欠区间列表 [(start, end), ...]\n    33\t    \&quot;\&quot;\&quot;\n    34\t    parts = line.strip().split()\n    35\t    if len(parts) &lt; 3:\n    36\t        return None, None, None\n    37\t        \n    38\t    filename = parts[0]\n    39\t    label = int(parts[1])\n    40\t    interval_str = parts[2]\n    41\t    \n    42\t    intervals = []\n    43\t    if interval_str != \&quot;-1,-1\&quot;:\n    44\t        # 解析多个区间：774-965,1623-1806,2387-2631\n    45\t        for interval in interval_str.split(','):\n    46\t            if '-' in interval:\n    47\t                start, end = map(int, interval.split('-'))\n    48\t                intervals.append((start, end))\n    49\t    \n    50\t    return filename, label, intervals\n    51\t\n    52\tdef extract_face_landmarks(image: np.ndarray, detector, predictor) -&gt; Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n    53\t    \&quot;\&quot;\&quot;\n    54\t    提取人脸区域和68个特征点\n    55\t\n    56\t    Args:\n    57\t        image: 输入图像\n    58\t        detector: dlib人脸检测器\n    59\t        predictor: dlib特征点预测器\n    60\t\n    61\t    Returns:\n    62\t        face_image: 人脸区域图像\n    63\t        landmarks: 68个特征点坐标 (68, 2)\n    64\t    \&quot;\&quot;\&quot;\n    65\t    try:\n    66\t        # 检查输入图像\n    67\t        if image is None or image.size == 0:\n    68\t            return None, None\n    69\t\n    70\t        # 转换为灰度图\n    71\t        if len(image.shape) == 3:\n    72\t            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    73\t        else:\n    74\t            gray = image\n    75\t\n    76\t        # 人脸检测\n    77\t        faces = detector(gray)\n    78\t\n    79\t        if len(faces) == 0:\n    80\t            return None, None\n    81\t\n    82\t        # 取第一个检测到的人脸\n    83\t        face = faces[0]\n    84\t\n    85\t        # 提取人脸区域坐标\n    86\t        x, y, w, h = face.left(), face.top(), face.width(), face.height()\n    87\t\n    88\t        # 边界检查\n    89\t        img_h, img_w = image.shape[:2]\n    90\t        x = max(0, x)\n    91\t        y = max(0, y)\n    92\t        x2 = min(img_w, x + w)\n    93\t        y2 = min(img_h, y + h)\n    94\t\n    95\t        # 检查区域是否有效\n    96\t        if x2 &lt;= x or y2 &lt;= y:\n    97\t            return None, None\n    98\t\n    99\t        # 提取人脸区域\n   100\t        face_image = image[y:y2, x:x2]\n   101\t\n   102\t        # 检查提取的人脸图像\n   103\t        if face_image.size == 0 or face_image.shape[0] == 0 or face_image.shape[1] == 0:\n   104\t            return None, None\n   105\t\n   106\t        # 提取特征点\n   107\t        landmarks = predictor(gray, face)\n   108\t        landmarks_array = np.array([[p.x - x, p.y - y] for p in landmarks.parts()])\n   109\t\n   110\t        return face_image, landmarks_array\n   111\t\n   112\t    except Exception as e:\n   113\t        # 任何异常都返回None\n   114\t        return None, None\n   115\t\n   116\tdef normalize_landmarks(landmarks: np.ndarray, face_size: Tuple[int, int]) -&gt; np.ndarray:\n   117\t    \&quot;\&quot;\&quot;\n   118\t    归一化特征点坐标\n   119\t    \n   120\t    Args:\n   121\t        landmarks: 原始特征点坐标\n   122\t        face_size: 人脸图像尺寸\n   123\t        \n   124\t    Returns:\n   125\t        normalized_landmarks: 归一化后的特征点坐标\n   126\t    \&quot;\&quot;\&quot;\n   127\t    if landmarks is None:\n   128\t        return None\n   129\t    \n   130\t    # 归一化到[0, 1]范围\n   131\t    landmarks_norm = landmarks.copy().astype(np.float32)\n   132\t    landmarks_norm[:, 0] /= face_size[0]\n   133\t    landmarks_norm[:, 1] /= face_size[1]\n   134\t    \n   135\t    return landmarks_norm\n   136\t\n   137\tdef augment_image(image: np.ndarray, landmarks: np.ndarray, augmentation_params: dict) -&gt; Tuple[np.ndarray, np.ndarray]:\n   138\t    \&quot;\&quot;\&quot;\n   139\t    数据增强\n   140\t    \n   141\t    Args:\n   142\t        image: 输入图像\n   143\t        landmarks: 特征点\n   144\t        augmentation_params: 增强参数\n   145\t        \n   146\t    Returns:\n   147\t        augmented_image: 增强后的图像\n   148\t        augmented_landmarks: 增强后的特征点\n   149\t    \&quot;\&quot;\&quot;\n   150\t    aug_image = image.copy()\n   151\t    aug_landmarks = landmarks.copy() if landmarks is not None else None\n   152\t    \n   153\t    # 亮度调整\n   154\t    if 'brightness_range' in augmentation_params:\n   155\t        brightness = np.random.uniform(-augmentation_params['brightness_range'], \n   156\t                                     augmentation_params['brightness_range'])\n   157\t        aug_image = cv2.convertScaleAbs(aug_image, alpha=1, beta=brightness * 255)\n   158\t    \n   159\t    # 对比度调整\n   160\t    if 'contrast_range' in augmentation_params:\n   161\t        contrast = np.random.uniform(1 - augmentation_params['contrast_range'],\n   162\t                                   1 + augmentation_params['contrast_range'])\n   163\t        aug_image = cv2.convertScaleAbs(aug_image, alpha=contrast, beta=0)\n   164\t    \n   165\t    # 水平翻转\n   166\t    if augmentation_params.get('horizontal_flip', False) and np.random.random() &gt; 0.5:\n   167\t        aug_image = cv2.flip(aug_image, 1)\n   168\t        if aug_landmarks is not None:\n   169\t            aug_landmarks[:, 0] = 1.0 - aug_landmarks[:, 0]  # 假设landmarks已归一化\n   170\t    \n   171\t    return aug_image, aug_landmarks\n   172\t\n   173\tdef create_sliding_windows(total_frames: int, sequence_length: int, overlap_ratio: float) -&gt; List[Tuple[int, int]]:\n   174\t    \&quot;\&quot;\&quot;\n   175\t    创建滑动窗口\n   176\t    \n   177\t    Args:\n   178\t        total_frames: 总帧数\n   179\t        sequence_length: 序列长度\n   180\t        overlap_ratio: 重叠比例\n   181\t        \n   182\t    Returns:\n   183\t        windows: 窗口列表 [(start, end), ...]\n   184\t    \&quot;\&quot;\&quot;\n   185\t    if total_frames &lt; sequence_length:\n   186\t        return [(0, total_frames)]\n   187\t    \n   188\t    step = int(sequence_length * (1 - overlap_ratio))\n   189\t    windows = []\n   190\t    \n   191\t    start = 0\n   192\t    while start + sequence_length &lt;= total_frames:\n   193\t        windows.append((start, start + sequence_length))\n   194\t        start += step\n   195\t    \n   196\t    # 确保最后一个窗口包含到最后一帧\n   197\t    if windows[-1][1] &lt; total_frames:\n   198\t        windows.append((total_frames - sequence_length, total_frames))\n   199\t    \n   200\t    return windows\n   201\t\n   202\tdef interpolate_missing_landmarks(landmarks_sequence: List[Optional[np.ndarray]]) -&gt; List[np.ndarray]:\n   203\t    \&quot;\&quot;\&quot;\n   204\t    插值缺失的特征点\n   205\t    \n   206\t    Args:\n   207\t        landmarks_sequence: 特征点序列，可能包含None\n   208\t        \n   209\t    Returns:\n   210\t        interpolated_sequence: 插值后的特征点序列\n   211\t    \&quot;\&quot;\&quot;\n   212\t    # 找到有效的特征点\n   213\t    valid_indices = [i for i, lm in enumerate(landmarks_sequence) if lm is not None]\n   214\t    \n   215\t    if len(valid_indices) == 0:\n   216\t        return [np.zeros((68, 2)) for _ in landmarks_sequence]\n   217\t    \n   218\t    interpolated = []\n   219\t    for i, landmarks in enumerate(landmarks_sequence):\n   220\t        if landmarks is not None:\n   221\t            interpolated.append(landmarks)\n   222\t        else:\n   223\t            # 线性插值\n   224\t            if len(valid_indices) == 1:\n   225\t                interpolated.append(landmarks_sequence[valid_indices[0]])\n   226\t            else:\n   227\t                # 找到最近的两个有效点进行插值\n   228\t                left_idx = max([idx for idx in valid_indices if idx &lt; i], default=valid_indices[0])\n   229\t                right_idx = min([idx for idx in valid_indices if idx &gt; i], default=valid_indices[-1])\n   230\t                \n   231\t                if left_idx == right_idx:\n   232\t                    interpolated.append(landmarks_sequence[left_idx])\n   233\t                else:\n   234\t                    # 线性插值\n   235\t                    alpha = (i - left_idx) / (right_idx - left_idx)\n   236\t                    left_lm = landmarks_sequence[left_idx]\n   237\t                    right_lm = landmarks_sequence[right_idx]\n   238\t                    interp_lm = (1 - alpha) * left_lm + alpha * right_lm\n   239\t                    interpolated.append(interp_lm)\n   240\t    \n   241\t    return interpolated\n   242\t\nTotal lines in file: 242\n&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_01FBQGEEnArs6ktauSHS5T93&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;,&quot;folderRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-08T06:52:00.256Z&quot;,&quot;response_text&quot;:&quot;&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;5b06f876-3257-4311-a896-8a74bd0bd606&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_01WMUzKd9bR9QCVK9DYsCzoj&quot;,&quot;tool_name&quot;:&quot;save-file&quot;,&quot;input_json&quot;:&quot;{\&quot;instructions_reminder\&quot;: \&quot;LIMIT THE FILE CONTENT TO AT MOST 300 LINES. IF MORE CONTENT NEEDS TO BE ADDED USE THE str-replace-editor TOOL TO EDIT THE FILE AFTER IT HAS BEEN CREATED.\&quot;, \&quot;path\&quot;: \&quot;requirements.txt\&quot;, \&quot;file_content\&quot;: \&quot;# Web\\u6846\\u67b6\\u548c\\u670d\\u52a1\\u5668\\nfastapi==0.104.1\\nuvicorn[standard]==0.24.0\\njinja2==3.1.2\\npython-multipart==0.0.6\\n\\n# AI/ML\\u6838\\u5fc3\\u5e93\\ntorch==2.1.0\\ntorchvision==0.16.0\\nnumpy==1.24.3\\n\\n# \\u8ba1\\u7b97\\u673a\\u89c6\\u89c9\\u548c\\u56fe\\u50cf\\u5904\\u7406\\nopencv-python==4.8.1.78\\nPillow==10.0.1\\n\\n# \\u4eba\\u8138\\u68c0\\u6d4b\\u548c\\u7279\\u5f81\\u70b9\\u63d0\\u53d6\\ndlib==19.24.2\\n\\n# \\u6570\\u636e\\u5e93\\u8fde\\u63a5\\npymysql==1.1.0\\n\\n# \\u97f3\\u9891\\u5904\\u7406\\npygame==2.5.2\\n\\n# \\u7cfb\\u7edf\\u76d1\\u63a7\\npsutil==5.9.6\\n\\n# \\u6570\\u636e\\u5904\\u7406\\u548c\\u5de5\\u5177\\nscipy==1.11.4\\nscikit-learn==1.3.2\\n\\n# \\u5f02\\u6b65\\u652f\\u6301\\naiofiles==23.2.1\\n\\n# \\u65e5\\u5fd7\\u548c\\u8c03\\u8bd5\\ncolorlog==6.8.0\\n\\n# \\u7c7b\\u578b\\u68c0\\u67e5\\u652f\\u6301\\ntyping-extensions==4.8.0\\n\\n# \\u6570\\u5b66\\u8ba1\\u7b97\\u4f18\\u5316\\nnumba==0.58.1\\n\\n# \\u56fe\\u50cf\\u7f16\\u89e3\\u7801\\u652f\\u6301\\nimageio==2.31.6\\n\\n# \\u7f51\\u7edc\\u8bf7\\u6c42\\uff08\\u5982\\u679c\\u9700\\u8981\\uff09\\nrequests==2.31.0\\n\\n# \\u914d\\u7f6e\\u6587\\u4ef6\\u5904\\u7406\\npyyaml==6.0.1\\n\\n# \\u8fdb\\u5ea6\\u6761\\u663e\\u793a\\ntqdm==4.66.1\\n\\n# \\u65f6\\u95f4\\u5904\\u7406\\npython-dateutil==2.8.2\\n\\n# \\u5185\\u5b58\\u4f18\\u5316\\nmemory-profiler==0.61.0\&quot;}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;现在我已经了解了项目的完整依赖关系。让我为您创建Docker部署所需的文件。\n\n## 详细计划\n\n基于对代码的分析，我需要创建以下文件：\n\n1. **requirements.txt** - 包含所有Python依赖包\n2. **Dockerfile** - 用于构建Docker镜像，支持linux/arm64架构\n3. 可能需要一个 **.dockerignore** 文件来优化构建\n\n### 主要依赖包分析：\n- **Web框架**: fastapi, uvicorn, jinja2\n- **AI/ML**: torch, torchvision, numpy, opencv-python\n- **人脸检测**: dlib\n- **数据库**: pymysql\n- **音频**: pygame\n- **系统监控**: psutil\n- **图像处理**: Pillow\n- **其他**: python-multipart (FastAPI表单处理)\n\n让我开始创建这些文件：\n&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;Here's the result of running `cat -n` on database_config.py:\n     1\t\&quot;\&quot;\&quot;\n     2\t数据库配置文件\n     3\t用于管理MySQL数据库连接配置\n     4\t\&quot;\&quot;\&quot;\n     5\t\n     6\timport pymysql\n     7\timport datetime\n     8\tfrom contextlib import contextmanager\n     9\t\n    10\t# MySQL数据库配置\n    11\tDB_CONFIG = {\n    12\t    'host': '101.245.79.154',\n    13\t    'port': 3306,\n    14\t    'user': 'root',\n    15\t    'password': '123456',\n    16\t    'database': 'pljc',\n    17\t    'charset': 'utf8mb4',\n    18\t    'autocommit': True\n    19\t}\n    20\t\n    21\t@contextmanager\n    22\tdef get_db_connection():\n    23\t    \&quot;\&quot;\&quot;\n    24\t    获取数据库连接的上下文管理器\n    25\t    自动处理连接的打开和关闭\n    26\t    \&quot;\&quot;\&quot;\n    27\t    connection = None\n    28\t    try:\n    29\t        connection = pymysql.connect(**DB_CONFIG)\n    30\t        yield connection\n    31\t    except Exception as e:\n    32\t        if connection:\n    33\t            connection.rollback()\n    34\t        raise e\n    35\t    finally:\n    36\t        if connection:\n    37\t            connection.close()\n    38\t\n    39\tdef init_database():\n    40\t    \&quot;\&quot;\&quot;\n    41\t    初始化数据库表结构\n    42\t    \&quot;\&quot;\&quot;\n    43\t    try:\n    44\t        with get_db_connection() as conn:\n    45\t            cursor = conn.cursor()\n    46\t            \n    47\t            # 创建用户表\n    48\t            cursor.execute('''\n    49\t                CREATE TABLE IF NOT EXISTS users (\n    50\t                    username VARCHAR(50) PRIMARY KEY,\n    51\t                    password VARCHAR(255) NOT NULL,\n    52\t                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n    53\t                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4\n    54\t            ''')\n    55\t            \n    56\t            # 创建疲劳记录表\n    57\t            cursor.execute('''\n    58\t                CREATE TABLE IF NOT EXISTS fatigue_records (\n    59\t                    id INT AUTO_INCREMENT PRIMARY KEY,\n    60\t                    username VARCHAR(50) NOT NULL,\n    61\t                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    62\t                    fatigue_level VARCHAR(20) NOT NULL,\n    63\t                    INDEX idx_username (username),\n    64\t                    INDEX idx_timestamp (timestamp),\n    65\t                    FOREIGN KEY (username) REFERENCES users(username) ON DELETE CASCADE\n    66\t                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4\n    67\t            ''')\n    68\t            \n    69\t            conn.commit()\n    70\t            print(\&quot;数据库初始化成功\&quot;)\n    71\t            \n    72\t    except Exception as e:\n    73\t        print(f\&quot;数据库初始化失败: {e}\&quot;)\n    74\t        raise e\n    75\t\n    76\tdef test_connection():\n    77\t    \&quot;\&quot;\&quot;\n    78\t    测试数据库连接\n    79\t    \&quot;\&quot;\&quot;\n    80\t    try:\n    81\t        with get_db_connection() as conn:\n    82\t            cursor = conn.cursor()\n    83\t            cursor.execute(\&quot;SELECT 1\&quot;)\n    84\t            result = cursor.fetchone()\n    85\t            if result:\n    86\t                print(\&quot;数据库连接测试成功\&quot;)\n    87\t                return True\n    88\t            else:\n    89\t                print(\&quot;数据库连接测试失败\&quot;)\n    90\t                return False\n    91\t    except Exception as e:\n    92\t        print(f\&quot;数据库连接测试失败: {e}\&quot;)\n    93\t        return False\n    94\t\n    95\tif __name__ == \&quot;__main__\&quot;:\n    96\t    # 测试数据库连接和初始化\n    97\t    print(\&quot;测试数据库连接...\&quot;)\n    98\t    if test_connection():\n    99\t        print(\&quot;初始化数据库表...\&quot;)\n   100\t        init_database()\n   101\t        print(\&quot;数据库配置完成\&quot;)\n   102\t    else:\n   103\t        print(\&quot;数据库连接失败，请检查配置\&quot;)\n   104\t\nTotal lines in file: 104\n&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_01FB94xbBVVsHVo3e7eWbNV3&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;,&quot;folderRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-08T06:52:05.121Z&quot;,&quot;response_text&quot;:&quot;现在我已经了解了项目的完整依赖关系。让我为您创建Docker部署所需的文件。\n\n## 详细计划\n\n基于对代码的分析，我需要创建以下文件：\n\n1. **requirements.txt** - 包含所有Python依赖包\n2. **Dockerfile** - 用于构建Docker镜像，支持linux/arm64架构\n3. 可能需要一个 **.dockerignore** 文件来优化构建\n\n### 主要依赖包分析：\n- **Web框架**: fastapi, uvicorn, jinja2\n- **AI/ML**: torch, torchvision, numpy, opencv-python\n- **人脸检测**: dlib\n- **数据库**: pymysql\n- **音频**: pygame\n- **系统监控**: psutil\n- **图像处理**: Pillow\n- **其他**: python-multipart (FastAPI表单处理)\n\n让我开始创建这些文件：\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;d9a1c9e0-3373-4a25-a6ff-2b42df19831d&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_01JTgwXoCi7QWcoWu4SaKAnf&quot;,&quot;tool_name&quot;:&quot;save-file&quot;,&quot;input_json&quot;:&quot;{\&quot;instructions_reminder\&quot;: \&quot;LIMIT THE FILE CONTENT TO AT MOST 300 LINES. IF MORE CONTENT NEEDS TO BE ADDED USE THE str-replace-editor TOOL TO EDIT THE FILE AFTER IT HAS BEEN CREATED.\&quot;, \&quot;path\&quot;: \&quot;Dockerfile\&quot;, \&quot;file_content\&quot;: \&quot;# \\u4f7f\\u7528Python 3.10\\u5b98\\u65b9\\u955c\\u50cf\\u4f5c\\u4e3a\\u57fa\\u7840\\u955c\\u50cf\\uff0c\\u652f\\u6301ARM64\\u67b6\\u6784\\nFROM --platform=linux/arm64 python:3.10-slim\\n\\n# \\u8bbe\\u7f6e\\u5de5\\u4f5c\\u76ee\\u5f55\\nWORKDIR /app\\n\\n# \\u8bbe\\u7f6e\\u73af\\u5883\\u53d8\\u91cf\\nENV PYTHONUNBUFFERED=1\\nENV PYTHONDONTWRITEBYTECODE=1\\nENV DEBIAN_FRONTEND=noninteractive\\n\\n# \\u5b89\\u88c5\\u7cfb\\u7edf\\u4f9d\\u8d56\\nRUN apt-get update &amp;&amp; apt-get install -y \\\\\\n    # \\u57fa\\u7840\\u6784\\u5efa\\u5de5\\u5177\\n    build-essential \\\\\\n    cmake \\\\\\n    pkg-config \\\\\\n    # OpenCV\\u4f9d\\u8d56\\n    libopencv-dev \\\\\\n    libgtk-3-dev \\\\\\n    libavcodec-dev \\\\\\n    libavformat-dev \\\\\\n    libswscale-dev \\\\\\n    libv4l-dev \\\\\\n    libxvidcore-dev \\\\\\n    libx264-dev \\\\\\n    libjpeg-dev \\\\\\n    libpng-dev \\\\\\n    libtiff-dev \\\\\\n    libatlas-base-dev \\\\\\n    # dlib\\u4f9d\\u8d56\\n    libboost-all-dev \\\\\\n    libopenblas-dev \\\\\\n    liblapack-dev \\\\\\n    # \\u97f3\\u9891\\u652f\\u6301\\n    libasound2-dev \\\\\\n    libpulse-dev \\\\\\n    libsdl2-dev \\\\\\n    libsdl2-mixer-2.0-0 \\\\\\n    # FFmpeg\\u652f\\u6301\\n    ffmpeg \\\\\\n    # \\u7f51\\u7edc\\u548cSSL\\n    libssl-dev \\\\\\n    libffi-dev \\\\\\n    # \\u6e05\\u7406\\u7f13\\u5b58\\n    &amp;&amp; apt-get clean \\\\\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\\n\\n# \\u5347\\u7ea7pip\\u5e76\\u5b89\\u88c5Python\\u4f9d\\u8d56\\nCOPY requirements.txt .\\nRUN pip install --no-cache-dir --upgrade pip setuptools wheel &amp;&amp; \\\\\\n    pip install --no-cache-dir -r requirements.txt\\n\\n# \\u521b\\u5efa\\u5fc5\\u8981\\u7684\\u76ee\\u5f55\\u7ed3\\u6784\\nRUN mkdir -p /app/output/models \\\\\\n    /app/output/logs \\\\\\n    /app/output/processed_data \\\\\\n    /app/static \\\\\\n    /app/templates\\n\\n# \\u590d\\u5236\\u9879\\u76ee\\u6587\\u4ef6\\nCOPY . .\\n\\n# \\u8bbe\\u7f6e\\u6743\\u9650\\nRUN chmod +x fatigue_web_ffmpeg.py\\n\\n# \\u521b\\u5efa\\u975eroot\\u7528\\u6237\\nRUN useradd -m -u 1000 appuser &amp;&amp; \\\\\\n    chown -R appuser:appuser /app\\nUSER appuser\\n\\n# \\u66b4\\u9732\\u7aef\\u53e3\\nEXPOSE 8000\\n\\n# \\u5065\\u5eb7\\u68c0\\u67e5\\nHEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \\\\\\n    CMD curl -f http://localhost:8000/ || exit 1\\n\\n# \\u542f\\u52a8\\u547d\\u4ee4\\nCMD [\\\&quot;python\\\&quot;, \\\&quot;-m\\\&quot;, \\\&quot;uvicorn\\\&quot;, \\\&quot;fatigue_web_ffmpeg:app\\\&quot;, \\\&quot;--host\\\&quot;, \\\&quot;0.0.0.0\\\&quot;, \\\&quot;--port\\\&quot;, \\\&quot;8000\\\&quot;, \\\&quot;--workers\\\&quot;, \\\&quot;1\\\&quot;]\&quot;}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;File saved.  Saved file {requirements.txt}&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_01WMUzKd9bR9QCVK9DYsCzoj&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;,&quot;folderRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-08T06:52:26.556Z&quot;,&quot;response_text&quot;:&quot;&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;69cc72a7-59d3-4616-8e99-2964e00fc620&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_015KTFw2S3vLd2UEh9hW3cBu&quot;,&quot;tool_name&quot;:&quot;save-file&quot;,&quot;input_json&quot;:&quot;{\&quot;instructions_reminder\&quot;: \&quot;LIMIT THE FILE CONTENT TO AT MOST 300 LINES. IF MORE CONTENT NEEDS TO BE ADDED USE THE str-replace-editor TOOL TO EDIT THE FILE AFTER IT HAS BEEN CREATED.\&quot;, \&quot;path\&quot;: \&quot;.dockerignore\&quot;, \&quot;file_content\&quot;: \&quot;# Python\\u7f13\\u5b58\\u6587\\u4ef6\\n__pycache__/\\n*.py[cod]\\n*$py.class\\n*.so\\n.Python\\nbuild/\\ndevelop-eggs/\\ndist/\\ndownloads/\\neggs/\\n.eggs/\\nlib/\\nlib64/\\nparts/\\nsdist/\\nvar/\\nwheels/\\n*.egg-info/\\n.installed.cfg\\n*.egg\\n\\n# \\u865a\\u62df\\u73af\\u5883\\nvenv/\\nenv/\\nENV/\\n\\n# IDE\\u6587\\u4ef6\\n.vscode/\\n.idea/\\n*.swp\\n*.swo\\n*~\\n\\n# \\u64cd\\u4f5c\\u7cfb\\u7edf\\u6587\\u4ef6\\n.DS_Store\\n.DS_Store?\\n._*\\n.Spotlight-V100\\n.Trashes\\nehthumbs.db\\nThumbs.db\\n\\n# Git\\u6587\\u4ef6\\n.git/\\n.gitignore\\n\\n# \\u65e5\\u5fd7\\u6587\\u4ef6\\n*.log\\nlogs/\\n\\n# \\u4e34\\u65f6\\u6587\\u4ef6\\ntmp/\\ntemp/\\n*.tmp\\n\\n# \\u6570\\u636e\\u96c6\\u6587\\u4ef6\\uff08\\u901a\\u5e38\\u5f88\\u5927\\uff0c\\u4e0d\\u5e94\\u5305\\u542b\\u5728\\u955c\\u50cf\\u4e2d\\uff09\\ndataset/\\ndata/\\n*.mp4\\n*.avi\\n*.mov\\n*.mkv\\n\\n# \\u6a21\\u578b\\u6587\\u4ef6\\uff08\\u5982\\u679c\\u5f88\\u5927\\u7684\\u8bdd\\uff09\\n# output/models/*.pth\\n# output/models/*.pt\\n\\n# \\u6d4b\\u8bd5\\u6587\\u4ef6\\ntest/\\ntests/\\n*_test.py\\ntest_*.py\\n\\n# \\u6587\\u6863\\nREADME.md\\ndocs/\\n*.md\\n\\n# Docker\\u76f8\\u5173\\nDockerfile*\\ndocker-compose*.yml\\n.dockerignore\\n\\n# \\u5176\\u4ed6\\u4e0d\\u9700\\u8981\\u7684\\u6587\\u4ef6\\n.env\\n.env.local\\n.env.*.local\\nnode_modules/\\nnpm-debug.log*\\nyarn-debug.log*\\nyarn-error.log*\&quot;}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;File saved.  Saved file {Dockerfile}&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_01JTgwXoCi7QWcoWu4SaKAnf&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;,&quot;folderRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-08T06:52:43.881Z&quot;,&quot;response_text&quot;:&quot;&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;b0c27318-6ea0-4997-b62c-16e9d29067cf&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_01Cm7TUjgrQk6YSgJwtUZFtm&quot;,&quot;tool_name&quot;:&quot;save-file&quot;,&quot;input_json&quot;:&quot;{\&quot;instructions_reminder\&quot;: \&quot;LIMIT THE FILE CONTENT TO AT MOST 300 LINES. IF MORE CONTENT NEEDS TO BE ADDED USE THE str-replace-editor TOOL TO EDIT THE FILE AFTER IT HAS BEEN CREATED.\&quot;, \&quot;path\&quot;: \&quot;docker-compose.yml\&quot;, \&quot;file_content\&quot;: \&quot;version: '3.8'\\n\\nservices:\\n  fatigue-detection:\\n    build:\\n      context: .\\n      dockerfile: Dockerfile\\n      platforms:\\n        - linux/arm64\\n    container_name: fatigue-detection-app\\n    ports:\\n      - \\\&quot;8000:8000\\\&quot;\\n    volumes:\\n      # \\u6302\\u8f7d\\u8f93\\u51fa\\u76ee\\u5f55\\u4ee5\\u6301\\u4e45\\u5316\\u6570\\u636e\\n      - ./output:/app/output\\n      # \\u6302\\u8f7d\\u9759\\u6001\\u6587\\u4ef6\\n      - ./static:/app/static\\n      # \\u6302\\u8f7d\\u6a21\\u677f\\u6587\\u4ef6\\n      - ./templates:/app/templates\\n    environment:\\n      - PYTHONUNBUFFERED=1\\n      - TZ=Asia/Shanghai\\n    restart: unless-stopped\\n    networks:\\n      - fatigue-network\\n    # \\u8d44\\u6e90\\u9650\\u5236\\n    deploy:\\n      resources:\\n        limits:\\n          memory: 2G\\n          cpus: '1.0'\\n        reservations:\\n          memory: 1G\\n          cpus: '0.5'\\n    # \\u5065\\u5eb7\\u68c0\\u67e5\\n    healthcheck:\\n      test: [\\\&quot;CMD\\\&quot;, \\\&quot;curl\\\&quot;, \\\&quot;-f\\\&quot;, \\\&quot;http://localhost:8000/\\\&quot;]\\n      interval: 30s\\n      timeout: 10s\\n      retries: 3\\n      start_period: 60s\\n\\nnetworks:\\n  fatigue-network:\\n    driver: bridge\\n\\nvolumes:\\n  fatigue-data:\\n    driver: local\&quot;}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;现在让我创建一个docker-compose.yml文件来简化部署：\n&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;File saved.  Saved file {.dockerignore}&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_015KTFw2S3vLd2UEh9hW3cBu&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;,&quot;folderRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-08T06:52:56.849Z&quot;,&quot;response_text&quot;:&quot;现在让我创建一个docker-compose.yml文件来简化部署：\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;c8f55c78-a91d-41b6-9a5e-2f89859e66ed&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_01ULcB34zoS7pR61wxUmjJpa&quot;,&quot;tool_name&quot;:&quot;save-file&quot;,&quot;input_json&quot;:&quot;{\&quot;instructions_reminder\&quot;: \&quot;LIMIT THE FILE CONTENT TO AT MOST 300 LINES. IF MORE CONTENT NEEDS TO BE ADDED USE THE str-replace-editor TOOL TO EDIT THE FILE AFTER IT HAS BEEN CREATED.\&quot;, \&quot;path\&quot;: \&quot;deploy.sh\&quot;, \&quot;file_content\&quot;: \&quot;#!/bin/bash\\n\\n# \\u75b2\\u52b3\\u68c0\\u6d4b\\u7cfb\\u7edfDocker\\u90e8\\u7f72\\u811a\\u672c\\n# \\u652f\\u6301linux/arm64\\u67b6\\u6784\\n\\nset -e\\n\\n# \\u989c\\u8272\\u5b9a\\u4e49\\nRED='\\\\033[0;31m'\\nGREEN='\\\\033[0;32m'\\nYELLOW='\\\\033[1;33m'\\nBLUE='\\\\033[0;34m'\\nNC='\\\\033[0m' # No Color\\n\\n# \\u6253\\u5370\\u5e26\\u989c\\u8272\\u7684\\u6d88\\u606f\\nprint_message() {\\n    echo -e \\\&quot;${GREEN}[INFO]${NC} $1\\\&quot;\\n}\\n\\nprint_warning() {\\n    echo -e \\\&quot;${YELLOW}[WARNING]${NC} $1\\\&quot;\\n}\\n\\nprint_error() {\\n    echo -e \\\&quot;${RED}[ERROR]${NC} $1\\\&quot;\\n}\\n\\nprint_step() {\\n    echo -e \\\&quot;${BLUE}[STEP]${NC} $1\\\&quot;\\n}\\n\\n# \\u68c0\\u67e5Docker\\u662f\\u5426\\u5b89\\u88c5\\ncheck_docker() {\\n    if ! command -v docker &amp;&gt; /dev/null; then\\n        print_error \\\&quot;Docker\\u672a\\u5b89\\u88c5\\uff0c\\u8bf7\\u5148\\u5b89\\u88c5Docker\\\&quot;\\n        exit 1\\n    fi\\n    \\n    if ! command -v docker-compose &amp;&gt; /dev/null; then\\n        print_error \\\&quot;Docker Compose\\u672a\\u5b89\\u88c5\\uff0c\\u8bf7\\u5148\\u5b89\\u88c5Docker Compose\\\&quot;\\n        exit 1\\n    fi\\n    \\n    print_message \\\&quot;Docker\\u73af\\u5883\\u68c0\\u67e5\\u901a\\u8fc7\\\&quot;\\n}\\n\\n# \\u68c0\\u67e5\\u5fc5\\u8981\\u6587\\u4ef6\\ncheck_files() {\\n    local required_files=(\\\&quot;fatigue_web_ffmpeg.py\\\&quot; \\\&quot;config.py\\\&quot; \\\&quot;model.py\\\&quot; \\\&quot;utils.py\\\&quot; \\\&quot;database_config.py\\\&quot; \\\&quot;requirements.txt\\\&quot; \\\&quot;Dockerfile\\\&quot;)\\n    \\n    for file in \\\&quot;${required_files[@]}\\\&quot;; do\\n        if [[ ! -f \\\&quot;$file\\\&quot; ]]; then\\n            print_error \\\&quot;\\u7f3a\\u5c11\\u5fc5\\u8981\\u6587\\u4ef6: $file\\\&quot;\\n            exit 1\\n        fi\\n    done\\n    \\n    print_message \\\&quot;\\u5fc5\\u8981\\u6587\\u4ef6\\u68c0\\u67e5\\u901a\\u8fc7\\\&quot;\\n}\\n\\n# \\u521b\\u5efa\\u5fc5\\u8981\\u76ee\\u5f55\\ncreate_directories() {\\n    print_step \\\&quot;\\u521b\\u5efa\\u5fc5\\u8981\\u76ee\\u5f55...\\\&quot;\\n    \\n    mkdir -p output/models\\n    mkdir -p output/logs\\n    mkdir -p output/processed_data\\n    mkdir -p static\\n    mkdir -p templates\\n    \\n    print_message \\\&quot;\\u76ee\\u5f55\\u521b\\u5efa\\u5b8c\\u6210\\\&quot;\\n}\\n\\n# \\u6784\\u5efaDocker\\u955c\\u50cf\\nbuild_image() {\\n    print_step \\\&quot;\\u6784\\u5efaDocker\\u955c\\u50cf (linux/arm64)...\\\&quot;\\n    \\n    # \\u8bbe\\u7f6e\\u6784\\u5efa\\u53c2\\u6570\\n    IMAGE_NAME=\\\&quot;fatigue-detection\\\&quot;\\n    IMAGE_TAG=\\\&quot;latest\\\&quot;\\n    \\n    # \\u6784\\u5efa\\u955c\\u50cf\\n    docker build \\\\\\n        --platform linux/arm64 \\\\\\n        -t \\\&quot;${IMAGE_NAME}:${IMAGE_TAG}\\\&quot; \\\\\\n        -f Dockerfile \\\\\\n        . || {\\n        print_error \\\&quot;Docker\\u955c\\u50cf\\u6784\\u5efa\\u5931\\u8d25\\\&quot;\\n        exit 1\\n    }\\n    \\n    print_message \\\&quot;Docker\\u955c\\u50cf\\u6784\\u5efa\\u6210\\u529f: ${IMAGE_NAME}:${IMAGE_TAG}\\\&quot;\\n}\\n\\n# \\u4f7f\\u7528docker-compose\\u90e8\\u7f72\\ndeploy_with_compose() {\\n    print_step \\\&quot;\\u4f7f\\u7528Docker Compose\\u90e8\\u7f72...\\\&quot;\\n    \\n    # \\u505c\\u6b62\\u73b0\\u6709\\u5bb9\\u5668\\n    docker-compose down 2&gt;/dev/null || true\\n    \\n    # \\u542f\\u52a8\\u670d\\u52a1\\n    docker-compose up -d --build\\n    \\n    print_message \\\&quot;\\u670d\\u52a1\\u90e8\\u7f72\\u5b8c\\u6210\\\&quot;\\n}\\n\\n# \\u68c0\\u67e5\\u670d\\u52a1\\u72b6\\u6001\\ncheck_service() {\\n    print_step \\\&quot;\\u68c0\\u67e5\\u670d\\u52a1\\u72b6\\u6001...\\\&quot;\\n    \\n    # \\u7b49\\u5f85\\u670d\\u52a1\\u542f\\u52a8\\n    sleep 10\\n    \\n    # \\u68c0\\u67e5\\u5bb9\\u5668\\u72b6\\u6001\\n    if docker-compose ps | grep -q \\\&quot;Up\\\&quot;; then\\n        print_message \\\&quot;\\u670d\\u52a1\\u8fd0\\u884c\\u6b63\\u5e38\\\&quot;\\n        print_message \\\&quot;\\u8bbf\\u95ee\\u5730\\u5740: http://localhost:8000\\\&quot;\\n    else\\n        print_error \\\&quot;\\u670d\\u52a1\\u542f\\u52a8\\u5931\\u8d25\\\&quot;\\n        print_warning \\\&quot;\\u67e5\\u770b\\u65e5\\u5fd7: docker-compose logs\\\&quot;\\n        exit 1\\n    fi\\n}\\n\\n# \\u663e\\u793a\\u4f7f\\u7528\\u8bf4\\u660e\\nshow_usage() {\\n    echo \\\&quot;\\u75b2\\u52b3\\u68c0\\u6d4b\\u7cfb\\u7edfDocker\\u90e8\\u7f72\\u811a\\u672c\\\&quot;\\n    echo \\\&quot;\\\&quot;\\n    echo \\\&quot;\\u7528\\u6cd5: $0 [\\u9009\\u9879]\\\&quot;\\n    echo \\\&quot;\\\&quot;\\n    echo \\\&quot;\\u9009\\u9879:\\\&quot;\\n    echo \\\&quot;  build     \\u4ec5\\u6784\\u5efaDocker\\u955c\\u50cf\\\&quot;\\n    echo \\\&quot;  deploy    \\u6784\\u5efa\\u5e76\\u90e8\\u7f72\\u670d\\u52a1\\\&quot;\\n    echo \\\&quot;  start     \\u542f\\u52a8\\u5df2\\u5b58\\u5728\\u7684\\u670d\\u52a1\\\&quot;\\n    echo \\\&quot;  stop      \\u505c\\u6b62\\u670d\\u52a1\\\&quot;\\n    echo \\\&quot;  restart   \\u91cd\\u542f\\u670d\\u52a1\\\&quot;\\n    echo \\\&quot;  logs      \\u67e5\\u770b\\u670d\\u52a1\\u65e5\\u5fd7\\\&quot;\\n    echo \\\&quot;  clean     \\u6e05\\u7406Docker\\u8d44\\u6e90\\\&quot;\\n    echo \\\&quot;  help      \\u663e\\u793a\\u6b64\\u5e2e\\u52a9\\u4fe1\\u606f\\\&quot;\\n    echo \\\&quot;\\\&quot;\\n    echo \\\&quot;\\u793a\\u4f8b:\\\&quot;\\n    echo \\\&quot;  $0 deploy    # \\u5b8c\\u6574\\u90e8\\u7f72\\\&quot;\\n    echo \\\&quot;  $0 logs      # \\u67e5\\u770b\\u65e5\\u5fd7\\\&quot;\\n    echo \\\&quot;  $0 stop      # \\u505c\\u6b62\\u670d\\u52a1\\\&quot;\\n}\\n\\n# \\u4e3b\\u51fd\\u6570\\nmain() {\\n    case \\\&quot;${1:-deploy}\\\&quot; in\\n        \\\&quot;build\\\&quot;)\\n            check_docker\\n            check_files\\n            create_directories\\n            build_image\\n            ;;\\n        \\\&quot;deploy\\\&quot;)\\n            check_docker\\n            check_files\\n            create_directories\\n            deploy_with_compose\\n            check_service\\n            ;;\\n        \\\&quot;start\\\&quot;)\\n            docker-compose up -d\\n            print_message \\\&quot;\\u670d\\u52a1\\u5df2\\u542f\\u52a8\\\&quot;\\n            ;;\\n        \\\&quot;stop\\\&quot;)\\n            docker-compose down\\n            print_message \\\&quot;\\u670d\\u52a1\\u5df2\\u505c\\u6b62\\\&quot;\\n            ;;\\n        \\\&quot;restart\\\&quot;)\\n            docker-compose restart\\n            print_message \\\&quot;\\u670d\\u52a1\\u5df2\\u91cd\\u542f\\\&quot;\\n            ;;\\n        \\\&quot;logs\\\&quot;)\\n            docker-compose logs -f\\n            ;;\\n        \\\&quot;clean\\\&quot;)\\n            docker-compose down -v\\n            docker system prune -f\\n            print_message \\\&quot;Docker\\u8d44\\u6e90\\u6e05\\u7406\\u5b8c\\u6210\\\&quot;\\n            ;;\\n        \\\&quot;help\\\&quot;|\\\&quot;-h\\\&quot;|\\\&quot;--help\\\&quot;)\\n            show_usage\\n            ;;\\n        *)\\n            print_error \\\&quot;\\u672a\\u77e5\\u9009\\u9879: $1\\\&quot;\\n            show_usage\\n            exit 1\\n            ;;\\n    esac\\n}\\n\\n# \\u6267\\u884c\\u4e3b\\u51fd\\u6570\\nmain \\\&quot;$@\\\&quot;\&quot;}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;现在让我创建一个部署脚本来简化构建和运行过程：\n&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;File saved.  Saved file {docker-compose.yml}&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_01Cm7TUjgrQk6YSgJwtUZFtm&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;,&quot;folderRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-08T06:53:16.246Z&quot;,&quot;response_text&quot;:&quot;现在让我创建一个部署脚本来简化构建和运行过程：\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;f5755f29-d717-4f93-b1ab-8d6a0a05e5a1&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_01GM2As1bbxwM3XN2c58asmN&quot;,&quot;tool_name&quot;:&quot;save-file&quot;,&quot;input_json&quot;:&quot;{\&quot;instructions_reminder\&quot;: \&quot;LIMIT THE FILE CONTENT TO AT MOST 300 LINES. IF MORE CONTENT NEEDS TO BE ADDED USE THE str-replace-editor TOOL TO EDIT THE FILE AFTER IT HAS BEEN CREATED.\&quot;, \&quot;path\&quot;: \&quot;README_Docker.md\&quot;, \&quot;file_content\&quot;: \&quot;# \\u75b2\\u52b3\\u68c0\\u6d4b\\u7cfb\\u7edf Docker \\u90e8\\u7f72\\u6307\\u5357\\n\\n\\u672c\\u6307\\u5357\\u4ecb\\u7ecd\\u5982\\u4f55\\u4f7f\\u7528Docker\\u5728linux/arm64\\u67b6\\u6784\\u4e0a\\u90e8\\u7f72\\u75b2\\u52b3\\u68c0\\u6d4b\\u7cfb\\u7edf\\u3002\\n\\n## \\u7cfb\\u7edf\\u8981\\u6c42\\n\\n- Docker 20.10+\\n- Docker Compose 2.0+\\n- \\u652f\\u6301linux/arm64\\u67b6\\u6784\\u7684\\u7cfb\\u7edf\\n- \\u81f3\\u5c112GB\\u53ef\\u7528\\u5185\\u5b58\\n- \\u81f3\\u5c115GB\\u53ef\\u7528\\u78c1\\u76d8\\u7a7a\\u95f4\\n\\n## \\u5feb\\u901f\\u5f00\\u59cb\\n\\n### 1. \\u51c6\\u5907\\u73af\\u5883\\n\\n\\u786e\\u4fddDocker\\u548cDocker Compose\\u5df2\\u5b89\\u88c5\\uff1a\\n\\n```bash\\n# \\u68c0\\u67e5Docker\\u7248\\u672c\\ndocker --version\\ndocker-compose --version\\n\\n# \\u68c0\\u67e5\\u7cfb\\u7edf\\u67b6\\u6784\\nuname -m  # \\u5e94\\u8be5\\u663e\\u793a aarch64 \\u6216 arm64\\n```\\n\\n### 2. \\u514b\\u9686\\u9879\\u76ee\\u5e76\\u8fdb\\u5165\\u76ee\\u5f55\\n\\n```bash\\ncd /path/to/your/project\\n```\\n\\n### 3. \\u4f7f\\u7528\\u90e8\\u7f72\\u811a\\u672c\\uff08\\u63a8\\u8350\\uff09\\n\\n\\u7ed9\\u90e8\\u7f72\\u811a\\u672c\\u6267\\u884c\\u6743\\u9650\\uff1a\\n\\n```bash\\nchmod +x deploy.sh\\n```\\n\\n\\u5b8c\\u6574\\u90e8\\u7f72\\uff1a\\n\\n```bash\\n./deploy.sh deploy\\n```\\n\\n### 4. \\u624b\\u52a8\\u90e8\\u7f72\\uff08\\u53ef\\u9009\\uff09\\n\\n\\u5982\\u679c\\u4e0d\\u4f7f\\u7528\\u90e8\\u7f72\\u811a\\u672c\\uff0c\\u53ef\\u4ee5\\u624b\\u52a8\\u6267\\u884c\\uff1a\\n\\n```bash\\n# \\u6784\\u5efa\\u5e76\\u542f\\u52a8\\u670d\\u52a1\\ndocker-compose up -d --build\\n\\n# \\u67e5\\u770b\\u670d\\u52a1\\u72b6\\u6001\\ndocker-compose ps\\n```\\n\\n## \\u90e8\\u7f72\\u811a\\u672c\\u4f7f\\u7528\\u8bf4\\u660e\\n\\n\\u90e8\\u7f72\\u811a\\u672c `deploy.sh` \\u63d0\\u4f9b\\u4e86\\u591a\\u79cd\\u64cd\\u4f5c\\u9009\\u9879\\uff1a\\n\\n```bash\\n# \\u5b8c\\u6574\\u90e8\\u7f72\\uff08\\u6784\\u5efa\\u955c\\u50cf\\u5e76\\u542f\\u52a8\\u670d\\u52a1\\uff09\\n./deploy.sh deploy\\n\\n# \\u4ec5\\u6784\\u5efaDocker\\u955c\\u50cf\\n./deploy.sh build\\n\\n# \\u542f\\u52a8\\u5df2\\u5b58\\u5728\\u7684\\u670d\\u52a1\\n./deploy.sh start\\n\\n# \\u505c\\u6b62\\u670d\\u52a1\\n./deploy.sh stop\\n\\n# \\u91cd\\u542f\\u670d\\u52a1\\n./deploy.sh restart\\n\\n# \\u67e5\\u770b\\u670d\\u52a1\\u65e5\\u5fd7\\n./deploy.sh logs\\n\\n# \\u6e05\\u7406Docker\\u8d44\\u6e90\\n./deploy.sh clean\\n\\n# \\u663e\\u793a\\u5e2e\\u52a9\\u4fe1\\u606f\\n./deploy.sh help\\n```\\n\\n## \\u8bbf\\u95ee\\u5e94\\u7528\\n\\n\\u90e8\\u7f72\\u6210\\u529f\\u540e\\uff0c\\u53ef\\u4ee5\\u901a\\u8fc7\\u4ee5\\u4e0b\\u5730\\u5740\\u8bbf\\u95ee\\u5e94\\u7528\\uff1a\\n\\n- **\\u4e3b\\u9875**: http://localhost:8000\\n- **\\u767b\\u5f55\\u9875\\u9762**: http://localhost:8000/\\n- **\\u6ce8\\u518c\\u9875\\u9762**: http://localhost:8000/register\\n- **\\u4eea\\u8868\\u677f**: http://localhost:8000/dashboard\\n\\n## \\u76ee\\u5f55\\u7ed3\\u6784\\n\\n```\\n.\\n\\u251c\\u2500\\u2500 Dockerfile              # Docker\\u955c\\u50cf\\u6784\\u5efa\\u6587\\u4ef6\\n\\u251c\\u2500\\u2500 docker-compose.yml      # Docker Compose\\u914d\\u7f6e\\n\\u251c\\u2500\\u2500 requirements.txt        # Python\\u4f9d\\u8d56\\u5305\\n\\u251c\\u2500\\u2500 deploy.sh              # \\u90e8\\u7f72\\u811a\\u672c\\n\\u251c\\u2500\\u2500 .dockerignore          # Docker\\u5ffd\\u7565\\u6587\\u4ef6\\n\\u251c\\u2500\\u2500 fatigue_web_ffmpeg.py  # \\u4e3b\\u5e94\\u7528\\u6587\\u4ef6\\n\\u251c\\u2500\\u2500 config.py              # \\u914d\\u7f6e\\u6587\\u4ef6\\n\\u251c\\u2500\\u2500 model.py               # AI\\u6a21\\u578b\\u5b9a\\u4e49\\n\\u251c\\u2500\\u2500 utils.py               # \\u5de5\\u5177\\u51fd\\u6570\\n\\u251c\\u2500\\u2500 database_config.py     # \\u6570\\u636e\\u5e93\\u914d\\u7f6e\\n\\u251c\\u2500\\u2500 static/                # \\u9759\\u6001\\u6587\\u4ef6\\u76ee\\u5f55\\n\\u251c\\u2500\\u2500 templates/             # \\u6a21\\u677f\\u6587\\u4ef6\\u76ee\\u5f55\\n\\u2514\\u2500\\u2500 output/                # \\u8f93\\u51fa\\u76ee\\u5f55\\n    \\u251c\\u2500\\u2500 models/            # \\u6a21\\u578b\\u6587\\u4ef6\\n    \\u251c\\u2500\\u2500 logs/              # \\u65e5\\u5fd7\\u6587\\u4ef6\\n    \\u2514\\u2500\\u2500 processed_data/    # \\u5904\\u7406\\u540e\\u7684\\u6570\\u636e\\n```\\n\\n## \\u914d\\u7f6e\\u8bf4\\u660e\\n\\n### \\u73af\\u5883\\u53d8\\u91cf\\n\\n\\u53ef\\u4ee5\\u5728 `docker-compose.yml` \\u4e2d\\u4fee\\u6539\\u73af\\u5883\\u53d8\\u91cf\\uff1a\\n\\n```yaml\\nenvironment:\\n  - PYTHONUNBUFFERED=1\\n  - TZ=Asia/Shanghai\\n```\\n\\n### \\u7aef\\u53e3\\u914d\\u7f6e\\n\\n\\u9ed8\\u8ba4\\u7aef\\u53e3\\u4e3a8000\\uff0c\\u53ef\\u4ee5\\u5728 `docker-compose.yml` \\u4e2d\\u4fee\\u6539\\uff1a\\n\\n```yaml\\nports:\\n  - \\\&quot;8000:8000\\\&quot;  # \\u4fee\\u6539\\u5de6\\u4fa7\\u7aef\\u53e3\\u53f7\\n```\\n\\n### \\u8d44\\u6e90\\u9650\\u5236\\n\\n\\u53ef\\u4ee5\\u5728 `docker-compose.yml` \\u4e2d\\u8c03\\u6574\\u8d44\\u6e90\\u9650\\u5236\\uff1a\\n\\n```yaml\\ndeploy:\\n  resources:\\n    limits:\\n      memory: 2G      # \\u5185\\u5b58\\u9650\\u5236\\n      cpus: '1.0'     # CPU\\u9650\\u5236\\n```\\n\\n## \\u6545\\u969c\\u6392\\u9664\\n\\n### 1. \\u67e5\\u770b\\u65e5\\u5fd7\\n\\n```bash\\n# \\u67e5\\u770b\\u6240\\u6709\\u670d\\u52a1\\u65e5\\u5fd7\\ndocker-compose logs\\n\\n# \\u67e5\\u770b\\u7279\\u5b9a\\u670d\\u52a1\\u65e5\\u5fd7\\ndocker-compose logs fatigue-detection\\n\\n# \\u5b9e\\u65f6\\u67e5\\u770b\\u65e5\\u5fd7\\ndocker-compose logs -f\\n```\\n\\n### 2. \\u68c0\\u67e5\\u5bb9\\u5668\\u72b6\\u6001\\n\\n```bash\\n# \\u67e5\\u770b\\u5bb9\\u5668\\u72b6\\u6001\\ndocker-compose ps\\n\\n# \\u67e5\\u770b\\u8be6\\u7ec6\\u4fe1\\u606f\\ndocker-compose top\\n```\\n\\n### 3. \\u8fdb\\u5165\\u5bb9\\u5668\\u8c03\\u8bd5\\n\\n```bash\\n# \\u8fdb\\u5165\\u5bb9\\u5668\\ndocker-compose exec fatigue-detection bash\\n\\n# \\u68c0\\u67e5Python\\u73af\\u5883\\ndocker-compose exec fatigue-detection python --version\\n```\\n\\n### 4. \\u5e38\\u89c1\\u95ee\\u9898\\n\\n**\\u95ee\\u98981**: \\u6784\\u5efa\\u5931\\u8d25\\uff0c\\u63d0\\u793a\\u67b6\\u6784\\u4e0d\\u5339\\u914d\\n```bash\\n# \\u89e3\\u51b3\\u65b9\\u6848\\uff1a\\u786e\\u4fdd\\u4f7f\\u7528\\u6b63\\u786e\\u7684\\u5e73\\u53f0\\u53c2\\u6570\\ndocker build --platform linux/arm64 -t fatigue-detection .\\n```\\n\\n**\\u95ee\\u98982**: \\u670d\\u52a1\\u65e0\\u6cd5\\u542f\\u52a8\\n```bash\\n# \\u68c0\\u67e5\\u7aef\\u53e3\\u662f\\u5426\\u88ab\\u5360\\u7528\\nnetstat -tlnp | grep 8000\\n\\n# \\u4fee\\u6539\\u7aef\\u53e3\\u6216\\u505c\\u6b62\\u5360\\u7528\\u7aef\\u53e3\\u7684\\u670d\\u52a1\\n```\\n\\n**\\u95ee\\u98983**: \\u5185\\u5b58\\u4e0d\\u8db3\\n```bash\\n# \\u68c0\\u67e5\\u7cfb\\u7edf\\u5185\\u5b58\\nfree -h\\n\\n# \\u8c03\\u6574docker-compose.yml\\u4e2d\\u7684\\u5185\\u5b58\\u9650\\u5236\\n```\\n\\n## \\u6570\\u636e\\u6301\\u4e45\\u5316\\n\\n\\u91cd\\u8981\\u6570\\u636e\\u901a\\u8fc7Docker\\u5377\\u8fdb\\u884c\\u6301\\u4e45\\u5316\\uff1a\\n\\n- `./output:/app/output` - \\u6a21\\u578b\\u6587\\u4ef6\\u3001\\u65e5\\u5fd7\\u3001\\u5904\\u7406\\u6570\\u636e\\n- `./static:/app/static` - \\u9759\\u6001\\u6587\\u4ef6\\n- `./templates:/app/templates` - \\u6a21\\u677f\\u6587\\u4ef6\\n\\n## \\u5b89\\u5168\\u6ce8\\u610f\\u4e8b\\u9879\\n\\n1. **\\u6570\\u636e\\u5e93\\u914d\\u7f6e**: \\u4fee\\u6539 `database_config.py` \\u4e2d\\u7684\\u6570\\u636e\\u5e93\\u8fde\\u63a5\\u4fe1\\u606f\\n2. **\\u7aef\\u53e3\\u66b4\\u9732**: \\u751f\\u4ea7\\u73af\\u5883\\u5efa\\u8bae\\u4f7f\\u7528\\u53cd\\u5411\\u4ee3\\u7406\\n3. **\\u6587\\u4ef6\\u6743\\u9650**: \\u786e\\u4fdd\\u6302\\u8f7d\\u76ee\\u5f55\\u6709\\u6b63\\u786e\\u7684\\u6743\\u9650\\u8bbe\\u7f6e\\n\\n## \\u6027\\u80fd\\u4f18\\u5316\\n\\n1. **\\u591a\\u6838CPU**: \\u53ef\\u4ee5\\u589e\\u52a0uvicorn\\u7684worker\\u6570\\u91cf\\n2. **\\u5185\\u5b58\\u4f18\\u5316**: \\u6839\\u636e\\u5b9e\\u9645\\u9700\\u6c42\\u8c03\\u6574\\u5185\\u5b58\\u9650\\u5236\\n3. **\\u5b58\\u50a8\\u4f18\\u5316**: \\u4f7f\\u7528SSD\\u5b58\\u50a8\\u63d0\\u9ad8I/O\\u6027\\u80fd\\n\\n## \\u66f4\\u65b0\\u548c\\u7ef4\\u62a4\\n\\n### \\u66f4\\u65b0\\u5e94\\u7528\\n\\n```bash\\n# \\u505c\\u6b62\\u670d\\u52a1\\n./deploy.sh stop\\n\\n# \\u66f4\\u65b0\\u4ee3\\u7801\\u540e\\u91cd\\u65b0\\u6784\\u5efa\\n./deploy.sh deploy\\n```\\n\\n### \\u5907\\u4efd\\u6570\\u636e\\n\\n```bash\\n# \\u5907\\u4efd\\u8f93\\u51fa\\u76ee\\u5f55\\ntar -czf backup_$(date +%Y%m%d).tar.gz output/\\n```\\n\\n### \\u6e05\\u7406\\u8d44\\u6e90\\n\\n```bash\\n# \\u6e05\\u7406\\u672a\\u4f7f\\u7528\\u7684Docker\\u8d44\\u6e90\\n./deploy.sh clean\\n```\\n\\n## \\u652f\\u6301\\n\\n\\u5982\\u679c\\u9047\\u5230\\u95ee\\u9898\\uff0c\\u8bf7\\u68c0\\u67e5\\uff1a\\n\\n1. \\u7cfb\\u7edf\\u67b6\\u6784\\u662f\\u5426\\u4e3aarm64\\n2. Docker\\u7248\\u672c\\u662f\\u5426\\u6ee1\\u8db3\\u8981\\u6c42\\n3. \\u5185\\u5b58\\u548c\\u78c1\\u76d8\\u7a7a\\u95f4\\u662f\\u5426\\u5145\\u8db3\\n4. \\u7f51\\u7edc\\u8fde\\u63a5\\u662f\\u5426\\u6b63\\u5e38\\n\\n\\u66f4\\u591a\\u6280\\u672f\\u652f\\u6301\\uff0c\\u8bf7\\u67e5\\u770b\\u9879\\u76ee\\u6587\\u6863\\u6216\\u8054\\u7cfb\\u5f00\\u53d1\\u56e2\\u961f\\u3002\&quot;}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;现在让我创建一个README文件来说明如何使用这些Docker部署文件：\n&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;File saved.  Saved file {deploy.sh}&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_01ULcB34zoS7pR61wxUmjJpa&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;,&quot;folderRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-08T06:53:41.612Z&quot;,&quot;response_text&quot;:&quot;现在让我创建一个README文件来说明如何使用这些Docker部署文件：\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;92e8c37f-8516-4478-bd6c-6afe98582047&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_01MFYezfsXZe1MDR98FPJcaE&quot;,&quot;tool_name&quot;:&quot;launch-process&quot;,&quot;input_json&quot;:&quot;{\&quot;command\&quot;: \&quot;chmod +x deploy.sh\&quot;, \&quot;cwd\&quot;: \&quot;C:/Users/wh/Downloads/FatigueDetection_user\&quot;, \&quot;wait\&quot;: true, \&quot;max_wait_seconds\&quot;: 10}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;最后，让我给部署脚本添加执行权限：\n&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;File saved.  Saved file {README_Docker.md}&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_01GM2As1bbxwM3XN2c58asmN&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;,&quot;folderRoot&quot;:&quot;C:/Users/wh/Downloads/FatigueDetection_user&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-08T06:54:24.666Z&quot;,&quot;response_text&quot;:&quot;最后，让我给部署脚本添加执行权限：\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;request_id&quot;:&quot;e04ca147-dd2d-4eef-ad34-398f5d4cd5d7&quot;,&quot;status&quot;:&quot;cancelled&quot;,&quot;chatItemType&quot;:&quot;agentic-turn-delimiter&quot;},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;8e88e51c-dcec-4d34-9904-913d9ec658a7&quot;,&quot;request_message&quot;:&quot;(pljc) PS C:\\Users\\wh\\Downloads&gt; docker login\r\nAuthenticating with existing credentials...\r\nLogin did not succeed, error: Error response from daemon: Get \&quot;https://registry-1.docker.io/v2/\&quot;: context deadline exceeded\r\nLog in with your Docker ID or email address to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com/ to create one.\r\nYou can log in with your password or a Personal Access Token (PAT). Using a limited-scope PAT grants better security and is required for organizations using SSO. Learn more at https://docs.docker.com/go/access-tokens/\r\n\r\nUsername (whwhwh123456): whwhwh123456\r\nPassword:\r\nError response from daemon: Get \&quot;https://registry-1.docker.io/v2/\&quot;: dialing registry-1.docker.io:443 with direct connection: resolving host registry-1.docker.io: lookup registry-1.docker.io: no such host\r\n(pljc) PS C:\\Users\\wh\\Downloads&gt; docker login\r\nAuthenticating with existing credentials...\r\nLogin did not succeed, error: Error response from daemon: Get \&quot;https://registry-1.docker.io/v2/\&quot;: EOF\r\nLog in with your Docker ID or email address to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com/ to create one.\r\nYou can log in with your password or a Personal Access Token (PAT). Using a limited-scope PAT grants better security and is required for organizations using SSO. Learn more at https://docs.docker.com/go/access-tokens/\r\n\r\nUsername (whwhwh123456): whwhwh123456\r\nPassword:\r\nError response from daemon: Get \&quot;https://registry-1.docker.io/v2/\&quot;: EOF\r\n怎么登录不上&quot;,&quot;rich_text_json_repr&quot;:{&quot;type&quot;:&quot;doc&quot;,&quot;content&quot;:[{&quot;type&quot;:&quot;paragraph&quot;,&quot;content&quot;:[{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;(pljc) PS C:\\Users\\wh\\Downloads&gt; docker login\r&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;Authenticating with existing credentials...\r&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;Login did not succeed, error: Error response from daemon: Get \&quot;https://registry-1.docker.io/v2/\&quot;: context deadline exceeded\r&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;Log in with your Docker ID or email address to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com/ to create one.\r&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;You can log in with your password or a Personal Access Token (PAT). Using a limited-scope PAT grants better security and is required for organizations using SSO. Learn more at https://docs.docker.com/go/access-tokens/\r&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;\r&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;Username (whwhwh123456): whwhwh123456\r&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;Password:\r&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;Error response from daemon: Get \&quot;https://registry-1.docker.io/v2/\&quot;: dialing registry-1.docker.io:443 with direct connection: resolving host registry-1.docker.io: lookup registry-1.docker.io: no such host\r&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;(pljc) PS C:\\Users\\wh\\Downloads&gt; docker login\r&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;Authenticating with existing credentials...\r&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;Login did not succeed, error: Error response from daemon: Get \&quot;https://registry-1.docker.io/v2/\&quot;: EOF\r&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;Log in with your Docker ID or email address to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com/ to create one.\r&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;You can log in with your password or a Personal Access Token (PAT). Using a limited-scope PAT grants better security and is required for organizations using SSO. Learn more at https://docs.docker.com/go/access-tokens/\r&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;\r&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;Username (whwhwh123456): whwhwh123456\r&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;Password:\r&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;Error response from daemon: Get \&quot;https://registry-1.docker.io/v2/\&quot;: EOF\r&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;怎么登录不上&quot;}]}]},&quot;mentioned_items&quot;:[],&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_01DXcuiRzjBSLdFgFBPBU3e8&quot;,&quot;tool_name&quot;:&quot;launch-process&quot;,&quot;input_json&quot;:&quot;{\&quot;command\&quot;: \&quot;ping registry-1.docker.io\&quot;, \&quot;cwd\&quot;: \&quot;D:/FatigueDetection_user\&quot;, \&quot;wait\&quot;: true, \&quot;max_wait_seconds\&quot;: 10}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;看起来您遇到了Docker Hub连接问题。这通常是网络连接或DNS解析问题导致的。让我帮您解决这个问题：\n\n## 解决Docker登录问题的方法\n\n### 1. 检查网络连接\n&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:0,&quot;type&quot;:0,&quot;text_node&quot;:{&quot;content&quot;:&quot;(pljc) PS C:\\Users\\wh\\Downloads&gt; docker login\r\nAuthenticating with existing credentials...\r\nLogin did not succeed, error: Error response from daemon: Get \&quot;https://registry-1.docker.io/v2/\&quot;: context deadline exceeded\r\nLog in with your Docker ID or email address to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com/ to create one.\r\nYou can log in with your password or a Personal Access Token (PAT). Using a limited-scope PAT grants better security and is required for organizations using SSO. Learn more at https://docs.docker.com/go/access-tokens/\r\n\r\nUsername (whwhwh123456): whwhwh123456\r\nPassword:\r\nError response from daemon: Get \&quot;https://registry-1.docker.io/v2/\&quot;: dialing registry-1.docker.io:443 with direct connection: resolving host registry-1.docker.io: lookup registry-1.docker.io: no such host\r\n(pljc) PS C:\\Users\\wh\\Downloads&gt; docker login\r\nAuthenticating with existing credentials...\r\nLogin did not succeed, error: Error response from daemon: Get \&quot;https://registry-1.docker.io/v2/\&quot;: EOF\r\nLog in with your Docker ID or email address to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com/ to create one.\r\nYou can log in with your password or a Personal Access Token (PAT). Using a limited-scope PAT grants better security and is required for organizations using SSO. Learn more at https://docs.docker.com/go/access-tokens/\r\n\r\nUsername (whwhwh123456): whwhwh123456\r\nPassword:\r\nError response from daemon: Get \&quot;https://registry-1.docker.io/v2/\&quot;: EOF\r\n怎么登录不上&quot;}},{&quot;id&quot;:1,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;D:/FatigueDetection_user&quot;,&quot;folderRoot&quot;:&quot;D:/FatigueDetection_user&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-08T07:13:00.814Z&quot;,&quot;response_text&quot;:&quot;看起来您遇到了Docker Hub连接问题。这通常是网络连接或DNS解析问题导致的。让我帮您解决这个问题：\n\n## 解决Docker登录问题的方法\n\n### 1. 检查网络连接\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;4800304e-4799-4701-a072-840c1d8902e1&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_014Yvbxn8YpCRVz2jhnzJd5U&quot;,&quot;tool_name&quot;:&quot;kill-process&quot;,&quot;input_json&quot;:&quot;{\&quot;terminal_id\&quot;: 2}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;Command is still running after 10 seconds. You can use read-process to get more output\n          and kill-process to terminate it if needed.\n          PID 2\n          Output so far:\n          &lt;stdout&gt;\n          \u001b[?9001h\u001b[?1004h\u001b[?25l\u001b[2J\u001b[m\u001b[2;1H正在 Ping registry-1.docker.io [3.94.224.37] 具有 32 字节的数据:\n\u001b]0;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe\u0007\u001b[?25h\u001b[?25l\u001b[8;7;161t\u001b[H\u001b[K\n正在 Ping registry-1.docker.io [3.94.224.37] 具有 32 字节的数据:\u001b[K\n\u001b[K\n\u001b[K\n\u001b[K\n\u001b[K\n\u001b[K\u001b[3;1H\u001b[?25h请求超时。\n请求超时。\n\n          &lt;/stdout&gt;\n          &lt;stderr&gt;\n          \n          &lt;/stderr&gt;&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_01DXcuiRzjBSLdFgFBPBU3e8&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;D:/FatigueDetection_user&quot;,&quot;folderRoot&quot;:&quot;D:/FatigueDetection_user&quot;}],&quot;workspaceFoldersUnchanged&quot;:false,&quot;currentTerminal&quot;:{&quot;terminalId&quot;:2,&quot;currentWorkingDirectory&quot;:&quot;D:/FatigueDetection_user&quot;}}}],&quot;timestamp&quot;:&quot;2025-07-08T07:13:33.186Z&quot;,&quot;response_text&quot;:&quot;&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;e2a9cfac-89ae-4ad7-8f44-574e9ed4866e&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_01JkM8ZrLxa1EU6AfABUSHZx&quot;,&quot;tool_name&quot;:&quot;save-file&quot;,&quot;input_json&quot;:&quot;{\&quot;instructions_reminder\&quot;: \&quot;LIMIT THE FILE CONTENT TO AT MOST 300 LINES. IF MORE CONTENT NEEDS TO BE ADDED USE THE str-replace-editor TOOL TO EDIT THE FILE AFTER IT HAS BEEN CREATED.\&quot;, \&quot;path\&quot;: \&quot;docker-daemon.json\&quot;, \&quot;file_content\&quot;: \&quot;{\\n  \\\&quot;registry-mirrors\\\&quot;: [\\n    \\\&quot;https://docker.mirrors.ustc.edu.cn\\\&quot;,\\n    \\\&quot;https://hub-mirror.c.163.com\\\&quot;,\\n    \\\&quot;https://mirror.baidubce.com\\\&quot;,\\n    \\\&quot;https://ccr.ccs.tencentyun.com\\\&quot;\\n  ],\\n  \\\&quot;insecure-registries\\\&quot;: [],\\n  \\\&quot;debug\\\&quot;: false,\\n  \\\&quot;experimental\\\&quot;: false\\n}\&quot;}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:0,&quot;type&quot;:0,&quot;content&quot;:&quot;看起来您的网络无法访问Docker Hub。这是一个常见的网络问题。让我为您提供几种解决方案：\n\n## 解决方案\n\n### 1. 使用国内Docker镜像源（推荐）\n\n由于网络问题，建议配置国内的Docker镜像源。创建或修改Docker配置文件：\n&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;Terminal 2 killed&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_014Yvbxn8YpCRVz2jhnzJd5U&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;D:/FatigueDetection_user&quot;,&quot;folderRoot&quot;:&quot;D:/FatigueDetection_user&quot;}],&quot;workspaceFoldersUnchanged&quot;:false,&quot;currentTerminal&quot;:{&quot;terminalId&quot;:2,&quot;currentWorkingDirectory&quot;:&quot;D:/FatigueDetection_user&quot;}}}],&quot;timestamp&quot;:&quot;2025-07-08T07:13:38.207Z&quot;,&quot;response_text&quot;:&quot;看起来您的网络无法访问Docker Hub。这是一个常见的网络问题。让我为您提供几种解决方案：\n\n## 解决方案\n\n### 1. 使用国内Docker镜像源（推荐）\n\n由于网络问题，建议配置国内的Docker镜像源。创建或修改Docker配置文件：\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;request_id&quot;:&quot;791bc5fe-e98d-4048-b63a-664e7a264b2d&quot;,&quot;uuid&quot;:&quot;3c926e3c-10e7-4689-89a2-3be2a0dc3597&quot;,&quot;chatItemType&quot;:&quot;agentic-checkpoint-delimiter&quot;,&quot;status&quot;:&quot;success&quot;,&quot;fromTimestamp&quot;:0,&quot;toTimestamp&quot;:1751958887523,&quot;seen_state&quot;:&quot;seen&quot;},{&quot;status&quot;:&quot;sent&quot;,&quot;request_id&quot;:&quot;temp-fe-2ae0cd19-c8af-4f0d-b374-8dccdede8d01&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;File saved.  Saved file {docker-daemon.json}&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_01JkM8ZrLxa1EU6AfABUSHZx&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;D:/FatigueDetection_user&quot;,&quot;folderRoot&quot;:&quot;D:/FatigueDetection_user&quot;}],&quot;workspaceFoldersUnchanged&quot;:false,&quot;currentTerminal&quot;:{&quot;terminalId&quot;:2,&quot;currentWorkingDirectory&quot;:&quot;D:/FatigueDetection_user&quot;}}}],&quot;timestamp&quot;:&quot;2025-07-08T07:14:49.709Z&quot;}],&quot;feedbackStates&quot;:{&quot;temp-fe-174bd646-8b04-4a02-b448-e363034a84d3&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-b39e4e2c-f31e-4dd6-9922-f178dcf4ba6f&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-f000d0e8-fd42-4836-931d-faecc149159f&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-199a758b-408a-4313-a87f-7a6ea483ac9f&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-a3756a48-0784-4836-a7a4-07cc6ec77b83&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-0c9cdabb-bbfb-4a58-b6b3-29594ac01bc9&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-c7ddec1c-ad63-4392-ac77-331b4e5adaa5&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-d52d7b1b-aa96-4686-bab2-8992c69e7937&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-dbc20f28-7cfb-4bef-b988-3ca8e3602f1e&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-f86faf8b-ba8a-4a5a-b0ee-1bddc00dbb24&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-9a06786d-f664-4ccc-9e3b-73a7a67b391b&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-fcf0da27-4be8-49fe-b619-1c37dae507b8&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-6ea9a19f-f421-4955-afd8-1f3354122363&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-93fa547f-5567-457c-b1b0-e9bb18e63e06&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-9a1f47f8-b20e-4e75-a581-cabd90458f63&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-1a5bd0db-5387-4ceb-8e4a-3e80d025bcf4&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-1c681c54-2783-451f-94e2-61977b71b0e9&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-af5416ab-5a61-4dd0-9a41-99189c1ae600&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-2ae0cd19-c8af-4f0d-b374-8dccdede8d01&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;}},&quot;toolUseStates&quot;:{&quot;eb97ae73-945b-4c90-a518-9a279174851b;toolu_01N4mzXrfMevV9ceESi1xJEC&quot;:{&quot;requestId&quot;:&quot;eb97ae73-945b-4c90-a518-9a279174851b&quot;,&quot;toolUseId&quot;:&quot;toolu_01N4mzXrfMevV9ceESi1xJEC&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;Here's the files and directories up to 2 levels deep in ., excluding hidden items:\n./__pycache__\n./__pycache__\\config.cpython-312.pyc\n./__pycache__\\database_config.cpython-312.pyc\n./__pycache__\\model.cpython-312.pyc\n./__pycache__\\utils.cpython-312.pyc\n./config.py\n./database_config.py\n./fatigue_web_ffmpeg.py\n./model.py\n./output\n./output\\logs\n./output\\models\n./output\\processed_data\n./static\n./static\\warning.mp3\n./templates\n./templates\\dashboard_backend_ffmpeg.html\n./templates\\login.html\n./templates\\register.html\n./utils.py\n&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;7136fb3b-b082-4877-8680-8493f4d26d81;toolu_01SeJKDiP8RGjhQGDFQY4KEd&quot;:{&quot;requestId&quot;:&quot;7136fb3b-b082-4877-8680-8493f4d26d81&quot;,&quot;toolUseId&quot;:&quot;toolu_01SeJKDiP8RGjhQGDFQY4KEd&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;Here's the result of running `cat -n` on fatigue_web_ffmpeg.py:\n     1\t\&quot;\&quot;\&quot;\n     2\t疲劳检测Web应用 - FFmpeg后端优化版本\n     3\t使用FFmpeg进行视频压缩和优化，大幅减少网络传输数据量\n     4\t集成完整的AI疲劳检测功能\n     5\t\&quot;\&quot;\&quot;\n     6\timport cv2\n     7\timport torch\n     8\timport numpy as np\n     9\timport dlib\n    10\tfrom collections import deque\n    11\timport base64\n    12\timport json\n    13\timport subprocess\n    14\timport tempfile\n    15\timport os\n    16\timport sys\n    17\timport pygame\n    18\timport asyncio\n    19\timport time\n    20\timport threading\n    21\tfrom queue import Queue, Empty\n    22\tfrom datetime import datetime\n    23\tfrom typing import Dict, Any\n    24\t\n    25\tfrom fastapi import FastAPI, WebSocket, WebSocketDisconnect, Request, Form, HTTPException\n    26\tfrom fastapi.responses import HTMLResponse, JSONResponse, RedirectResponse\n    27\tfrom fastapi.staticfiles import StaticFiles\n    28\tfrom fastapi.templating import Jinja2Templates\n    29\timport uvicorn\n    30\t\n    31\t# 导入AI检测相关模块\n    32\ttry:\n    33\t    from config import *\n    34\t    from model import create_model\n    35\t    from utils import extract_face_landmarks, normalize_landmarks\n    36\t    from database_config import get_db_connection, init_database\n    37\t\n    38\t    # 定义模型路径\n    39\t    MODEL_PATH = os.path.join(MODEL_SAVE_PATH, \&quot;best_model.pth\&quot;)\n    40\t    AI_MODULES_AVAILABLE = True\n    41\t    print(\&quot;✅ AI模块导入成功\&quot;)\n    42\texcept ImportError as e:\n    43\t    print(f\&quot;⚠️  AI模块导入失败: {e}\&quot;)\n    44\t    print(\&quot;系统将在基础模式下运行\&quot;)\n    45\t    AI_MODULES_AVAILABLE = False\n    46\t\n    47\t    # 定义基础配置\n    48\t    MODEL_PATH = \&quot;output/models/best_model.pth\&quot;\n    49\t    DLIB_PREDICTOR_PATH = \&quot;output/models/shape_predictor_68_face_landmarks.dat\&quot;\n    50\t    SEQUENCE_LENGTH = 30\n    51\t    FACE_SIZE = (64, 64)\n    52\t\n    53\t    # 创建模拟函数\n    54\t    def create_model():\n    55\t        return None\n    56\t\n    57\t    def extract_face_landmarks(frame, detector, predictor):\n    58\t        return None, None\n    59\t\n    60\t    def normalize_landmarks(landmarks, shape):\n    61\t        return None\n    62\t\n    63\t    def init_database():\n    64\t        pass\n    65\t\n    66\tapp = FastAPI(title=\&quot;疲劳检测系统 - FFmpeg优化版\&quot;)\n    67\t\n    68\t# 静态文件和模板\n    69\tapp.mount(\&quot;/static\&quot;, StaticFiles(directory=\&quot;static\&quot;), name=\&quot;static\&quot;)\n    70\ttemplates = Jinja2Templates(directory=\&quot;templates\&quot;)\n    71\t\n    72\tclass FFmpegVideoProcessor:\n    73\t    \&quot;\&quot;\&quot;FFmpeg视频处理器\&quot;\&quot;\&quot;\n    74\t    \n    75\t    def __init__(self):\n    76\t        self.temp_dir = tempfile.mkdtemp()\n    77\t        self.frame_counter = 0\n    78\t        self.ffmpeg_path = None  # 将存储FFmpeg的路径\n    79\t\n    80\t        # FFmpeg压缩参数\n    81\t        self.compression_settings = {\n    82\t            'preset': 'veryfast',\n    83\t            'crf': '28',  # 恒定质量因子 (18-28为合理范围)\n    84\t            'scale': '320:240',\n    85\t            'fps': '5',\n    86\t            'format': 'webm'\n    87\t        }\n    88\t        \n    89\t    def check_ffmpeg(self):\n    90\t        \&quot;\&quot;\&quot;检查FFmpeg是否可用\&quot;\&quot;\&quot;\n    91\t        # 首先尝试PATH中的ffmpeg\n    92\t        try:\n    93\t            result = subprocess.run(['ffmpeg', '-version'],\n    94\t                                  capture_output=True, text=True, timeout=5)\n    95\t            if result.returncode == 0:\n    96\t                self.ffmpeg_path = 'ffmpeg'  # 使用PATH中的ffmpeg\n    97\t                return True\n    98\t        except:\n    99\t            pass\n   100\t\n   101\t        # 如果PATH中没有，尝试常见的Windows安装路径\n   102\t        common_paths = [\n   103\t            r\&quot;C:\\ffmpeg\\bin\\ffmpeg.exe\&quot;,\n   104\t            r\&quot;C:\\Program Files\\ffmpeg\\bin\\ffmpeg.exe\&quot;,\n   105\t            r\&quot;C:\\Program Files (x86)\\ffmpeg\\bin\\ffmpeg.exe\&quot;,\n   106\t            r\&quot;D:\\ffmpeg\\bin\\ffmpeg.exe\&quot;\n   107\t        ]\n   108\t\n   109\t        for path in common_paths:\n   110\t            if os.path.exists(path):\n   111\t                try:\n   112\t                    result = subprocess.run([path, '-version'],\n   113\t                                          capture_output=True, text=True, timeout=5)\n   114\t                    if result.returncode == 0:\n   115\t                        self.ffmpeg_path = path  # 使用找到的完整路径\n   116\t                        print(f\&quot;✅ 找到FFmpeg: {path}\&quot;)\n   117\t                        return True\n   118\t                except:\n   119\t                    continue\n   120\t\n   121\t        self.ffmpeg_path = None\n   122\t        return False\n   123\t    \n   124\t    def compress_frame(self, frame_data):\n   125\t        \&quot;\&quot;\&quot;使用FFmpeg压缩单帧\&quot;\&quot;\&quot;\n   126\t        try:\n   127\t            # 解码base64图像\n   128\t            if frame_data.startswith('data:image'):\n   129\t                frame_data = frame_data.split(',')[1]\n   130\t            \n   131\t            img_data = base64.b64decode(frame_data)\n   132\t            \n   133\t            # 创建临时文件\n   134\t            self.frame_counter += 1\n   135\t            input_file = os.path.join(self.temp_dir, f'input_{self.frame_counter}.png')\n   136\t            output_file = os.path.join(self.temp_dir, f'output_{self.frame_counter}.webm')\n   137\t            \n   138\t            # 写入输入文件\n   139\t            with open(input_file, 'wb') as f:\n   140\t                f.write(img_data)\n   141\t            \n   142\t            # FFmpeg压缩命令\n   143\t            cmd = [\n   144\t                self.ffmpeg_path or 'ffmpeg',  # 使用找到的FFmpeg路径\n   145\t                '-y',  # 覆盖输出文件\n   146\t                '-i', input_file,\n   147\t                '-c:v', 'libvpx-vp9',  # VP9编码器\n   148\t                '-preset', self.compression_settings['preset'],\n   149\t                '-crf', self.compression_settings['crf'],\n   150\t                '-vf', f\&quot;scale={self.compression_settings['scale']},fps={self.compression_settings['fps']}\&quot;,\n   151\t                '-f', self.compression_settings['format'],\n   152\t                '-loglevel', 'quiet',  # 静默模式\n   153\t                output_file\n   154\t            ]\n   155\t            \n   156\t            # 执行压缩\n   157\t            result = subprocess.run(cmd, capture_output=True, timeout=10)\n   158\t            \n   159\t            if result.returncode == 0 and os.path.exists(output_file):\n   160\t                # 读取压缩后的文件\n   161\t                with open(output_file, 'rb') as f:\n   162\t                    compressed_data = f.read()\n   163\t                \n   164\t                # 清理临时文件\n   165\t                self._cleanup_files([input_file, output_file])\n   166\t                \n   167\t                # 计算压缩比\n   168\t                original_size = len(img_data)\n   169\t                compressed_size = len(compressed_data)\n   170\t                compression_ratio = (1 - compressed_size / original_size) * 100\n   171\t                \n   172\t                return {\n   173\t                    'data': base64.b64encode(compressed_data).decode('utf-8'),\n   174\t                    'original_size': original_size,\n   175\t                    'compressed_size': compressed_size,\n   176\t                    'compression_ratio': compression_ratio\n   177\t                }\n   178\t            else:\n   179\t                self._cleanup_files([input_file, output_file])\n   180\t                return None\n   181\t                \n   182\t        except Exception as e:\n   183\t            print(f\&quot;FFmpeg压缩失败: {e}\&quot;)\n   184\t            return None\n   185\t    \n   186\t    def compress_frame_fast(self, frame_data):\n   187\t        \&quot;\&quot;\&quot;快速压缩模式 - 使用更激进的压缩参数\&quot;\&quot;\&quot;\n   188\t        try:\n   189\t            # 解码并转换为OpenCV格式\n   190\t            if frame_data.startswith('data:image'):\n   191\t                frame_data = frame_data.split(',')[1]\n   192\t\n   193\t            img_data = base64.b64decode(frame_data)\n   194\t            nparr = np.frombuffer(img_data, np.uint8)\n   195\t            frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n   196\t\n   197\t            if frame is None:\n   198\t                return None\n   199\t\n   200\t            # 获取目标分辨率\n   201\t            scale_parts = self.compression_settings['scale'].split(':')\n   202\t            new_width, new_height = int(scale_parts[0]), int(scale_parts[1])\n   203\t\n   204\t            # 1. 降低分辨率\n   205\t            frame_resized = cv2.resize(frame, (new_width, new_height))\n   206\t\n   207\t            # 2. 可选的图像预处理\n   208\t            if self.compression_settings.get('denoise', False):\n   209\t                frame_resized = cv2.fastNlMeansDenoisingColored(frame_resized)\n   210\t\n   211\t            # 3. 根据CRF值调整JPEG质量\n   212\t            crf = int(self.compression_settings.get('crf', '28'))\n   213\t            jpeg_quality = max(10, min(95, 100 - crf * 2))  # CRF转JPEG质量\n   214\t\n   215\t            # 4. 高压缩编码\n   216\t            encode_params = [cv2.IMWRITE_JPEG_QUALITY, jpeg_quality]\n   217\t            _, buffer = cv2.imencode('.jpg', frame_resized, encode_params)\n   218\t\n   219\t            compressed_data = buffer.tobytes()\n   220\t            original_size = len(img_data)\n   221\t            compressed_size = len(compressed_data)\n   222\t            compression_ratio = (1 - compressed_size / original_size) * 100\n   223\t\n   224\t            return {\n   225\t                'data': base64.b64encode(compressed_data).decode('utf-8'),\n   226\t                'original_size': original_size,\n   227\t                'compressed_size': compressed_size,\n   228\t                'compression_ratio': compression_ratio,\n   229\t                'method': 'opencv_fast'\n   230\t            }\n   231\t\n   232\t        except Exception as e:\n   233\t            print(f\&quot;快速压缩失败: {e}\&quot;)\n   234\t            return None\n   235\t\n   236\t    def compress_frame_webp(self, frame_data):\n   237\t        \&quot;\&quot;\&quot;使用WebP格式进行压缩 - 更好的压缩比\&quot;\&quot;\&quot;\n   238\t        try:\n   239\t            if frame_data.startswith('data:image'):\n   240\t                frame_data = frame_data.split(',')[1]\n   241\t\n   242\t            img_data = base64.b64decode(frame_data)\n   243\t            nparr = np.frombuffer(img_data, np.uint8)\n   244\t            frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n   245\t\n   246\t            if frame is None:\n   247\t                return None\n   248\t\n   249\t            # 获取目标分辨率\n   250\t            scale_parts = self.compression_settings['scale'].split(':')\n   251\t            new_width, new_height = int(scale_parts[0]), int(scale_parts[1])\n   252\t            frame_resized = cv2.resize(frame, (new_width, new_height))\n   253\t\n   254\t            # WebP压缩参数\n   255\t            crf = int(self.compression_settings.get('crf', '28'))\n   256\t            webp_quality = max(10, min(100, 100 - crf))\n   257\t\n   258\t            encode_params = [cv2.IMWRITE_WEBP_QUALITY, webp_quality]\n   259\t            _, buffer = cv2.imencode('.webp', frame_resized, encode_params)\n   260\t\n   261\t            compressed_data = buffer.tobytes()\n   262\t            original_size = len(img_data)\n   263\t            compressed_size = len(compressed_data)\n   264\t            compression_ratio = (1 - compressed_size / original_size) * 100\n   265\t\n   266\t            return {\n   267\t                'data': base64.b64encode(compressed_data).decode('utf-8'),\n   268\t                'original_size': original_size,\n   269\t                'compressed_size': compressed_size,\n   270\t                'compression_ratio': compression_ratio,\n   271\t                'method': 'webp'\n   272\t            }\n   273\t\n   274\t        except Exception as e:\n   275\t            print(f\&quot;WebP压缩失败: {e}\&quot;)\n   276\t            return None\n   277\t\n   278\t    def compress_frame_adaptive(self, frame_data):\n   279\t        \&quot;\&quot;\&quot;自适应压缩 - 根据内容复杂度选择压缩策略\&quot;\&quot;\&quot;\n   280\t        try:\n   281\t            if frame_data.startswith('data:image'):\n   282\t                frame_data = frame_data.split(',')[1]\n   283\t\n   284\t            img_data = base64.b64decode(frame_data)\n   285\t            nparr = np.frombuffer(img_data, np.uint8)\n   286\t            frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n   287\t\n   288\t            if frame is None:\n   289\t                return None\n   290\t\n   291\t            # 分析图像复杂度\n   292\t            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n   293\t            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n   294\t\n   295\t            # 根据复杂度调整压缩参数\n   296\t            if laplacian_var &gt; 1000:  # 高复杂度图像\n   297\t                quality_factor = 0.8\n   298\t            elif laplacian_var &gt; 500:  # 中等复杂度\n   299\t                quality_factor = 0.6\n   300\t            else:  # 低复杂度图像\n   301\t                quality_factor = 0.4\n   302\t\n   303\t            # 获取目标分辨率\n   304\t            scale_parts = self.compression_settings['scale'].split(':')\n   305\t            new_width, new_height = int(scale_parts[0]), int(scale_parts[1])\n   306\t            frame_resized = cv2.resize(frame, (new_width, new_height))\n   307\t\n   308\t            # 自适应质量\n   309\t            base_crf = int(self.compression_settings.get('crf', '28'))\n   310\t            adaptive_quality = max(10, min(95, int((100 - base_crf) * quality_factor)))\n   311\t\n   312\t            encode_params = [cv2.IMWRITE_JPEG_QUALITY, adaptive_quality]\n   313\t            _, buffer = cv2.imencode('.jpg', frame_resized, encode_params)\n   314\t\n   315\t            compressed_data = buffer.tobytes()\n   316\t            original_size = len(img_data)\n   317\t            compressed_size = len(compressed_data)\n   318\t            compression_ratio = (1 - compressed_size / original_size) * 100\n   319\t\n   320\t            return {\n   321\t                'data': base64.b64encode(compressed_data).decode('utf-8'),\n   322\t                'original_size': original_size,\n   323\t                'compressed_size': compressed_size,\n   324\t                'compression_ratio': compression_ratio,\n   325\t                'method': 'adaptive',\n   326\t                'complexity': laplacian_var,\n   327\t                'quality_used': adaptive_quality\n   328\t            }\n   329\t\n   330\t        except Exception as e:\n   331\t            print(f\&quot;自适应压缩失败: {e}\&quot;)\n   332\t            return None\n   333\t    \n   334\t    def _cleanup_files(self, files):\n   335\t        \&quot;\&quot;\&quot;清理临时文件\&quot;\&quot;\&quot;\n   336\t        for file in files:\n   337\t            try:\n   338\t                if os.path.exists(file):\n   339\t                    os.remove(file)\n   340\t            except:\n   341\t                pass\n   342\t    \n   343\t    def update_settings(self, settings):\n   344\t        \&quot;\&quot;\&quot;更新压缩设置\&quot;\&quot;\&quot;\n   345\t        self.compression_settings.update(settings)\n   346\t\n   347\tclass OptimizedFatigueDetectionSystem:\n   348\t    \&quot;\&quot;\&quot;优化的疲劳检测系统 - 集成完整AI检测功能\&quot;\&quot;\&quot;\n   349\t\n   350\t    def __init__(self, model_path: str):\n   351\t        self.model_path = model_path\n   352\t        self.ai_available = AI_MODULES_AVAILABLE\n   353\t\n   354\t        if self.ai_available:\n   355\t            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n   356\t            print(f\&quot; 使用设备: {self.device}\&quot;)\n   357\t\n   358\t            # 加载AI模型\n   359\t            self.model = self._load_model()\n   360\t\n   361\t            # 初始化dlib\n   362\t            self.detector = dlib.get_frontal_face_detector()\n   363\t            # 检查两个可能的dlib模型路径\n   364\t            dlib_paths = [\n   365\t                DLIB_PREDICTOR_PATH,  # config.py中定义的路径\n   366\t                \&quot;output/models/shape_predictor_68_face_landmarks.dat\&quot;  # 备用路径\n   367\t            ]\n   368\t\n   369\t            self.predictor = None\n   370\t            for dlib_path in dlib_paths:\n   371\t                if os.path.exists(dlib_path):\n   372\t                    self.predictor = dlib.shape_predictor(dlib_path)\n   373\t                    print(f\&quot;✅ dlib模型加载成功: {dlib_path}\&quot;)\n   374\t                    break\n   375\t\n   376\t            if self.predictor is None:\n   377\t                print(f\&quot;⚠️  dlib模型文件不存在，检查路径: {dlib_paths}\&quot;)\n   378\t                print(\&quot;系统将在模拟模式下运行\&quot;)\n   379\t                self.ai_available = False\n   380\t        else:\n   381\t            print(\&quot;⚠️  AI模块不可用，系统将在模拟模式下运行\&quot;)\n   382\t            self.device = None\n   383\t            self.model = None\n   384\t            self.detector = None\n   385\t            self.predictor = None\n   386\t\n   387\t        # 检测参数（保持原有逻辑）\n   388\t        class SimpleVar:\n   389\t            def __init__(self, value):\n   390\t                self._value = value\n   391\t            def get(self):\n   392\t                return self._value\n   393\t            def set(self, value):\n   394\t                self._value = value\n   395\t\n   396\t        self.yawn_threshold = SimpleVar(0.6)\n   397\t        self.mar_threshold = SimpleVar(0.5)  # 默认使用平衡模式的MAR阈值\n   398\t        self.ear_threshold = SimpleVar(0.18)  # 统一EAR阈值，适应所有眼型用户\n   399\t        self.alert_cooldown = SimpleVar(5.0)\n   400\t\n   401\t        # 检测状态\n   402\t        self.is_detecting = False\n   403\t        self.current_user = None\n   404\t        self.current_mode = \&quot;平衡模式\&quot;\n   405\t\n   406\t        # 疲劳状态评估相关（与PyQt版本保持一致）\n   407\t        self.recent_yawns = []\n   408\t        self.recent_blinks = []\n   409\t        self.fatigue_window = 30  # 30秒窗口\n   410\t        self.last_fatigue_status = \&quot;正常\&quot;  # 记录上一次的疲劳状态\n   411\t        self.last_blink_time = 0\n   412\t        self.eye_closed_frames = 0\n   413\t        self.eye_closed_threshold = 10\n   414\t        self.long_eye_closed_threshold = 60\n   415\t        self.eye_closed_start_time = None\n   416\t\n   417\t        # 缓冲区（保持原有逻辑）\n   418\t        self.face_buffer = deque(maxlen=SEQUENCE_LENGTH)\n   419\t        self.landmark_buffer = deque(maxlen=SEQUENCE_LENGTH)\n   420\t\n   421\t        # 统计变量（保持原有逻辑）\n   422\t        self.session_start_time = None\n   423\t        self.yawn_count = 0\n   424\t        self.blink_count = 0\n   425\t        self.total_predictions = 0\n   426\t        self.consecutive_yawns = 0\n   427\t        self.consecutive_threshold = 15  # 默认使用平衡模式的连续检测阈值\n   428\t        self.last_yawn_time = 0\n   429\t        self.last_detection_time = 0\n   430\t        self.no_detection_frames = 0\n   431\t        self.decay_rate = 2.0\n   432\t\n   433\t        # 眨眼检测变量（保持原有逻辑）\n   434\t        self.eye_closed_frames = 0\n   435\t        self.eye_closed_threshold = 3\n   436\t        self.long_eye_closed_threshold = 30\n   437\t        self.eye_closed_start_time = None\n   438\t        self.last_blink_time = 0\n   439\t        self.recent_blinks = []\n   440\t        self.recent_yawns = []\n   441\t        self.fatigue_window = 30.0\n   442\t        self.last_fatigue_status = \&quot;正常\&quot;\n   443\t\n   444\t        # 内部状态变量\n   445\t        self._last_mar = 0.0\n   446\t        self._last_ear = 0.3\n   447\t\n   448\t        # 音频系统\n   449\t        self.audio_path = \&quot;static/warning.mp3\&quot;\n   450\t        self.audio_initialized = False\n   451\t        self.warning_sound = None\n   452\t        self._init_audio()\n   453\t\n   454\t        # FFmpeg处理器\n   455\t        self.video_processor = FFmpegVideoProcessor()\n   456\t        self.use_ffmpeg = self.video_processor.check_ffmpeg()\n   457\t\n   458\t        if self.use_ffmpeg:\n   459\t            print(\&quot;✅ FFmpeg可用，将使用FFmpeg进行视频压缩\&quot;)\n   460\t        else:\n   461\t            print(\&quot;⚠️ FFmpeg不可用，将使用快速压缩模式\&quot;)\n   462\t\n   463\t        # 处理队列\n   464\t        self.frame_queue = Queue(maxsize=5)\n   465\t        self.processing_thread = None\n   466\t\n   467\t        # 压缩统计\n   468\t        self.compression_stats = {\n   469\t            'total_original_size': 0,\n   470\t            'total_compressed_size': 0,\n   471\t            'frames_processed': 0,\n   472\t            'avg_compression_ratio': 0\n   473\t        }\n   474\t\n   475\t        # 最新结果\n   476\t        self.latest_results = {\n   477\t            'frame': None,\n   478\t            'face_detected': False,\n   479\t            'yawn_prob': 0.0,\n   480\t            'prediction': 0,\n   481\t            'mar': 0.0,\n   482\t            'ear': 0.0,\n   483\t            'fatigue_status': '正常',\n   484\t            'consecutive_yawns': 0,\n   485\t            'session_time': '00:00',\n   486\t            'buffer_status': '0/30',\n   487\t            'total_predictions': 0,\n   488\t            'yawn_count': 0,\n   489\t            'blink_count': 0,\n   490\t            'progress': 0,\n   491\t            'compression_stats': self.compression_stats.copy()\n   492\t        }\n   493\t\n   494\t    def _load_model(self):\n   495\t        \&quot;\&quot;\&quot;加载AI模型（保持原有逻辑）\&quot;\&quot;\&quot;\n   496\t        if not self.ai_available:\n   497\t            return None\n   498\t\n   499\t        try:\n   500\t            if not os.path.exists(self.model_path):\n   501\t                print(f\&quot;⚠️  模型文件不存在: {self.model_path}\&quot;)\n   502\t                print(\&quot;系统将在模拟模式下运行\&quot;)\n   503\t                return None\n   504\t\n   505\t            model = create_model().to(self.device)\n   506\t            checkpoint = torch.load(self.model_path, map_location=self.device)\n   507\t            model.load_state_dict(checkpoint['model_state_dict'])\n   508\t            model.eval()\n   509\t            print(\&quot;✅ AI模型加载成功\&quot;)\n   510\t            return model\n   511\t        except Exception as e:\n   512\t            print(f\&quot;❌ AI模型加载失败: {e}\&quot;)\n   513\t            print(\&quot;系统将在模拟模式下运行\&quot;)\n   514\t            return None\n   515\t\n   516\t    def _init_audio(self):\n   517\t        \&quot;\&quot;\&quot;初始化音频系统\&quot;\&quot;\&quot;\n   518\t        try:\n   519\t            pygame.mixer.init()\n   520\t            if os.path.exists(self.audio_path):\n   521\t                self.warning_sound = pygame.mixer.Sound(self.audio_path)\n   522\t                self.audio_initialized = True\n   523\t                print(\&quot;✅ 音频系统初始化成功\&quot;)\n   524\t            else:\n   525\t                print(f\&quot;❌ 警告音频文件不存在: {self.audio_path}\&quot;)\n   526\t                self.audio_initialized = False\n   527\t        except Exception as e:\n   528\t            print(f\&quot;❌ 音频系统初始化失败: {e}\&quot;)\n   529\t            self.audio_initialized = False\n   530\t\n   531\t    def _play_warning_sound(self):\n   532\t        \&quot;\&quot;\&quot;播放警告音频\&quot;\&quot;\&quot;\n   533\t        if self.audio_initialized and self.warning_sound:\n   534\t            try:\n   535\t                self.warning_sound.play()\n   536\t                print(\&quot; 播放警告音频\&quot;)\n   537\t            except Exception as e:\n   538\t                print(f\&quot;❌ 播放音频失败: {e}\&quot;)\n   539\t\n   540\t    def _preprocess_frame(self, frame):\n   541\t        \&quot;\&quot;\&quot;预处理帧（保持原有逻辑）\&quot;\&quot;\&quot;\n   542\t        if self.predictor is None:\n   543\t            return None, None, None\n   544\t\n   545\t        face_img, landmarks = extract_face_landmarks(frame, self.detector, self.predictor)\n   546\t\n   547\t        if face_img is None or landmarks is None:\n   548\t            return None, None, None\n   549\t\n   550\t        face_resized = cv2.resize(face_img, FACE_SIZE)\n   551\t        landmarks_norm = normalize_landmarks(landmarks, face_img.shape[:2])\n   552\t\n   553\t        # 获取人脸区域\n   554\t        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n   555\t        faces = self.detector(gray)\n   556\t        face_rect = faces[0] if len(faces) &gt; 0 else None\n   557\t\n   558\t        return face_resized, landmarks_norm, face_rect\n   559\t\n   560\t    def _predict_yawn(self):\n   561\t        \&quot;\&quot;\&quot;预测打哈欠（保持原有逻辑）\&quot;\&quot;\&quot;\n   562\t        if self.model is None or len(self.face_buffer) &lt; SEQUENCE_LENGTH:\n   563\t            return 0.0, 0\n   564\t\n   565\t        faces = np.array(list(self.face_buffer))\n   566\t        landmarks = np.array(list(self.landmark_buffer))\n   567\t\n   568\t        faces_tensor = torch.from_numpy(faces).float().unsqueeze(0)\n   569\t        landmarks_tensor = torch.from_numpy(landmarks).float().unsqueeze(0)\n   570\t\n   571\t        faces_tensor = faces_tensor.permute(0, 1, 4, 2, 3)\n   572\t        landmarks_tensor = landmarks_tensor.reshape(1, SEQUENCE_LENGTH, -1)\n   573\t        faces_tensor = faces_tensor / 255.0\n   574\t\n   575\t        faces_tensor = faces_tensor.to(self.device)\n   576\t        landmarks_tensor = landmarks_tensor.to(self.device)\n   577\t\n   578\t        with torch.no_grad():\n   579\t            outputs = self.model(faces_tensor, landmarks_tensor)\n   580\t            probabilities = torch.softmax(outputs, dim=1)\n   581\t            yawn_prob = probabilities[0, 1].item()\n   582\t            prediction = 1 if yawn_prob &gt; self.yawn_threshold.get() else 0\n   583\t\n   584\t        return yawn_prob, prediction\n   585\t\n   586\t    def _calculate_mouth_aspect_ratio(self, landmarks):\n   587\t        \&quot;\&quot;\&quot;计算嘴部长宽比(MAR)（保持原有逻辑）\&quot;\&quot;\&quot;\n   588\t        try:\n   589\t            mouth_points = landmarks[48:68]\n   590\t            A = np.linalg.norm(mouth_points[13] - mouth_points[19])\n   591\t            B = np.linalg.norm(mouth_points[14] - mouth_points[18])\n   592\t            C = np.linalg.norm(mouth_points[15] - mouth_points[17])\n   593\t            D = np.linalg.norm(mouth_points[0] - mouth_points[6])\n   594\t            mar = (A + B + C) / (3.0 * D)\n   595\t            self._last_mar = mar\n   596\t            return mar\n   597\t        except:\n   598\t            return 0.0\n   599\t\n   600\t    def _calculate_eye_aspect_ratio(self, landmarks):\n   601\t        \&quot;\&quot;\&quot;计算眼部长宽比(EAR)（保持原有逻辑）\&quot;\&quot;\&quot;\n   602\t        try:\n   603\t            left_eye = landmarks[36:42]\n   604\t            right_eye = landmarks[42:48]\n   605\t\n   606\t            def eye_aspect_ratio(eye_points):\n   607\t                A = np.linalg.norm(eye_points[1] - eye_points[5])\n   608\t                B = np.linalg.norm(eye_points[2] - eye_points[4])\n   609\t                C = np.linalg.norm(eye_points[0] - eye_points[3])\n   610\t                ear = (A + B) / (2.0 * C)\n   611\t                return ear\n   612\t\n   613\t            left_ear = eye_aspect_ratio(left_eye)\n   614\t            right_ear = eye_aspect_ratio(right_eye)\n   615\t            avg_ear = (left_ear + right_ear) / 2.0\n   616\t            self._last_ear = avg_ear\n   617\t\n   618\t            return avg_ear\n   619\t        except:\n   620\t            return 0.3\n   621\t\n   622\t    def _detect_blink(self, ear):\n   623\t        \&quot;\&quot;\&quot;检测眨眼和长时间闭眼（使用可配置的EAR阈值）\&quot;\&quot;\&quot;\n   624\t        ear_threshold = self.ear_threshold.get()\n   625\t        current_time = time.time()\n   626\t\n   627\t        if ear &lt; ear_threshold:\n   628\t            if self.eye_closed_frames == 0:\n   629\t                self.eye_closed_start_time = current_time\n   630\t            self.eye_closed_frames += 1\n   631\t        else:\n   632\t            if self.eye_closed_frames &gt;= self.eye_closed_threshold:\n   633\t                if current_time - self.last_blink_time &gt; 0.3:\n   634\t                    self.blink_count += 1\n   635\t                    self.last_blink_time = current_time\n   636\t                    self.recent_blinks.append(current_time)\n   637\t                    return True\n   638\t\n   639\t            self.eye_closed_frames = 0\n   640\t            self.eye_closed_start_time = None\n   641\t\n   642\t        return False\n   643\t\n   644\t    def _evaluate_fatigue_status(self):\n   645\t        \&quot;\&quot;\&quot;评估疲劳状态（与PyQt版本保持一致）\&quot;\&quot;\&quot;\n   646\t        current_time = time.time()\n   647\t\n   648\t        # 清理过期的记录（30秒窗口）\n   649\t        self.recent_yawns = [t for t in self.recent_yawns if current_time - t &lt;= self.fatigue_window]\n   650\t        self.recent_blinks = [t for t in self.recent_blinks if current_time - t &lt;= self.fatigue_window]\n   651\t\n   652\t        yawn_count_30s = len(self.recent_yawns)  # 30秒窗口内的打哈欠次数\n   653\t        long_eye_closed = self.eye_closed_frames &gt;= self.long_eye_closed_threshold\n   654\t\n   655\t        # 疲劳状态判断逻辑（与PyQt版本完全一致）\n   656\t        if yawn_count_30s &gt;= 3 or long_eye_closed:\n   657\t            return \&quot;重度疲劳\&quot;\n   658\t        elif yawn_count_30s &gt;= 2:\n   659\t            return \&quot;中度疲劳\&quot;\n   660\t        elif yawn_count_30s &gt;= 1:\n   661\t            return \&quot;轻度疲劳\&quot;\n   662\t        else:\n   663\t            return \&quot;正常\&quot;\n   664\t\n   665\t    def _draw_face_landmarks(self, frame, face_rect, landmarks_norm):\n   666\t        \&quot;\&quot;\&quot;在人脸上绘制特征点和人脸框（增强显示效果）\&quot;\&quot;\&quot;\n   667\t        if face_rect is None:\n   668\t            print(\&quot;⚠️  face_rect为None，跳过绘制\&quot;)\n   669\t            return frame\n   670\t\n   671\t        # 绘制人脸框 - 使用更粗的线条和更亮的颜色\n   672\t        x, y, w, h = face_rect.left(), face_rect.top(), face_rect.width(), face_rect.height()\n   673\t        print(f\&quot; 开始绘制人脸框: ({x}, {y}, {w}, {h})\&quot;)\n   674\t\n   675\t        # 绘制多层人脸框以确保可见性 - 使用更粗的线条\n   676\t        cv2.rectangle(frame, (x-3, y-3), (x + w + 3, y + h + 3), (0, 255, 0), 6)  # 外层绿框（更粗）\n   677\t        cv2.rectangle(frame, (x-1, y-1), (x + w + 1, y + h + 1), (255, 255, 255), 4)  # 中层白框\n   678\t        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # 内层绿框\n   679\t        print(f\&quot;✅ 人脸框绘制完成\&quot;)\n   680\t\n   681\t        # 如果有归一化的landmarks，需要转换回原始坐标\n   682\t        if landmarks_norm is not None and self.ai_available:\n   683\t            print(f\&quot; 开始绘制特征点，AI可用: {self.ai_available}\&quot;)\n   684\t            # 重新获取原始landmarks来绘制\n   685\t            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n   686\t            faces = self.detector(gray)\n   687\t            if len(faces) &gt; 0:\n   688\t                landmarks = self.predictor(gray, faces[0])\n   689\t                print(f\&quot; 获取到landmarks，开始绘制68个特征点\&quot;)\n   690\t\n   691\t                # 绘制68个特征点 - 使用更大更明显的点\n   692\t                for i in range(68):\n   693\t                    x_point = landmarks.part(i).x\n   694\t                    y_point = landmarks.part(i).y\n   695\t\n   696\t                    # 根据不同区域使用不同颜色和大小\n   697\t                    if i &lt; 17:  # 下巴轮廓\n   698\t                        color = (255, 255, 0)  # 青色\n   699\t                        radius = 3\n   700\t                    elif i &lt; 22:  # 右眉毛\n   701\t                        color = (0, 255, 255)  # 黄色\n   702\t                        radius = 3\n   703\t                    elif i &lt; 27:  # 左眉毛\n   704\t                        color = (0, 255, 255)  # 黄色\n   705\t                        radius = 3\n   706\t                    elif i &lt; 36:  # 鼻子\n   707\t                        color = (255, 0, 255)  # 紫色\n   708\t                        radius = 3\n   709\t                    elif i &lt; 42:  # 右眼\n   710\t                        color = (255, 0, 0)    # 蓝色\n   711\t                        radius = 4  # 眼部特征点稍大\n   712\t                    elif i &lt; 48:  # 左眼\n   713\t                        color = (255, 0, 0)    # 蓝色\n   714\t                        radius = 4  # 眼部特征点稍大\n   715\t                    else:  # 嘴部\n   716\t                        color = (0, 0, 255)    # 红色\n   717\t                        radius = 4  # 嘴部特征点稍大\n   718\t\n   719\t                    # 绘制更大的特征点，带黑色边框增强对比度\n   720\t                    cv2.circle(frame, (x_point, y_point), radius + 2, (0, 0, 0), -1)  # 黑色底\n   721\t                    cv2.circle(frame, (x_point, y_point), radius + 1, (255, 255, 255), -1)  # 白色中层\n   722\t                    cv2.circle(frame, (x_point, y_point), radius, color, -1)  # 彩色点\n   723\t\n   724\t                # 绘制关键区域的连线 - 使用更粗的线条和更好的对比度\n   725\t                # 眼部轮廓\n   726\t                for eye_start, eye_end in [(36, 42), (42, 48)]:\n   727\t                    eye_points = []\n   728\t                    for i in range(eye_start, eye_end):\n   729\t                        eye_points.append((landmarks.part(i).x, landmarks.part(i).y))\n   730\t                    eye_points = np.array(eye_points, np.int32)\n   731\t                    cv2.polylines(frame, [eye_points], True, (0, 0, 0), 5)  # 黑色底线\n   732\t                    cv2.polylines(frame, [eye_points], True, (255, 255, 255), 3)  # 白色中线\n   733\t                    cv2.polylines(frame, [eye_points], True, (255, 0, 0), 2)  # 蓝色线\n   734\t\n   735\t                # 嘴部轮廓\n   736\t                mouth_points = []\n   737\t                for i in range(48, 68):\n   738\t                    mouth_points.append((landmarks.part(i).x, landmarks.part(i).y))\n   739\t                mouth_points = np.array(mouth_points, np.int32)\n   740\t                cv2.polylines(frame, [mouth_points], True, (0, 0, 0), 5)  # 黑色底线\n   741\t                cv2.polylines(frame, [mouth_points], True, (255, 255, 255), 3)  # 白色中线\n   742\t                cv2.polylines(frame, [mouth_points], True, (0, 0, 255), 2)  # 红色线\n   743\t        else:\n   744\t            # 如果AI不可用，只绘制人脸框，不添加文字标识\n   745\t            print(f\&quot;⚠️  AI不可用或landmarks为None，只绘制人脸框\&quot;)\n   746\t\n   747\t        print(f\&quot;✅ 人脸框和特征点绘制完成\&quot;)\n   748\t\n   749\t        return frame\n   750\t\n   751\t    def _record_fatigue_status(self, fatigue_status):\n   752\t        \&quot;\&quot;\&quot;记录疲劳状态到数据库（与PyQt版本保持一致）\&quot;\&quot;\&quot;\n   753\t        # 只在疲劳状态发生变化时记录（与PyQt版本逻辑一致）\n   754\t        if fatigue_status != self.last_fatigue_status:\n   755\t            if fatigue_status == \&quot;轻度疲劳\&quot; and self.last_fatigue_status == \&quot;正常\&quot;:\n   756\t                self._save_fatigue_record(\&quot;轻度疲劳\&quot;)\n   757\t                print(\&quot;⚠️ 轻度疲劳警告\&quot;)\n   758\t            elif fatigue_status == \&quot;中度疲劳\&quot; and self.last_fatigue_status in [\&quot;正常\&quot;, \&quot;轻度疲劳\&quot;]:\n   759\t                self._save_fatigue_record(\&quot;中度疲劳\&quot;)\n   760\t                print(\&quot;⚠️⚠️ 中度疲劳警告\&quot;)\n   761\t            elif fatigue_status == \&quot;重度疲劳\&quot; and self.last_fatigue_status in [\&quot;正常\&quot;, \&quot;轻度疲劳\&quot;, \&quot;中度疲劳\&quot;]:\n   762\t                self._save_fatigue_record(\&quot;重度疲劳\&quot;)\n   763\t                print(\&quot; 重度疲劳警告\&quot;)\n   764\t            elif fatigue_status == \&quot;正常\&quot;:\n   765\t                print(\&quot;✅ 疲劳状态恢复正常\&quot;)\n   766\t\n   767\t            # 播放警告音频（如果状态变化且非正常）\n   768\t            if fatigue_status != \&quot;正常\&quot;:\n   769\t                self._play_warning_sound()\n   770\t\n   771\t            # 更新上一次疲劳状态\n   772\t            self.last_fatigue_status = fatigue_status\n   773\t\n   774\t    def _save_fatigue_record(self, fatigue_level):\n   775\t        \&quot;\&quot;\&quot;保存疲劳记录到数据库（与PyQt版本保持一致）\&quot;\&quot;\&quot;\n   776\t        if not self.current_user:\n   777\t            return\n   778\t\n   779\t        try:\n   780\t            if AI_MODULES_AVAILABLE:\n   781\t                with get_db_connection() as conn:\n   782\t                    cursor = conn.cursor()\n   783\t                    cursor.execute('''\n   784\t                        INSERT INTO fatigue_records\n   785\t                        (username, timestamp, fatigue_level)\n   786\t                        VALUES (%s, %s, %s)\n   787\t                    ''', (\n   788\t                        self.current_user['username'],\n   789\t                        datetime.now(),\n   790\t                        fatigue_level\n   791\t                    ))\n   792\t                    conn.commit()\n   793\t\n   794\t                print(f\&quot; 疲劳记录已保存: {self.current_user['username']} - {fatigue_level}\&quot;)\n   795\t            else:\n   796\t                print(f\&quot; 疲劳记录（模拟）: {self.current_user['username']} - {fatigue_level}\&quot;)\n   797\t\n   798\t        except Exception as e:\n   799\t            print(f\&quot;❌ 保存疲劳记录失败: {e}\&quot;)\n   800\t\n   801\t    def apply_preset(self, mode):\n   802\t        \&quot;\&quot;\&quot;应用预设模式（EAR阈值保持不变）\&quot;\&quot;\&quot;\n   803\t        if mode == 'sensitive':\n   804\t            self.yawn_threshold.set(0.6)  # 保持模型阈值不变\n   805\t            self.mar_threshold.set(0.45)  # MAR阈值调整为0.45\n   806\t            # EAR阈值保持不变，统一为0.18\n   807\t            self.consecutive_threshold = 10  # 连续检测阈值10帧\n   808\t            self.alert_cooldown.set(3.0)\n   809\t            self.current_mode = \&quot;敏感模式\&quot;\n   810\t        elif mode == 'balanced':\n   811\t            self.yawn_threshold.set(0.6)  # 保持模型阈值不变\n   812\t            self.mar_threshold.set(0.5)   # MAR阈值调整为0.5\n   813\t            # EAR阈值保持不变，统一为0.18\n   814\t            self.consecutive_threshold = 15  # 连续检测阈值15帧\n   815\t            self.alert_cooldown.set(5.0)\n   816\t            self.current_mode = \&quot;平衡模式\&quot;\n   817\t        elif mode == 'conservative':\n   818\t            self.yawn_threshold.set(0.6)  # 保持模型阈值不变\n   819\t            self.mar_threshold.set(0.55)  # MAR阈值调整为0.55\n   820\t            # EAR阈值保持不变，统一为0.18\n   821\t            self.consecutive_threshold = 20  # 连续检测阈值20帧\n   822\t            self.alert_cooldown.set(8.0)\n   823\t            self.current_mode = \&quot;保守模式\&quot;\n   824\t\n   825\t    def start_detection(self):\n   826\t        \&quot;\&quot;\&quot;开始检测\&quot;\&quot;\&quot;\n   827\t        if self.is_detecting:\n   828\t            return False\n   829\t\n   830\t        self.is_detecting = True\n   831\t        self.session_start_time = time.time()\n   832\t\n   833\t        # 重置AI检测统计数据\n   834\t        self.yawn_count = 0\n   835\t        self.blink_count = 0\n   836\t        self.total_predictions = 0\n   837\t        self.consecutive_yawns = 0\n   838\t        self.last_yawn_time = 0\n   839\t        self.last_detection_time = 0\n   840\t        self.no_detection_frames = 0\n   841\t\n   842\t        # 重置缓冲区\n   843\t        self.face_buffer.clear()\n   844\t        self.landmark_buffer.clear()\n   845\t\n   846\t        # 重置眨眼检测\n   847\t        self.eye_closed_frames = 0\n   848\t        self.eye_closed_start_time = None\n   849\t        self.last_blink_time = 0\n   850\t        self.recent_blinks = []\n   851\t        self.recent_yawns = []\n   852\t\n   853\t        # 重置压缩统计\n   854\t        self.compression_stats = {\n   855\t            'total_original_size': 0,\n   856\t            'total_compressed_size': 0,\n   857\t            'frames_processed': 0,\n   858\t            'avg_compression_ratio': 0\n   859\t        }\n   860\t\n   861\t        # 启动处理线程\n   862\t        self.processing_thread = threading.Thread(target=self._processing_loop, daemon=True)\n   863\t        self.processing_thread.start()\n   864\t\n   865\t        print(\&quot;✅ 检测已开始 - AI + FFmpeg优化模式\&quot;)\n   866\t        return True\n   867\t\n   868\t    def stop_detection(self):\n   869\t        \&quot;\&quot;\&quot;停止检测\&quot;\&quot;\&quot;\n   870\t        self.is_detecting = False\n   871\t        \n   872\t        # 清空队列\n   873\t        while not self.frame_queue.empty():\n   874\t            try:\n   875\t                self.frame_queue.get_nowait()\n   876\t            except Empty:\n   877\t                break\n   878\t                \n   879\t        print(\&quot;✅ 检测已停止\&quot;)\n   880\t\n   881\t    def add_frame(self, frame_data):\n   882\t        \&quot;\&quot;\&quot;添加帧到处理队列\&quot;\&quot;\&quot;\n   883\t        if not self.is_detecting:\n   884\t            return False\n   885\t            \n   886\t        try:\n   887\t            if self.frame_queue.full():\n   888\t                try:\n   889\t                    self.frame_queue.get_nowait()  # 移除最老的帧\n   890\t                except Empty:\n   891\t                    pass\n   892\t            \n   893\t            self.frame_queue.put_nowait(frame_data)\n   894\t            return True\n   895\t        except:\n   896\t            return False\n   897\t\n   898\t    def _processing_loop(self):\n   899\t        \&quot;\&quot;\&quot;处理循环 - 集成完整AI检测\&quot;\&quot;\&quot;\n   900\t        compression_methods = ['fast', 'webp', 'adaptive']\n   901\t        current_method_index = 0\n   902\t\n   903\t        while self.is_detecting:\n   904\t            try:\n   905\t                # 获取帧数据\n   906\t                frame_data = None\n   907\t                try:\n   908\t                    frame_data = self.frame_queue.get(timeout=0.5)\n   909\t                except Empty:\n   910\t                    continue\n   911\t\n   912\t                # 执行完整的AI检测流程\n   913\t                detection_result = self.process_frame_with_ai(frame_data)\n   914\t\n   915\t                if detection_result:\n   916\t                    # 更新最新结果\n   917\t                    self.latest_results.update(detection_result)\n   918\t                    self.latest_results['compression_stats'] = self.compression_stats.copy()\n   919\t\n   920\t                    # 动态调整压缩方法（基于性能）\n   921\t                    if self.compression_stats['frames_processed'] % 50 == 0:\n   922\t                        self._optimize_compression_method()\n   923\t\n   924\t            except Exception as e:\n   925\t                print(f\&quot;处理循环错误: {e}\&quot;)\n   926\t                time.sleep(0.1)\n   927\t\n   928\t    def process_frame_with_ai(self, frame_data):\n   929\t        \&quot;\&quot;\&quot;处理视频帧并进行AI检测（完整检测逻辑）\&quot;\&quot;\&quot;\n   930\t        try:\n   931\t            # 将base64数据转换为OpenCV图像\n   932\t            if frame_data.startswith('data:image'):\n   933\t                frame_data = frame_data.split(',')[1]\n   934\t\n   935\t            # 解码base64数据\n   936\t            img_data = base64.b64decode(frame_data)\n   937\t            nparr = np.frombuffer(img_data, np.uint8)\n   938\t            frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n   939\t\n   940\t            if frame is None:\n   941\t                return None\n   942\t\n   943\t            # 先进行FFmpeg压缩\n   944\t            compressed_result = self.video_processor.compress_frame_fast(frame_data)\n   945\t            if compressed_result:\n   946\t                self._update_compression_stats(compressed_result)\n   947\t\n   948\t            # 如果AI不可用，使用模拟检测\n   949\t            if not self.ai_available:\n   950\t                return self._simulate_detection(frame)\n   951\t\n   952\t            # 执行AI检测逻辑（保持原有逻辑）\n   953\t            face_img, landmarks_norm, face_rect = self._preprocess_frame(frame)\n   954\t            face_detected = face_img is not None\n   955\t\n   956\t            # 获取原始landmarks用于MAR/EAR计算\n   957\t            original_landmarks = None\n   958\t            if face_detected:\n   959\t                original_face_img, original_landmarks = extract_face_landmarks(frame, self.detector, self.predictor)\n   960\t                # 立即绘制人脸框和特征点\n   961\t                print(f\&quot; 绘制人脸框和特征点，人脸区域: {face_rect.left()}, {face_rect.top()}, {face_rect.width()}, {face_rect.height()}\&quot;)\n   962\t                frame = self._draw_face_landmarks(frame, face_rect, landmarks_norm)\n   963\t                print(f\&quot;✅ 人脸框和特征点绘制完成\&quot;)\n   964\t\n   965\t            yawn_prob = 0.0\n   966\t            prediction = 0\n   967\t\n   968\t            if face_detected:\n   969\t                self.face_buffer.append(face_img)\n   970\t                self.landmark_buffer.append(landmarks_norm)\n   971\t\n   972\t                # 如果缓冲区满了，进行预测（保持原有逻辑）\n   973\t                if len(self.face_buffer) &gt;= SEQUENCE_LENGTH:\n   974\t                    yawn_prob, model_prediction = self._predict_yawn()\n   975\t                    self.total_predictions += 1\n   976\t\n   977\t                    # 计算当前帧的嘴部长宽比和眼部长宽比（使用原始landmarks）\n   978\t                    current_mar = self._calculate_mouth_aspect_ratio(original_landmarks)\n   979\t                    current_ear = self._calculate_eye_aspect_ratio(original_landmarks)\n   980\t\n   981\t                    # 检测眨眼（保持原有逻辑）\n   982\t                    blink_detected = self._detect_blink(current_ear)\n   983\t\n   984\t                    # 新的检测逻辑：模型预测 + MAR阈值的组合判断（保持原有逻辑）\n   985\t                    model_says_yawn = yawn_prob &gt; self.yawn_threshold.get()\n   986\t                    mar_says_yawn = current_mar &gt; self.mar_threshold.get()\n   987\t\n   988\t                    # 最终判断：两个条件都满足才认为是打哈欠（保持原有逻辑）\n   989\t                    final_prediction = 1 if (model_says_yawn and mar_says_yawn) else 0\n   990\t\n   991\t                    # 更新连续检测计数 - 使用平滑衰减机制（保持原有逻辑）\n   992\t                    current_time = time.time()\n   993\t                    if final_prediction == 1:\n   994\t                        # 检测到打哈欠：增加计数，更新最后检测时间\n   995\t                        self.consecutive_yawns += 1\n   996\t                        self.last_detection_time = current_time\n   997\t                        self.no_detection_frames = 0  # 重置未检测帧数\n   998\t                        print(f\&quot; 打哈欠检测: 模型={yawn_prob:.3f}({'✓' if model_says_yawn else '✗'}), MAR={current_mar:.3f}({'✓' if mar_says_yawn else '✗'}), 连续={self.consecutive_yawns}\&quot;)\n   999\t                    else:\n  1000\t                        # 未检测到打哈欠：使用平滑衰减\n  1001\t                        self.no_detection_frames += 1\n  1002\t\n  1003\t                        # 如果有之前的检测记录，则开始衰减\n  1004\t                        if self.consecutive_yawns &gt; 0:\n  1005\t                            # 计算衰减量：基于时间的衰减\n  1006\t                            if self.last_detection_time &gt; 0:\n  1007\t                                time_since_last = current_time - self.last_detection_time\n  1008\t                                # 每秒衰减decay_rate帧，但至少保持1秒不衰减\n  1009\t                                if time_since_last &gt; 1.0:  # 1秒后开始衰减\n  1010\t                                    decay_amount = int((time_since_last - 1.0) * self.decay_rate)\n  1011\t                                    self.consecutive_yawns = max(0, self.consecutive_yawns - decay_amount)\n  1012\t\n  1013\t                                    if self.consecutive_yawns == 0:\n  1014\t                                        print(f\&quot; 进度条衰减至零（未检测{self.no_detection_frames}帧，时间间隔{time_since_last:.1f}秒）\&quot;)\n  1015\t                                    else:\n  1016\t                                        print(f\&quot; 进度条衰减: {self.consecutive_yawns}（未检测{self.no_detection_frames}帧）\&quot;)\n  1017\t                            else:\n  1018\t                                # 如果没有时间记录，立即开始衰减\n  1019\t                                if self.no_detection_frames &gt; 30:  # 30帧后开始衰减（约1秒）\n  1020\t                                    self.consecutive_yawns = max(0, self.consecutive_yawns - 1)\n  1021\t                        else:\n  1022\t                            # 如果consecutive_yawns已经是0，保持为0\n  1023\t                            self.consecutive_yawns = 0\n  1024\t\n  1025\t                    # 检查是否触发警报（保持原有逻辑）\n  1026\t                    if (self.consecutive_yawns &gt;= self.consecutive_threshold and\n  1027\t                        (current_time - self.last_yawn_time) &gt; self.alert_cooldown.get()):\n  1028\t                        self.yawn_count += 1\n  1029\t                        self.last_yawn_time = current_time\n  1030\t                        self.recent_yawns.append(current_time)\n  1031\t                        print(f\&quot; 触发警报！连续{self.consecutive_yawns}帧检测到打哈欠\&quot;)\n  1032\t                        self._play_warning_sound()\n  1033\t\n  1034\t                    # 更新prediction变量用于GUI显示\n  1035\t                    prediction = final_prediction\n  1036\t\n  1037\t                # 人脸框和特征点已在前面绘制\n  1038\t            else:\n  1039\t                # 未检测到人脸时的衰减逻辑（保持原有逻辑）\n  1040\t                if self.consecutive_yawns &gt; 0:\n  1041\t                    current_time = time.time()\n  1042\t                    self.no_detection_frames += 1\n  1043\t\n  1044\t                    # 如果有之前的检测记录，则开始衰减\n  1045\t                    if self.last_detection_time &gt; 0:\n  1046\t                        time_since_last = current_time - self.last_detection_time\n  1047\t                        # 未检测到人脸时，衰减更快一些\n  1048\t                        if time_since_last &gt; 0.5:  # 0.5秒后开始衰减\n  1049\t                            decay_amount = int((time_since_last - 0.5) * self.decay_rate * 1.5)  # 衰减速度1.5倍\n  1050\t                            old_consecutive = self.consecutive_yawns\n  1051\t                            self.consecutive_yawns = max(0, self.consecutive_yawns - decay_amount)\n  1052\t\n  1053\t                            if old_consecutive != self.consecutive_yawns:\n  1054\t                                if self.consecutive_yawns == 0:\n  1055\t                                    print(f\&quot; 未检测到人脸，进度条衰减至零（未检测{self.no_detection_frames}帧）\&quot;)\n  1056\t                                else:\n  1057\t                                    print(f\&quot; 未检测到人脸，进度条衰减: {self.consecutive_yawns}\&quot;)\n  1058\t                    else:\n  1059\t                        # 如果没有时间记录，较快衰减\n  1060\t                        if self.no_detection_frames &gt; 15:  # 15帧后开始衰减（约0.5秒）\n  1061\t                            self.consecutive_yawns = max(0, self.consecutive_yawns - 1)\n  1062\t\n  1063\t            # 评估疲劳状态\n  1064\t            fatigue_status = self._evaluate_fatigue_status()\n  1065\t\n  1066\t            # 记录疲劳状态到数据库\n  1067\t            self._record_fatigue_status(fatigue_status)\n  1068\t\n  1069\t            # 更新最新结果用于Web显示\n  1070\t            return self._update_latest_results(frame, face_detected, yawn_prob, prediction, fatigue_status)\n  1071\t\n  1072\t        except Exception as e:\n  1073\t            print(f\&quot;❌ AI检测处理错误: {e}\&quot;)\n  1074\t            return None\n  1075\t\n  1076\t    def _update_latest_results(self, frame, face_detected, yawn_prob, prediction, fatigue_status):\n  1077\t        \&quot;\&quot;\&quot;更新最新的检测结果用于Web显示\&quot;\&quot;\&quot;\n  1078\t        # 注意：frame已经包含了人脸框和特征点（在_draw_face_landmarks中绘制）\n  1079\t        # 不再添加额外的文本信息，只保留人脸框和特征点\n  1080\t\n  1081\t        # 将frame转换为base64用于Web显示，使用更高质量\n  1082\t        _, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 90])\n  1083\t        frame_base64 = base64.b64encode(buffer).decode('utf-8')\n  1084\t\n  1085\t        # 计算会话时间\n  1086\t        session_time = \&quot;00:00\&quot;\n  1087\t        if self.session_start_time:\n  1088\t            elapsed = int(time.time() - self.session_start_time)\n  1089\t            minutes = elapsed // 60\n  1090\t            seconds = elapsed % 60\n  1091\t            session_time = f\&quot;{minutes:02d}:{seconds:02d}\&quot;\n  1092\t\n  1093\t        return {\n  1094\t            'frame': frame_base64,\n  1095\t            'face_detected': face_detected,\n  1096\t            'yawn_prob': round(yawn_prob, 3),\n  1097\t            'prediction': prediction,\n  1098\t            'mar': round(self._last_mar, 3),\n  1099\t            'ear': round(self._last_ear, 3),\n  1100\t            'fatigue_status': fatigue_status,\n  1101\t            'consecutive_yawns': self.consecutive_yawns,\n  1102\t            'session_time': session_time,\n  1103\t            'buffer_status': f\&quot;{min(SEQUENCE_LENGTH, len(self.face_buffer))}/{SEQUENCE_LENGTH}\&quot;,\n  1104\t            'total_predictions': self.total_predictions,\n  1105\t            'yawn_count': self.yawn_count,\n  1106\t            'blink_count': self.blink_count,\n  1107\t            'progress': min(100, int((self.consecutive_yawns / self.consecutive_threshold) * 100))\n  1108\t        }\n  1109\t\n  1110\t    def _simulate_detection(self, frame):\n  1111\t        \&quot;\&quot;\&quot;模拟AI检测（当AI模块不可用时）\&quot;\&quot;\&quot;\n  1112\t        import random\n  1113\t\n  1114\t        # 模拟检测结果\n  1115\t        face_detected = random.choice([True, False, True, True])  # 75%概率检测到人脸\n  1116\t        yawn_prob = random.uniform(0.0, 1.0)\n  1117\t        prediction = 1 if yawn_prob &gt; 0.7 else 0\n  1118\t\n  1119\t        # 更新统计\n  1120\t        self.total_predictions += 1\n  1121\t        if prediction == 1:\n  1122\t            self.yawn_count += 1\n  1123\t\n  1124\t        # 模拟眨眼检测\n  1125\t        if random.random() &lt; 0.1:  # 10%概率检测到眨眼\n  1126\t            self.blink_count += 1\n  1127\t\n  1128\t        # 模拟疲劳状态\n  1129\t        if self.yawn_count &gt;= 3:\n  1130\t            fatigue_status = \&quot;中度疲劳\&quot;\n  1131\t        elif self.yawn_count &gt;= 1:\n  1132\t            fatigue_status = \&quot;轻度疲劳\&quot;\n  1133\t        else:\n  1134\t            fatigue_status = \&quot;正常\&quot;\n  1135\t\n  1136\t        # 在frame上绘制模拟检测框和特征点\n  1137\t        if face_detected:\n  1138\t            h, w = frame.shape[:2]\n  1139\t            # 绘制人脸框\n  1140\t            face_x, face_y = w//4, h//4\n  1141\t            face_w, face_h = w//2, h//2\n  1142\t            cv2.rectangle(frame, (face_x, face_y), (face_x + face_w, face_y + face_h), (0, 255, 0), 3)\n  1143\t\n  1144\t            # 绘制模拟特征点\n  1145\t            # 眼部区域\n  1146\t            eye_y = face_y + face_h//3\n  1147\t            left_eye_x = face_x + face_w//4\n  1148\t            right_eye_x = face_x + 3*face_w//4\n  1149\t\n  1150\t            # 左眼\n  1151\t            for i in range(6):\n  1152\t                angle = i * 60 * np.pi / 180\n  1153\t                x = int(left_eye_x + 15 * np.cos(angle))\n  1154\t                y = int(eye_y + 8 * np.sin(angle))\n  1155\t                cv2.circle(frame, (x, y), 2, (255, 0, 0), -1)\n  1156\t\n  1157\t            # 右眼\n  1158\t            for i in range(6):\n  1159\t                angle = i * 60 * np.pi / 180\n  1160\t                x = int(right_eye_x + 15 * np.cos(angle))\n  1161\t                y = int(eye_y + 8 * np.sin(angle))\n  1162\t                cv2.circle(frame, (x, y), 2, (255, 0, 0), -1)\n  1163\t\n  1164\t            # 嘴部区域\n  1165\t            mouth_y = face_y + 2*face_h//3\n  1166\t            mouth_x = face_x + face_w//2\n  1167\t\n  1168\t            # 嘴部轮廓\n  1169\t            for i in range(8):\n  1170\t                angle = i * 45 * np.pi / 180\n  1171\t                x = int(mouth_x + 20 * np.cos(angle))\n  1172\t                y = int(mouth_y + 10 * np.sin(angle))\n  1173\t                cv2.circle(frame, (x, y), 2, (0, 0, 255), -1)\n  1174\t\n  1175\t            # 鼻子\n  1176\t            nose_y = face_y + face_h//2\n  1177\t            nose_x = face_x + face_w//2\n  1178\t            cv2.circle(frame, (nose_x, nose_y), 3, (255, 0, 255), -1)\n  1179\t            cv2.circle(frame, (nose_x-5, nose_y+5), 2, (255, 0, 255), -1)\n  1180\t            cv2.circle(frame, (nose_x+5, nose_y+5), 2, (255, 0, 255), -1)\n  1181\t\n  1182\t        # 记录疲劳状态到数据库\n  1183\t        self._record_fatigue_status(fatigue_status)\n  1184\t\n  1185\t        return self._update_latest_results(frame, face_detected, yawn_prob, prediction, fatigue_status)\n  1186\t\n  1187\t    def _optimize_compression_method(self):\n  1188\t        \&quot;\&quot;\&quot;根据性能动态优化压缩方法\&quot;\&quot;\&quot;\n  1189\t        try:\n  1190\t            avg_ratio = self.compression_stats['avg_compression_ratio']\n  1191\t\n  1192\t            # 如果压缩比不够好，切换到更激进的压缩\n  1193\t            if avg_ratio &lt; 70:  # 压缩比低于70%\n  1194\t                new_crf = min(35, int(self.video_processor.compression_settings['crf']) + 2)\n  1195\t                self.video_processor.compression_settings['crf'] = str(new_crf)\n  1196\t                print(f\&quot; 自动优化: 提高压缩比，CRF调整为 {new_crf}\&quot;)\n  1197\t\n  1198\t            # 如果压缩比太高可能影响质量，适当降低\n  1199\t            elif avg_ratio &gt; 90:  # 压缩比高于90%\n  1200\t                new_crf = max(18, int(self.video_processor.compression_settings['crf']) - 1)\n  1201\t                self.video_processor.compression_settings['crf'] = str(new_crf)\n  1202\t                print(f\&quot; 自动优化: 保持质量，CRF调整为 {new_crf}\&quot;)\n  1203\t\n  1204\t        except Exception as e:\n  1205\t            print(f\&quot;优化压缩方法失败: {e}\&quot;)\n  1206\t\n  1207\t    def _update_compression_stats(self, result):\n  1208\t        \&quot;\&quot;\&quot;更新压缩统计\&quot;\&quot;\&quot;\n  1209\t        self.compression_stats['total_original_size'] += result['original_size']\n  1210\t        self.compression_stats['total_compressed_size'] += result['compressed_size']\n  1211\t        self.compression_stats['frames_processed'] += 1\n  1212\t        \n  1213\t        if self.compression_stats['total_original_size'] &gt; 0:\n  1214\t            self.compression_stats['avg_compression_ratio'] = (\n  1215\t                (1 - self.compression_stats['total_compressed_size'] / \n  1216\t                 self.compression_stats['total_original_size']) * 100\n  1217\t            )\n  1218\t\n  1219\t\n  1220\t\n  1221\t    def get_latest_results(self):\n  1222\t        \&quot;\&quot;\&quot;获取最新结果\&quot;\&quot;\&quot;\n  1223\t        return self.latest_results.copy()\n  1224\t\n  1225\t    def update_compression_settings(self, settings):\n  1226\t        \&quot;\&quot;\&quot;更新压缩设置\&quot;\&quot;\&quot;\n  1227\t        self.video_processor.update_settings(settings)\n  1228\t\n  1229\t# 全局检测系统实例\n  1230\tdetection_system = OptimizedFatigueDetectionSystem(MODEL_PATH)\n  1231\t\n  1232\t# 在应用启动时初始化数据库\n  1233\t@app.on_event(\&quot;startup\&quot;)\n  1234\tasync def startup_event():\n  1235\t    \&quot;\&quot;\&quot;应用启动事件\&quot;\&quot;\&quot;\n  1236\t    if AI_MODULES_AVAILABLE:\n  1237\t        try:\n  1238\t            init_database()\n  1239\t            print(\&quot;✅ 数据库初始化成功\&quot;)\n  1240\t        except Exception as e:\n  1241\t            print(f\&quot;⚠️  数据库初始化失败: {e}\&quot;)\n  1242\t    else:\n  1243\t        print(\&quot;⚠️  跳过数据库初始化（AI模块不可用）\&quot;)\n  1244\t\n  1245\t# 路由定义\n  1246\t@app.get(\&quot;/\&quot;, response_class=HTMLResponse)\n  1247\tasync def login_page(request: Request):\n  1248\t    return templates.TemplateResponse(\&quot;login.html\&quot;, {\&quot;request\&quot;: request})\n  1249\t\n  1250\t@app.post(\&quot;/login\&quot;)\n  1251\tasync def login(username: str = Form(...), password: str = Form(...)):\n  1252\t    \&quot;\&quot;\&quot;用户登录 - 使用数据库认证\&quot;\&quot;\&quot;\n  1253\t    try:\n  1254\t        if AI_MODULES_AVAILABLE:\n  1255\t            # 使用数据库认证\n  1256\t            with get_db_connection() as conn:\n  1257\t                cursor = conn.cursor()\n  1258\t                cursor.execute(\&quot;SELECT username, password FROM users WHERE username = %s\&quot;, (username,))\n  1259\t                user = cursor.fetchone()\n  1260\t\n  1261\t            if user and user[1] == password:\n  1262\t                detection_system.current_user = {\n  1263\t                    'username': user[0],\n  1264\t                    'full_name': user[0]\n  1265\t                }\n  1266\t                return RedirectResponse(url=\&quot;/dashboard\&quot;, status_code=302)\n  1267\t            else:\n  1268\t                return JSONResponse({\&quot;success\&quot;: False, \&quot;message\&quot;: \&quot;用户名或密码错误\&quot;})\n  1269\t        else:\n  1270\t            # 如果数据库不可用，使用简化认证\n  1271\t            simple_users = {\&quot;test\&quot;: \&quot;123456\&quot;, \&quot;admin\&quot;: \&quot;admin\&quot;}\n  1272\t            if username in simple_users and simple_users[username] == password:\n  1273\t                detection_system.current_user = {\n  1274\t                    'username': username,\n  1275\t                    'full_name': username\n  1276\t                }\n  1277\t                return RedirectResponse(url=\&quot;/dashboard\&quot;, status_code=302)\n  1278\t            else:\n  1279\t                return JSONResponse({\&quot;success\&quot;: False, \&quot;message\&quot;: \&quot;用户名或密码错误\&quot;})\n  1280\t\n  1281\t    except Exception as e:\n  1282\t        print(f\&quot;❌ 登录失败: {e}\&quot;)\n  1283\t        return JSONResponse({\&quot;success\&quot;: False, \&quot;message\&quot;: f\&quot;登录失败: {e}\&quot;})\n  1284\t\n  1285\t@app.get(\&quot;/register\&quot;, response_class=HTMLResponse)\n  1286\tasync def register_page(request: Request):\n  1287\t    \&quot;\&quot;\&quot;注册页面\&quot;\&quot;\&quot;\n  1288\t    return templates.TemplateResponse(\&quot;register.html\&quot;, {\&quot;request\&quot;: request})\n  1289\t\n  1290\t@app.post(\&quot;/register\&quot;)\n  1291\tasync def register(username: str = Form(...), password: str = Form(...), confirm_password: str = Form(...)):\n  1292\t    \&quot;\&quot;\&quot;用户注册 - 使用数据库存储\&quot;\&quot;\&quot;\n  1293\t    if len(password) &lt; 6:\n  1294\t        return JSONResponse({\&quot;success\&quot;: False, \&quot;message\&quot;: \&quot;密码长度至少6位\&quot;})\n  1295\t\n  1296\t    if password != confirm_password:\n  1297\t        return JSONResponse({\&quot;success\&quot;: False, \&quot;message\&quot;: \&quot;两次输入的密码不一致\&quot;})\n  1298\t\n  1299\t    try:\n  1300\t        if AI_MODULES_AVAILABLE:\n  1301\t            # 使用数据库存储\n  1302\t            with get_db_connection() as conn:\n  1303\t                cursor = conn.cursor()\n  1304\t                cursor.execute(\&quot;INSERT INTO users (username, password) VALUES (%s, %s)\&quot;, (username, password))\n  1305\t                conn.commit()\n  1306\t\n  1307\t            return JSONResponse({\&quot;success\&quot;: True, \&quot;message\&quot;: f\&quot;用户 {username} 注册成功！\&quot;})\n  1308\t        else:\n  1309\t            # 如果数据库不可用，返回提示\n  1310\t            return JSONResponse({\&quot;success\&quot;: False, \&quot;message\&quot;: \&quot;数据库不可用，无法注册新用户\&quot;})\n  1311\t\n  1312\t    except Exception as e:\n  1313\t        if \&quot;Duplicate entry\&quot; in str(e):\n  1314\t            return JSONResponse({\&quot;success\&quot;: False, \&quot;message\&quot;: \&quot;用户名已存在，请选择其他用户名\&quot;})\n  1315\t        else:\n  1316\t            return JSONResponse({\&quot;success\&quot;: False, \&quot;message\&quot;: f\&quot;注册失败: {e}\&quot;})\n  1317\t\n  1318\t@app.get(\&quot;/dashboard\&quot;, response_class=HTMLResponse)\n  1319\tasync def dashboard(request: Request):\n  1320\t    if not detection_system.current_user:\n  1321\t        return RedirectResponse(url=\&quot;/\&quot;, status_code=302)\n  1322\t    return templates.TemplateResponse(\&quot;dashboard_backend_ffmpeg.html\&quot;, {\n  1323\t        \&quot;request\&quot;: request,\n  1324\t        \&quot;user\&quot;: detection_system.current_user\n  1325\t    })\n  1326\t\n  1327\t@app.post(\&quot;/api/start_detection\&quot;)\n  1328\tasync def start_detection():\n  1329\t    if detection_system.start_detection():\n  1330\t        return JSONResponse({\&quot;success\&quot;: True, \&quot;message\&quot;: \&quot;检测已开始\&quot;})\n  1331\t    else:\n  1332\t        return JSONResponse({\&quot;success\&quot;: False, \&quot;message\&quot;: \&quot;启动检测失败\&quot;})\n  1333\t\n  1334\t@app.post(\&quot;/api/stop_detection\&quot;)\n  1335\tasync def stop_detection():\n  1336\t    detection_system.stop_detection()\n  1337\t    return JSONResponse({\&quot;success\&quot;: True, \&quot;message\&quot;: \&quot;检测已停止\&quot;})\n  1338\t\n  1339\t@app.post(\&quot;/api/update_compression\&quot;)\n  1340\tasync def update_compression(\n  1341\t    preset: str = Form(...),\n  1342\t    crf: str = Form(...),\n  1343\t    scale: str = Form(...),\n  1344\t    fps: str = Form(...)\n  1345\t):\n  1346\t    \&quot;\&quot;\&quot;更新压缩设置\&quot;\&quot;\&quot;\n  1347\t    settings = {\n  1348\t        'preset': preset,\n  1349\t        'crf': crf,\n  1350\t        'scale': scale,\n  1351\t        'fps': fps\n  1352\t    }\n  1353\t    detection_system.update_compression_settings(settings)\n  1354\t    return JSONResponse({\&quot;success\&quot;: True, \&quot;message\&quot;: \&quot;压缩设置已更新\&quot;})\n  1355\t\n  1356\t@app.get(\&quot;/api/compression_stats\&quot;)\n  1357\tasync def get_compression_stats():\n  1358\t    \&quot;\&quot;\&quot;获取压缩统计\&quot;\&quot;\&quot;\n  1359\t    return JSONResponse(detection_system.compression_stats)\n  1360\t\n  1361\t@app.post(\&quot;/api/reset_stats\&quot;)\n  1362\tasync def reset_stats():\n  1363\t    \&quot;\&quot;\&quot;重置统计数据\&quot;\&quot;\&quot;\n  1364\t    detection_system.compression_stats = {\n  1365\t        'total_original_size': 0,\n  1366\t        'total_compressed_size': 0,\n  1367\t        'frames_processed': 0,\n  1368\t        'avg_compression_ratio': 0\n  1369\t    }\n  1370\t    return JSONResponse({\&quot;success\&quot;: True, \&quot;message\&quot;: \&quot;统计数据已重置\&quot;})\n  1371\t\n  1372\t@app.get(\&quot;/api/system_info\&quot;)\n  1373\tasync def get_system_info():\n  1374\t    \&quot;\&quot;\&quot;获取系统信息\&quot;\&quot;\&quot;\n  1375\t    import psutil\n  1376\t    import platform\n  1377\t\n  1378\t    try:\n  1379\t        cpu_percent = psutil.cpu_percent(interval=1)\n  1380\t        memory = psutil.virtual_memory()\n  1381\t        disk = psutil.disk_usage(&lt;response clipped&gt;&lt;NOTE&gt;To save on context only part of this file has been shown to you.&lt;/NOTE&gt;\nTotal lines in file: 1595\n&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;0a0fbeac-6293-44cd-85a0-fdaa870478c4;toolu_01Rgv7RZZiWHvGPAvuL9C4wG&quot;:{&quot;requestId&quot;:&quot;0a0fbeac-6293-44cd-85a0-fdaa870478c4&quot;,&quot;toolUseId&quot;:&quot;toolu_01Rgv7RZZiWHvGPAvuL9C4wG&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;Here's the result of running `cat -n` on config.py:\n     1\t\&quot;\&quot;\&quot;\n     2\t配置文件 - 打哈欠检测项目\n     3\t\&quot;\&quot;\&quot;\n     4\timport os\n     5\t\n     6\t# 数据路径配置\n     7\tDATASET_ROOT = \&quot;dataset\&quot;\n     8\tDASH_PATH = os.path.join(DATASET_ROOT, \&quot;Dash\&quot;)\n     9\tMIRROR_PATH = os.path.join(DATASET_ROOT, \&quot;Mirror\&quot;)\n    10\t\n    11\t# dlib模型路径\n    12\tDLIB_PREDICTOR_PATH = \&quot;output/models/shape_predictor_68_face_landmarks.dat\&quot;\n    13\t\n    14\t# 数据预处理参数\n    15\tFACE_SIZE = (128, 128)  # 人脸图像resize尺寸\n    16\tSEQUENCE_LENGTH = 30    # LSTM输入序列长度（帧数）\n    17\tOVERLAP_RATIO = 0.5     # 滑动窗口重叠比例\n    18\t\n    19\t# 模型参数\n    20\tCNN_CHANNELS = [32, 64, 128]  # CNN各层通道数\n    21\tLSTM_HIDDEN_SIZE = 256        # LSTM隐藏层大小\n    22\tLSTM_LAYERS = 2               # LSTM层数\n    23\tDROPOUT_RATE = 0.3            # Dropout比例\n    24\tNUM_CLASSES = 2               # 分类数：0-正常/说话，1-打哈欠\n    25\t\n    26\t# 训练参数\n    27\tBATCH_SIZE = 8  # 减小批次大小以节省内存\n    28\tLEARNING_RATE = 0.001\n    29\tNUM_EPOCHS = 100\n    30\tTRAIN_SPLIT = 0.8\n    31\tVAL_SPLIT = 0.1\n    32\tTEST_SPLIT = 0.1\n    33\t\n    34\t# 调试模式参数（已关闭，进行完整训练）\n    35\tDEBUG_MODE = False  # 设置为False进行完整训练\n    36\tDEBUG_MAX_VIDEOS = 3  # 调试模式下每个文件夹最多处理的视频数\n    37\tDEBUG_MAX_FRAMES = 600  # 调试模式下每个视频最多处理的帧数\n    38\tDEBUG_EPOCHS = 5  # 调试模式下的训练轮数\n    39\tDEBUG_BATCH_SIZE = 4  # 调试模式下的批次大小\n    40\t\n    41\t# 数据增强参数\n    42\tAUGMENTATION = {\n    43\t    'brightness_range': 0.2,\n    44\t    'contrast_range': 0.2,\n    45\t    'rotation_range': 10,\n    46\t    'horizontal_flip': True\n    47\t}\n    48\t\n    49\t# 输出路径\n    50\tOUTPUT_DIR = \&quot;output\&quot;\n    51\tMODEL_SAVE_PATH = os.path.join(OUTPUT_DIR, \&quot;models\&quot;)\n    52\tLOG_PATH = os.path.join(OUTPUT_DIR, \&quot;logs\&quot;)\n    53\tPROCESSED_DATA_PATH = os.path.join(OUTPUT_DIR, \&quot;processed_data\&quot;)\n    54\t\n    55\t# 创建输出目录\n    56\tos.makedirs(OUTPUT_DIR, exist_ok=True)\n    57\tos.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n    58\tos.makedirs(LOG_PATH, exist_ok=True)\n    59\tos.makedirs(PROCESSED_DATA_PATH, exist_ok=True)\n    60\t\nTotal lines in file: 60\n&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;4b01cf87-4efb-4295-a588-cb705977b8ce;toolu_01P1PkvfXtuvEdR77tNHUgYe&quot;:{&quot;requestId&quot;:&quot;4b01cf87-4efb-4295-a588-cb705977b8ce&quot;,&quot;toolUseId&quot;:&quot;toolu_01P1PkvfXtuvEdR77tNHUgYe&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;Here's the result of running `cat -n` on model.py:\n     1\t\&quot;\&quot;\&quot;\n     2\tCNN+LSTM模型架构 - 打哈欠检测\n     3\t\&quot;\&quot;\&quot;\n     4\timport torch\n     5\timport torch.nn as nn\n     6\timport torch.nn.functional as F\n     7\tfrom typing import Tuple\n     8\t\n     9\tfrom config import *\n    10\t\n    11\tclass ResidualBlock(nn.Module):\n    12\t    \&quot;\&quot;\&quot;残差块\&quot;\&quot;\&quot;\n    13\t    def __init__(self, in_channels, out_channels, stride=1):\n    14\t        super(ResidualBlock, self).__init__()\n    15\t\n    16\t        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n    17\t        self.bn1 = nn.BatchNorm2d(out_channels)\n    18\t        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n    19\t        self.bn2 = nn.BatchNorm2d(out_channels)\n    20\t\n    21\t        # 跳跃连接\n    22\t        self.shortcut = nn.Sequential()\n    23\t        if stride != 1 or in_channels != out_channels:\n    24\t            self.shortcut = nn.Sequential(\n    25\t                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n    26\t                nn.BatchNorm2d(out_channels)\n    27\t            )\n    28\t\n    29\t    def forward(self, x):\n    30\t        residual = self.shortcut(x)\n    31\t\n    32\t        out = F.relu(self.bn1(self.conv1(x)))\n    33\t        out = self.bn2(self.conv2(out))\n    34\t\n    35\t        out += residual  # 残差连接\n    36\t        out = F.relu(out)\n    37\t\n    38\t        return out\n    39\t\n    40\tclass CNNFeatureExtractor(nn.Module):\n    41\t    \&quot;\&quot;\&quot;基于残差网络的CNN特征提取器\&quot;\&quot;\&quot;\n    42\t\n    43\t    def __init__(self, input_channels: int = 3):\n    44\t        super(CNNFeatureExtractor, self).__init__()\n    45\t\n    46\t        # 初始卷积层\n    47\t        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=7, stride=2, padding=3, bias=False)\n    48\t        self.bn1 = nn.BatchNorm2d(32)\n    49\t        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n    50\t\n    51\t        # 残差层\n    52\t        self.layer1 = self._make_layer(32, 32, 2, stride=1)\n    53\t        self.layer2 = self._make_layer(32, 64, 2, stride=2)\n    54\t        self.layer3 = self._make_layer(64, 128, 2, stride=2)\n    55\t\n    56\t        # 全局平均池化\n    57\t        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n    58\t\n    59\t        # 特征映射层\n    60\t        self.fc = nn.Sequential(\n    61\t            nn.Linear(128, 256),\n    62\t            nn.ReLU(inplace=True),\n    63\t            nn.Dropout(DROPOUT_RATE),\n    64\t            nn.Linear(256, 512),\n    65\t            nn.ReLU(inplace=True),\n    66\t            nn.Dropout(DROPOUT_RATE)\n    67\t        )\n    68\t\n    69\t        # 权重初始化\n    70\t        self._initialize_weights()\n    71\t\n    72\t    def _make_layer(self, in_channels, out_channels, blocks, stride):\n    73\t        layers = []\n    74\t        layers.append(ResidualBlock(in_channels, out_channels, stride))\n    75\t        for _ in range(1, blocks):\n    76\t            layers.append(ResidualBlock(out_channels, out_channels))\n    77\t        return nn.Sequential(*layers)\n    78\t\n    79\t    def _initialize_weights(self):\n    80\t        for m in self.modules():\n    81\t            if isinstance(m, nn.Conv2d):\n    82\t                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n    83\t            elif isinstance(m, nn.BatchNorm2d):\n    84\t                nn.init.constant_(m.weight, 1)\n    85\t                nn.init.constant_(m.bias, 0)\n    86\t            elif isinstance(m, nn.Linear):\n    87\t                nn.init.normal_(m.weight, 0, 0.01)\n    88\t                nn.init.constant_(m.bias, 0)\n    89\t\n    90\t    def forward(self, x):\n    91\t        # x shape: (batch_size, channels, height, width)\n    92\t        x = F.relu(self.bn1(self.conv1(x)))\n    93\t        x = self.maxpool(x)\n    94\t\n    95\t        x = self.layer1(x)\n    96\t        x = self.layer2(x)\n    97\t        x = self.layer3(x)\n    98\t\n    99\t        x = self.avgpool(x)\n   100\t        x = x.reshape(x.size(0), -1)\n   101\t\n   102\t        x = self.fc(x)\n   103\t\n   104\t        return x\n   105\t\n   106\tclass LandmarkProcessor(nn.Module):\n   107\t    \&quot;\&quot;\&quot;改进的特征点处理器 - 使用残差连接\&quot;\&quot;\&quot;\n   108\t\n   109\t    def __init__(self, landmark_dim: int = 68 * 2):\n   110\t        super(LandmarkProcessor, self).__init__()\n   111\t\n   112\t        # 输入投影\n   113\t        self.input_proj = nn.Linear(landmark_dim, 256)\n   114\t\n   115\t        # 残差块\n   116\t        self.res_block1 = nn.Sequential(\n   117\t            nn.Linear(256, 256),\n   118\t            nn.BatchNorm1d(256),\n   119\t            nn.ReLU(inplace=True),\n   120\t            nn.Dropout(DROPOUT_RATE),\n   121\t            nn.Linear(256, 256),\n   122\t            nn.BatchNorm1d(256)\n   123\t        )\n   124\t\n   125\t        self.res_block2 = nn.Sequential(\n   126\t            nn.Linear(256, 128),\n   127\t            nn.BatchNorm1d(128),\n   128\t            nn.ReLU(inplace=True),\n   129\t            nn.Dropout(DROPOUT_RATE),\n   130\t            nn.Linear(128, 128),\n   131\t            nn.BatchNorm1d(128)\n   132\t        )\n   133\t\n   134\t        # 输出层\n   135\t        self.output_proj = nn.Linear(128, 64)\n   136\t\n   137\t        # 跳跃连接的投影层\n   138\t        self.skip_proj1 = nn.Identity()  # 256 -&gt; 256\n   139\t        self.skip_proj2 = nn.Linear(256, 128)  # 256 -&gt; 128\n   140\t\n   141\t        # 权重初始化\n   142\t        self._initialize_weights()\n   143\t\n   144\t    def _initialize_weights(self):\n   145\t        for m in self.modules():\n   146\t            if isinstance(m, nn.Linear):\n   147\t                nn.init.xavier_normal_(m.weight)\n   148\t                if m.bias is not None:\n   149\t                    nn.init.constant_(m.bias, 0)\n   150\t\n   151\t    def forward(self, x):\n   152\t        # x shape: (batch_size, 68*2)\n   153\t        x = F.relu(self.input_proj(x))  # (batch_size, 256)\n   154\t\n   155\t        # 第一个残差块\n   156\t        residual1 = self.skip_proj1(x)\n   157\t        out1 = self.res_block1(x)\n   158\t        x = F.relu(out1 + residual1)\n   159\t\n   160\t        # 第二个残差块\n   161\t        residual2 = self.skip_proj2(x)\n   162\t        out2 = self.res_block2(x)\n   163\t        x = F.relu(out2 + residual2)\n   164\t\n   165\t        # 输出投影\n   166\t        x = F.relu(self.output_proj(x))\n   167\t\n   168\t        return x\n   169\t\n   170\tclass YawnDetectionModel(nn.Module):\n   171\t    \&quot;\&quot;\&quot;改进的打哈欠检测模型 - 残差CNN+LSTM架构\&quot;\&quot;\&quot;\n   172\t\n   173\t    def __init__(self):\n   174\t        super(YawnDetectionModel, self).__init__()\n   175\t\n   176\t        # CNN特征提取器\n   177\t        self.cnn_extractor = CNNFeatureExtractor()\n   178\t\n   179\t        # 特征点处理器\n   180\t        self.landmark_processor = LandmarkProcessor()\n   181\t\n   182\t        # 改进的特征融合层\n   183\t        self.feature_fusion = nn.Sequential(\n   184\t            nn.Linear(512 + 64, 512),\n   185\t            nn.BatchNorm1d(512),\n   186\t            nn.ReLU(inplace=True),\n   187\t            nn.Dropout(DROPOUT_RATE),\n   188\t            nn.Linear(512, 256),\n   189\t            nn.BatchNorm1d(256),\n   190\t            nn.ReLU(inplace=True)\n   191\t        )\n   192\t\n   193\t        # 多层LSTM\n   194\t        self.lstm1 = nn.LSTM(\n   195\t            input_size=256,\n   196\t            hidden_size=LSTM_HIDDEN_SIZE,\n   197\t            num_layers=1,\n   198\t            batch_first=True,\n   199\t            bidirectional=True\n   200\t        )\n   201\t\n   202\t        self.lstm2 = nn.LSTM(\n   203\t            input_size=LSTM_HIDDEN_SIZE * 2,\n   204\t            hidden_size=LSTM_HIDDEN_SIZE,\n   205\t            num_layers=1,\n   206\t            batch_first=True,\n   207\t            bidirectional=True\n   208\t        )\n   209\t\n   210\t        # LSTM层归一化\n   211\t        self.ln1 = nn.LayerNorm(LSTM_HIDDEN_SIZE * 2)\n   212\t        self.ln2 = nn.LayerNorm(LSTM_HIDDEN_SIZE * 2)\n   213\t\n   214\t        # 改进的注意力机制\n   215\t        self.attention = nn.MultiheadAttention(\n   216\t            embed_dim=LSTM_HIDDEN_SIZE * 2,\n   217\t            num_heads=8,\n   218\t            dropout=DROPOUT_RATE,\n   219\t            batch_first=True\n   220\t        )\n   221\t\n   222\t        # 残差分类器\n   223\t        self.classifier = nn.Sequential(\n   224\t            nn.Linear(LSTM_HIDDEN_SIZE * 2, 256),\n   225\t            nn.BatchNorm1d(256),\n   226\t            nn.ReLU(inplace=True),\n   227\t            nn.Dropout(DROPOUT_RATE),\n   228\t            nn.Linear(256, 128),\n   229\t            nn.BatchNorm1d(128),\n   230\t            nn.ReLU(inplace=True),\n   231\t            nn.Dropout(DROPOUT_RATE),\n   232\t            nn.Linear(128, NUM_CLASSES)\n   233\t        )\n   234\t\n   235\t        # 权重初始化\n   236\t        self._initialize_weights()\n   237\t\n   238\t    def _initialize_weights(self):\n   239\t        for m in self.modules():\n   240\t            if isinstance(m, nn.Linear):\n   241\t                nn.init.xavier_normal_(m.weight)\n   242\t                if m.bias is not None:\n   243\t                    nn.init.constant_(m.bias, 0)\n   244\t            elif isinstance(m, nn.LSTM):\n   245\t                for name, param in m.named_parameters():\n   246\t                    if 'weight' in name:\n   247\t                        nn.init.orthogonal_(param)\n   248\t                    elif 'bias' in name:\n   249\t                        nn.init.constant_(param, 0)\n   250\t        \n   251\t    def forward(self, faces, landmarks):\n   252\t        \&quot;\&quot;\&quot;\n   253\t        改进的前向传播\n   254\t\n   255\t        Args:\n   256\t            faces: 人脸图像序列 (batch_size, sequence_length, channels, height, width)\n   257\t            landmarks: 特征点序列 (batch_size, sequence_length, 68*2)\n   258\t\n   259\t        Returns:\n   260\t            output: 分类结果 (batch_size, num_classes)\n   261\t        \&quot;\&quot;\&quot;\n   262\t        batch_size, seq_len = faces.size(0), faces.size(1)\n   263\t\n   264\t        # 重塑输入以便CNN处理\n   265\t        faces = faces.reshape(batch_size * seq_len, *faces.shape[2:])\n   266\t        landmarks = landmarks.reshape(batch_size * seq_len, -1)\n   267\t\n   268\t        # CNN特征提取\n   269\t        cnn_features = self.cnn_extractor(faces)  # (batch_size * seq_len, 512)\n   270\t\n   271\t        # 特征点处理\n   272\t        landmark_features = self.landmark_processor(landmarks)  # (batch_size * seq_len, 64)\n   273\t\n   274\t        # 特征融合\n   275\t        fused_features = torch.cat([cnn_features, landmark_features], dim=1)\n   276\t        fused_features = self.feature_fusion(fused_features)\n   277\t\n   278\t        # 重塑为序列格式\n   279\t        fused_features = fused_features.reshape(batch_size, seq_len, -1)\n   280\t\n   281\t        # 多层LSTM处理\n   282\t        lstm_out1, _ = self.lstm1(fused_features)\n   283\t        lstm_out1 = self.ln1(lstm_out1)  # 层归一化\n   284\t\n   285\t        # 残差连接（如果维度匹配）\n   286\t        if lstm_out1.size(-1) == fused_features.size(-1):\n   287\t            lstm_out1 = lstm_out1 + fused_features\n   288\t\n   289\t        lstm_out2, _ = self.lstm2(lstm_out1)\n   290\t        lstm_out2 = self.ln2(lstm_out2)  # 层归一化\n   291\t\n   292\t        # 残差连接\n   293\t        lstm_out2 = lstm_out2 + lstm_out1\n   294\t\n   295\t        # 注意力机制\n   296\t        attn_out, attn_weights = self.attention(lstm_out2, lstm_out2, lstm_out2)\n   297\t\n   298\t        # 多种池化策略结合\n   299\t        # 1. 全局平均池化\n   300\t        avg_pooled = torch.mean(attn_out, dim=1)\n   301\t        # 2. 全局最大池化\n   302\t        max_pooled, _ = torch.max(attn_out, dim=1)\n   303\t        # 3. 最后时间步\n   304\t        last_step = attn_out[:, -1, :]\n   305\t\n   306\t        # 组合不同的池化结果\n   307\t        pooled_features = (avg_pooled + max_pooled + last_step) / 3\n   308\t\n   309\t        # 分类\n   310\t        output = self.classifier(pooled_features)\n   311\t\n   312\t        return output\n   313\t    \n   314\t    def get_attention_weights(self, faces, landmarks):\n   315\t        \&quot;\&quot;\&quot;获取注意力权重用于可视化\&quot;\&quot;\&quot;\n   316\t        with torch.no_grad():\n   317\t            batch_size, seq_len = faces.size(0), faces.size(1)\n   318\t            \n   319\t            # 重塑输入\n   320\t            faces = faces.reshape(batch_size * seq_len, *faces.shape[2:])\n   321\t            landmarks = landmarks.reshape(batch_size * seq_len, -1)\n   322\t            \n   323\t            # 特征提取和融合\n   324\t            cnn_features = self.cnn_extractor(faces)\n   325\t            landmark_features = self.landmark_processor(landmarks)\n   326\t            fused_features = torch.cat([cnn_features, landmark_features], dim=1)\n   327\t            fused_features = F.relu(self.feature_fusion(fused_features))\n   328\t            fused_features = fused_features.reshape(batch_size, seq_len, -1)\n   329\t            \n   330\t            # LSTM处理\n   331\t            lstm_out, _ = self.lstm(fused_features)\n   332\t            \n   333\t            # 注意力权重\n   334\t            _, attn_weights = self.attention(lstm_out, lstm_out, lstm_out)\n   335\t            \n   336\t            return attn_weights\n   337\t\n   338\tclass ImprovedFocalLoss(nn.Module):\n   339\t    \&quot;\&quot;\&quot;改进的Focal Loss - 处理类别不平衡和标签平滑\&quot;\&quot;\&quot;\n   340\t\n   341\t    def __init__(self, alpha=None, gamma=2, label_smoothing=0.1, reduction='mean'):\n   342\t        super(ImprovedFocalLoss, self).__init__()\n   343\t        self.alpha = alpha\n   344\t        self.gamma = gamma\n   345\t        self.label_smoothing = label_smoothing\n   346\t        self.reduction = reduction\n   347\t\n   348\t    def forward(self, inputs, targets):\n   349\t        # 标签平滑\n   350\t        num_classes = inputs.size(-1)\n   351\t        if self.label_smoothing &gt; 0:\n   352\t            smooth_targets = torch.zeros_like(inputs)\n   353\t            smooth_targets.fill_(self.label_smoothing / (num_classes - 1))\n   354\t            smooth_targets.scatter_(1, targets.unsqueeze(1), 1.0 - self.label_smoothing)\n   355\t\n   356\t            # 使用KL散度计算平滑损失\n   357\t            log_probs = F.log_softmax(inputs, dim=-1)\n   358\t            loss = -torch.sum(smooth_targets * log_probs, dim=-1)\n   359\t        else:\n   360\t            loss = F.cross_entropy(inputs, targets, reduction='none')\n   361\t\n   362\t        # Focal权重\n   363\t        probs = F.softmax(inputs, dim=-1)\n   364\t        pt = probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n   365\t        focal_weight = (1 - pt) ** self.gamma\n   366\t\n   367\t        # 类别权重\n   368\t        if self.alpha is not None:\n   369\t            if isinstance(self.alpha, (float, int)):\n   370\t                alpha_t = self.alpha\n   371\t            else:\n   372\t                alpha_t = self.alpha[targets]\n   373\t            focal_weight = alpha_t * focal_weight\n   374\t\n   375\t        focal_loss = focal_weight * loss\n   376\t\n   377\t        if self.reduction == 'mean':\n   378\t            return focal_loss.mean()\n   379\t        elif self.reduction == 'sum':\n   380\t            return focal_loss.sum()\n   381\t        else:\n   382\t            return focal_loss\n   383\t\n   384\tclass LabelSmoothingCrossEntropy(nn.Module):\n   385\t    \&quot;\&quot;\&quot;标签平滑交叉熵损失\&quot;\&quot;\&quot;\n   386\t\n   387\t    def __init__(self, smoothing=0.1):\n   388\t        super(LabelSmoothingCrossEntropy, self).__init__()\n   389\t        self.smoothing = smoothing\n   390\t\n   391\t    def forward(self, inputs, targets):\n   392\t        log_probs = F.log_softmax(inputs, dim=-1)\n   393\t        num_classes = inputs.size(-1)\n   394\t\n   395\t        # 创建平滑标签\n   396\t        smooth_targets = torch.zeros_like(log_probs)\n   397\t        smooth_targets.fill_(self.smoothing / (num_classes - 1))\n   398\t        smooth_targets.scatter_(1, targets.unsqueeze(1), 1.0 - self.smoothing)\n   399\t\n   400\t        loss = -torch.sum(smooth_targets * log_probs, dim=-1)\n   401\t        return loss.mean()\n   402\t\n   403\tdef create_model():\n   404\t    \&quot;\&quot;\&quot;创建模型实例\&quot;\&quot;\&quot;\n   405\t    model = YawnDetectionModel()\n   406\t    return model\n   407\t\n   408\tdef count_parameters(model):\n   409\t    \&quot;\&quot;\&quot;统计模型参数数量\&quot;\&quot;\&quot;\n   410\t    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n   411\t\n   412\tif __name__ == \&quot;__main__\&quot;:\n   413\t    # 测试模型\n   414\t    model = create_model()\n   415\t    \n   416\t    # 创建测试输入\n   417\t    batch_size = 2\n   418\t    faces = torch.randn(batch_size, SEQUENCE_LENGTH, 3, FACE_SIZE[0], FACE_SIZE[1])\n   419\t    landmarks = torch.randn(batch_size, SEQUENCE_LENGTH, 68 * 2)\n   420\t    \n   421\t    # 前向传播\n   422\t    output = model(faces, landmarks)\n   423\t    \n   424\t    print(f\&quot;模型参数数量: {count_parameters(model):,}\&quot;)\n   425\t    print(f\&quot;输入形状 - 人脸: {faces.shape}, 特征点: {landmarks.shape}\&quot;)\n   426\t    print(f\&quot;输出形状: {output.shape}\&quot;)\n   427\t    print(f\&quot;输出: {output}\&quot;)\n   428\t\nTotal lines in file: 428\n&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;8d6330df-d97c-4e88-b0df-7ff94ca41ca1;toolu_01FBQGEEnArs6ktauSHS5T93&quot;:{&quot;requestId&quot;:&quot;8d6330df-d97c-4e88-b0df-7ff94ca41ca1&quot;,&quot;toolUseId&quot;:&quot;toolu_01FBQGEEnArs6ktauSHS5T93&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;Here's the result of running `cat -n` on utils.py:\n     1\t\&quot;\&quot;\&quot;\n     2\t工具函数\n     3\t\&quot;\&quot;\&quot;\n     4\timport cv2\n     5\timport numpy as np\n     6\timport dlib\n     7\tfrom typing import List, Tuple, Optional\n     8\timport logging\n     9\t\n    10\tdef setup_logging():\n    11\t    \&quot;\&quot;\&quot;设置日志\&quot;\&quot;\&quot;\n    12\t    logging.basicConfig(\n    13\t        level=logging.INFO,\n    14\t        format='%(asctime)s - %(levelname)s - %(message)s',\n    15\t        handlers=[\n    16\t            logging.FileHandler('training.log'),\n    17\t            logging.StreamHandler()\n    18\t        ]\n    19\t    )\n    20\t    return logging.getLogger(__name__)\n    21\t\n    22\tdef parse_annotation_line(line: str) -&gt; Tuple[str, int, List[Tuple[int, int]]]:\n    23\t    \&quot;\&quot;\&quot;\n    24\t    解析标注文件的一行\n    25\t    \n    26\t    Args:\n    27\t        line: 标注行，格式：filename label intervals\n    28\t        \n    29\t    Returns:\n    30\t        filename: 视频文件名\n    31\t        label: 类别标签\n    32\t        intervals: 打哈欠区间列表 [(start, end), ...]\n    33\t    \&quot;\&quot;\&quot;\n    34\t    parts = line.strip().split()\n    35\t    if len(parts) &lt; 3:\n    36\t        return None, None, None\n    37\t        \n    38\t    filename = parts[0]\n    39\t    label = int(parts[1])\n    40\t    interval_str = parts[2]\n    41\t    \n    42\t    intervals = []\n    43\t    if interval_str != \&quot;-1,-1\&quot;:\n    44\t        # 解析多个区间：774-965,1623-1806,2387-2631\n    45\t        for interval in interval_str.split(','):\n    46\t            if '-' in interval:\n    47\t                start, end = map(int, interval.split('-'))\n    48\t                intervals.append((start, end))\n    49\t    \n    50\t    return filename, label, intervals\n    51\t\n    52\tdef extract_face_landmarks(image: np.ndarray, detector, predictor) -&gt; Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n    53\t    \&quot;\&quot;\&quot;\n    54\t    提取人脸区域和68个特征点\n    55\t\n    56\t    Args:\n    57\t        image: 输入图像\n    58\t        detector: dlib人脸检测器\n    59\t        predictor: dlib特征点预测器\n    60\t\n    61\t    Returns:\n    62\t        face_image: 人脸区域图像\n    63\t        landmarks: 68个特征点坐标 (68, 2)\n    64\t    \&quot;\&quot;\&quot;\n    65\t    try:\n    66\t        # 检查输入图像\n    67\t        if image is None or image.size == 0:\n    68\t            return None, None\n    69\t\n    70\t        # 转换为灰度图\n    71\t        if len(image.shape) == 3:\n    72\t            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    73\t        else:\n    74\t            gray = image\n    75\t\n    76\t        # 人脸检测\n    77\t        faces = detector(gray)\n    78\t\n    79\t        if len(faces) == 0:\n    80\t            return None, None\n    81\t\n    82\t        # 取第一个检测到的人脸\n    83\t        face = faces[0]\n    84\t\n    85\t        # 提取人脸区域坐标\n    86\t        x, y, w, h = face.left(), face.top(), face.width(), face.height()\n    87\t\n    88\t        # 边界检查\n    89\t        img_h, img_w = image.shape[:2]\n    90\t        x = max(0, x)\n    91\t        y = max(0, y)\n    92\t        x2 = min(img_w, x + w)\n    93\t        y2 = min(img_h, y + h)\n    94\t\n    95\t        # 检查区域是否有效\n    96\t        if x2 &lt;= x or y2 &lt;= y:\n    97\t            return None, None\n    98\t\n    99\t        # 提取人脸区域\n   100\t        face_image = image[y:y2, x:x2]\n   101\t\n   102\t        # 检查提取的人脸图像\n   103\t        if face_image.size == 0 or face_image.shape[0] == 0 or face_image.shape[1] == 0:\n   104\t            return None, None\n   105\t\n   106\t        # 提取特征点\n   107\t        landmarks = predictor(gray, face)\n   108\t        landmarks_array = np.array([[p.x - x, p.y - y] for p in landmarks.parts()])\n   109\t\n   110\t        return face_image, landmarks_array\n   111\t\n   112\t    except Exception as e:\n   113\t        # 任何异常都返回None\n   114\t        return None, None\n   115\t\n   116\tdef normalize_landmarks(landmarks: np.ndarray, face_size: Tuple[int, int]) -&gt; np.ndarray:\n   117\t    \&quot;\&quot;\&quot;\n   118\t    归一化特征点坐标\n   119\t    \n   120\t    Args:\n   121\t        landmarks: 原始特征点坐标\n   122\t        face_size: 人脸图像尺寸\n   123\t        \n   124\t    Returns:\n   125\t        normalized_landmarks: 归一化后的特征点坐标\n   126\t    \&quot;\&quot;\&quot;\n   127\t    if landmarks is None:\n   128\t        return None\n   129\t    \n   130\t    # 归一化到[0, 1]范围\n   131\t    landmarks_norm = landmarks.copy().astype(np.float32)\n   132\t    landmarks_norm[:, 0] /= face_size[0]\n   133\t    landmarks_norm[:, 1] /= face_size[1]\n   134\t    \n   135\t    return landmarks_norm\n   136\t\n   137\tdef augment_image(image: np.ndarray, landmarks: np.ndarray, augmentation_params: dict) -&gt; Tuple[np.ndarray, np.ndarray]:\n   138\t    \&quot;\&quot;\&quot;\n   139\t    数据增强\n   140\t    \n   141\t    Args:\n   142\t        image: 输入图像\n   143\t        landmarks: 特征点\n   144\t        augmentation_params: 增强参数\n   145\t        \n   146\t    Returns:\n   147\t        augmented_image: 增强后的图像\n   148\t        augmented_landmarks: 增强后的特征点\n   149\t    \&quot;\&quot;\&quot;\n   150\t    aug_image = image.copy()\n   151\t    aug_landmarks = landmarks.copy() if landmarks is not None else None\n   152\t    \n   153\t    # 亮度调整\n   154\t    if 'brightness_range' in augmentation_params:\n   155\t        brightness = np.random.uniform(-augmentation_params['brightness_range'], \n   156\t                                     augmentation_params['brightness_range'])\n   157\t        aug_image = cv2.convertScaleAbs(aug_image, alpha=1, beta=brightness * 255)\n   158\t    \n   159\t    # 对比度调整\n   160\t    if 'contrast_range' in augmentation_params:\n   161\t        contrast = np.random.uniform(1 - augmentation_params['contrast_range'],\n   162\t                                   1 + augmentation_params['contrast_range'])\n   163\t        aug_image = cv2.convertScaleAbs(aug_image, alpha=contrast, beta=0)\n   164\t    \n   165\t    # 水平翻转\n   166\t    if augmentation_params.get('horizontal_flip', False) and np.random.random() &gt; 0.5:\n   167\t        aug_image = cv2.flip(aug_image, 1)\n   168\t        if aug_landmarks is not None:\n   169\t            aug_landmarks[:, 0] = 1.0 - aug_landmarks[:, 0]  # 假设landmarks已归一化\n   170\t    \n   171\t    return aug_image, aug_landmarks\n   172\t\n   173\tdef create_sliding_windows(total_frames: int, sequence_length: int, overlap_ratio: float) -&gt; List[Tuple[int, int]]:\n   174\t    \&quot;\&quot;\&quot;\n   175\t    创建滑动窗口\n   176\t    \n   177\t    Args:\n   178\t        total_frames: 总帧数\n   179\t        sequence_length: 序列长度\n   180\t        overlap_ratio: 重叠比例\n   181\t        \n   182\t    Returns:\n   183\t        windows: 窗口列表 [(start, end), ...]\n   184\t    \&quot;\&quot;\&quot;\n   185\t    if total_frames &lt; sequence_length:\n   186\t        return [(0, total_frames)]\n   187\t    \n   188\t    step = int(sequence_length * (1 - overlap_ratio))\n   189\t    windows = []\n   190\t    \n   191\t    start = 0\n   192\t    while start + sequence_length &lt;= total_frames:\n   193\t        windows.append((start, start + sequence_length))\n   194\t        start += step\n   195\t    \n   196\t    # 确保最后一个窗口包含到最后一帧\n   197\t    if windows[-1][1] &lt; total_frames:\n   198\t        windows.append((total_frames - sequence_length, total_frames))\n   199\t    \n   200\t    return windows\n   201\t\n   202\tdef interpolate_missing_landmarks(landmarks_sequence: List[Optional[np.ndarray]]) -&gt; List[np.ndarray]:\n   203\t    \&quot;\&quot;\&quot;\n   204\t    插值缺失的特征点\n   205\t    \n   206\t    Args:\n   207\t        landmarks_sequence: 特征点序列，可能包含None\n   208\t        \n   209\t    Returns:\n   210\t        interpolated_sequence: 插值后的特征点序列\n   211\t    \&quot;\&quot;\&quot;\n   212\t    # 找到有效的特征点\n   213\t    valid_indices = [i for i, lm in enumerate(landmarks_sequence) if lm is not None]\n   214\t    \n   215\t    if len(valid_indices) == 0:\n   216\t        return [np.zeros((68, 2)) for _ in landmarks_sequence]\n   217\t    \n   218\t    interpolated = []\n   219\t    for i, landmarks in enumerate(landmarks_sequence):\n   220\t        if landmarks is not None:\n   221\t            interpolated.append(landmarks)\n   222\t        else:\n   223\t            # 线性插值\n   224\t            if len(valid_indices) == 1:\n   225\t                interpolated.append(landmarks_sequence[valid_indices[0]])\n   226\t            else:\n   227\t                # 找到最近的两个有效点进行插值\n   228\t                left_idx = max([idx for idx in valid_indices if idx &lt; i], default=valid_indices[0])\n   229\t                right_idx = min([idx for idx in valid_indices if idx &gt; i], default=valid_indices[-1])\n   230\t                \n   231\t                if left_idx == right_idx:\n   232\t                    interpolated.append(landmarks_sequence[left_idx])\n   233\t                else:\n   234\t                    # 线性插值\n   235\t                    alpha = (i - left_idx) / (right_idx - left_idx)\n   236\t                    left_lm = landmarks_sequence[left_idx]\n   237\t                    right_lm = landmarks_sequence[right_idx]\n   238\t                    interp_lm = (1 - alpha) * left_lm + alpha * right_lm\n   239\t                    interpolated.append(interp_lm)\n   240\t    \n   241\t    return interpolated\n   242\t\nTotal lines in file: 242\n&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;8c71ee27-eb8e-4c3b-95f6-13c29e8bd63c;toolu_01FB94xbBVVsHVo3e7eWbNV3&quot;:{&quot;requestId&quot;:&quot;8c71ee27-eb8e-4c3b-95f6-13c29e8bd63c&quot;,&quot;toolUseId&quot;:&quot;toolu_01FB94xbBVVsHVo3e7eWbNV3&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;Here's the result of running `cat -n` on database_config.py:\n     1\t\&quot;\&quot;\&quot;\n     2\t数据库配置文件\n     3\t用于管理MySQL数据库连接配置\n     4\t\&quot;\&quot;\&quot;\n     5\t\n     6\timport pymysql\n     7\timport datetime\n     8\tfrom contextlib import contextmanager\n     9\t\n    10\t# MySQL数据库配置\n    11\tDB_CONFIG = {\n    12\t    'host': '101.245.79.154',\n    13\t    'port': 3306,\n    14\t    'user': 'root',\n    15\t    'password': '123456',\n    16\t    'database': 'pljc',\n    17\t    'charset': 'utf8mb4',\n    18\t    'autocommit': True\n    19\t}\n    20\t\n    21\t@contextmanager\n    22\tdef get_db_connection():\n    23\t    \&quot;\&quot;\&quot;\n    24\t    获取数据库连接的上下文管理器\n    25\t    自动处理连接的打开和关闭\n    26\t    \&quot;\&quot;\&quot;\n    27\t    connection = None\n    28\t    try:\n    29\t        connection = pymysql.connect(**DB_CONFIG)\n    30\t        yield connection\n    31\t    except Exception as e:\n    32\t        if connection:\n    33\t            connection.rollback()\n    34\t        raise e\n    35\t    finally:\n    36\t        if connection:\n    37\t            connection.close()\n    38\t\n    39\tdef init_database():\n    40\t    \&quot;\&quot;\&quot;\n    41\t    初始化数据库表结构\n    42\t    \&quot;\&quot;\&quot;\n    43\t    try:\n    44\t        with get_db_connection() as conn:\n    45\t            cursor = conn.cursor()\n    46\t            \n    47\t            # 创建用户表\n    48\t            cursor.execute('''\n    49\t                CREATE TABLE IF NOT EXISTS users (\n    50\t                    username VARCHAR(50) PRIMARY KEY,\n    51\t                    password VARCHAR(255) NOT NULL,\n    52\t                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n    53\t                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4\n    54\t            ''')\n    55\t            \n    56\t            # 创建疲劳记录表\n    57\t            cursor.execute('''\n    58\t                CREATE TABLE IF NOT EXISTS fatigue_records (\n    59\t                    id INT AUTO_INCREMENT PRIMARY KEY,\n    60\t                    username VARCHAR(50) NOT NULL,\n    61\t                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    62\t                    fatigue_level VARCHAR(20) NOT NULL,\n    63\t                    INDEX idx_username (username),\n    64\t                    INDEX idx_timestamp (timestamp),\n    65\t                    FOREIGN KEY (username) REFERENCES users(username) ON DELETE CASCADE\n    66\t                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4\n    67\t            ''')\n    68\t            \n    69\t            conn.commit()\n    70\t            print(\&quot;数据库初始化成功\&quot;)\n    71\t            \n    72\t    except Exception as e:\n    73\t        print(f\&quot;数据库初始化失败: {e}\&quot;)\n    74\t        raise e\n    75\t\n    76\tdef test_connection():\n    77\t    \&quot;\&quot;\&quot;\n    78\t    测试数据库连接\n    79\t    \&quot;\&quot;\&quot;\n    80\t    try:\n    81\t        with get_db_connection() as conn:\n    82\t            cursor = conn.cursor()\n    83\t            cursor.execute(\&quot;SELECT 1\&quot;)\n    84\t            result = cursor.fetchone()\n    85\t            if result:\n    86\t                print(\&quot;数据库连接测试成功\&quot;)\n    87\t                return True\n    88\t            else:\n    89\t                print(\&quot;数据库连接测试失败\&quot;)\n    90\t                return False\n    91\t    except Exception as e:\n    92\t        print(f\&quot;数据库连接测试失败: {e}\&quot;)\n    93\t        return False\n    94\t\n    95\tif __name__ == \&quot;__main__\&quot;:\n    96\t    # 测试数据库连接和初始化\n    97\t    print(\&quot;测试数据库连接...\&quot;)\n    98\t    if test_connection():\n    99\t        print(\&quot;初始化数据库表...\&quot;)\n   100\t        init_database()\n   101\t        print(\&quot;数据库配置完成\&quot;)\n   102\t    else:\n   103\t        print(\&quot;数据库连接失败，请检查配置\&quot;)\n   104\t\nTotal lines in file: 104\n&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;5b06f876-3257-4311-a896-8a74bd0bd606;toolu_01WMUzKd9bR9QCVK9DYsCzoj&quot;:{&quot;requestId&quot;:&quot;5b06f876-3257-4311-a896-8a74bd0bd606&quot;,&quot;toolUseId&quot;:&quot;toolu_01WMUzKd9bR9QCVK9DYsCzoj&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;File saved.  Saved file {requirements.txt}&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;d9a1c9e0-3373-4a25-a6ff-2b42df19831d;toolu_01JTgwXoCi7QWcoWu4SaKAnf&quot;:{&quot;requestId&quot;:&quot;d9a1c9e0-3373-4a25-a6ff-2b42df19831d&quot;,&quot;toolUseId&quot;:&quot;toolu_01JTgwXoCi7QWcoWu4SaKAnf&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;File saved.  Saved file {Dockerfile}&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;69cc72a7-59d3-4616-8e99-2964e00fc620;toolu_015KTFw2S3vLd2UEh9hW3cBu&quot;:{&quot;requestId&quot;:&quot;69cc72a7-59d3-4616-8e99-2964e00fc620&quot;,&quot;toolUseId&quot;:&quot;toolu_015KTFw2S3vLd2UEh9hW3cBu&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;File saved.  Saved file {.dockerignore}&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;b0c27318-6ea0-4997-b62c-16e9d29067cf;toolu_01Cm7TUjgrQk6YSgJwtUZFtm&quot;:{&quot;requestId&quot;:&quot;b0c27318-6ea0-4997-b62c-16e9d29067cf&quot;,&quot;toolUseId&quot;:&quot;toolu_01Cm7TUjgrQk6YSgJwtUZFtm&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;File saved.  Saved file {docker-compose.yml}&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;c8f55c78-a91d-41b6-9a5e-2f89859e66ed;toolu_01ULcB34zoS7pR61wxUmjJpa&quot;:{&quot;requestId&quot;:&quot;c8f55c78-a91d-41b6-9a5e-2f89859e66ed&quot;,&quot;toolUseId&quot;:&quot;toolu_01ULcB34zoS7pR61wxUmjJpa&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;File saved.  Saved file {deploy.sh}&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;f5755f29-d717-4f93-b1ab-8d6a0a05e5a1;toolu_01GM2As1bbxwM3XN2c58asmN&quot;:{&quot;requestId&quot;:&quot;f5755f29-d717-4f93-b1ab-8d6a0a05e5a1&quot;,&quot;toolUseId&quot;:&quot;toolu_01GM2As1bbxwM3XN2c58asmN&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;File saved.  Saved file {README_Docker.md}&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;92e8c37f-8516-4478-bd6c-6afe98582047;toolu_01MFYezfsXZe1MDR98FPJcaE&quot;:{&quot;requestId&quot;:&quot;92e8c37f-8516-4478-bd6c-6afe98582047&quot;,&quot;toolUseId&quot;:&quot;toolu_01MFYezfsXZe1MDR98FPJcaE&quot;,&quot;phase&quot;:8},&quot;8e88e51c-dcec-4d34-9904-913d9ec658a7;toolu_01DXcuiRzjBSLdFgFBPBU3e8&quot;:{&quot;requestId&quot;:&quot;8e88e51c-dcec-4d34-9904-913d9ec658a7&quot;,&quot;toolUseId&quot;:&quot;toolu_01DXcuiRzjBSLdFgFBPBU3e8&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;Command is still running after 10 seconds. You can use read-process to get more output\n          and kill-process to terminate it if needed.\n          PID 2\n          Output so far:\n          &lt;stdout&gt;\n          \u001b[?9001h\u001b[?1004h\u001b[?25l\u001b[2J\u001b[m\u001b[2;1H正在 Ping registry-1.docker.io [3.94.224.37] 具有 32 字节的数据:\n\u001b]0;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe\u0007\u001b[?25h\u001b[?25l\u001b[8;7;161t\u001b[H\u001b[K\n正在 Ping registry-1.docker.io [3.94.224.37] 具有 32 字节的数据:\u001b[K\n\u001b[K\n\u001b[K\n\u001b[K\n\u001b[K\n\u001b[K\u001b[3;1H\u001b[?25h请求超时。\n请求超时。\n\n          &lt;/stdout&gt;\n          &lt;stderr&gt;\n          \n          &lt;/stderr&gt;&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;4800304e-4799-4701-a072-840c1d8902e1;toolu_014Yvbxn8YpCRVz2jhnzJd5U&quot;:{&quot;requestId&quot;:&quot;4800304e-4799-4701-a072-840c1d8902e1&quot;,&quot;toolUseId&quot;:&quot;toolu_014Yvbxn8YpCRVz2jhnzJd5U&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;Terminal 2 killed&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;e2a9cfac-89ae-4ad7-8f44-574e9ed4866e;toolu_01JkM8ZrLxa1EU6AfABUSHZx&quot;:{&quot;requestId&quot;:&quot;e2a9cfac-89ae-4ad7-8f44-574e9ed4866e&quot;,&quot;toolUseId&quot;:&quot;toolu_01JkM8ZrLxa1EU6AfABUSHZx&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;File saved.  Saved file {docker-daemon.json}&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}}},&quot;draftExchange&quot;:{&quot;request_message&quot;:&quot;&quot;,&quot;rich_text_json_repr&quot;:{&quot;type&quot;:&quot;doc&quot;,&quot;content&quot;:[{&quot;type&quot;:&quot;paragraph&quot;}]},&quot;status&quot;:&quot;draft&quot;},&quot;requestIds&quot;:[],&quot;isPinned&quot;:false,&quot;isShareable&quot;:true,&quot;extraData&quot;:{&quot;isAgentConversation&quot;:true,&quot;baselineTimestamp&quot;:0,&quot;hasDirtyEdits&quot;:true},&quot;personaType&quot;:0,&quot;rootTaskUuid&quot;:&quot;c64a5269-3bc9-401f-b492-ca295b81a9c2&quot;}},&quot;agentExecutionMode&quot;:&quot;manual&quot;,&quot;isPanelCollapsed&quot;:true,&quot;displayedAnnouncements&quot;:[],&quot;sortConversationsBy&quot;:&quot;lastMessageTimestamp&quot;,&quot;sendMode&quot;:&quot;send&quot;}" />
      </map>
    </option>
  </component>
</project>