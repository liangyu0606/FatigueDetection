"""
ç–²åŠ³æ£€æµ‹Webåº”ç”¨ - FFmpegåç«¯ä¼˜åŒ–ç‰ˆæœ¬
ä½¿ç”¨FFmpegè¿›è¡Œè§†é¢‘å‹ç¼©å’Œä¼˜åŒ–ï¼Œå¤§å¹…å‡å°‘ç½‘ç»œä¼ è¾“æ•°æ®é‡
é›†æˆå®Œæ•´çš„AIç–²åŠ³æ£€æµ‹åŠŸèƒ½
"""
import cv2
import torch
import numpy as np
import dlib
from collections import deque
import base64
import json
import subprocess
import tempfile
import os
import sys
import pygame
import asyncio
import time
import threading
from queue import Queue, Empty
from datetime import datetime
from typing import Dict, Any

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, Request, Form, HTTPException
from fastapi.responses import HTMLResponse, JSONResponse, RedirectResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
import uvicorn

# å¯¼å…¥AIæ£€æµ‹ç›¸å…³æ¨¡å—
try:
    from config import *
    from model import create_model
    from utils import extract_face_landmarks, normalize_landmarks
    from database_config import get_db_connection, init_database

    # å®šä¹‰æ¨¡å‹è·¯å¾„
    MODEL_PATH = os.path.join(MODEL_SAVE_PATH, "best_model.pth")
    AI_MODULES_AVAILABLE = True
    print("âœ… AIæ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸  AIæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    print("ç³»ç»Ÿå°†åœ¨åŸºç¡€æ¨¡å¼ä¸‹è¿è¡Œ")
    AI_MODULES_AVAILABLE = False

    # å®šä¹‰åŸºç¡€é…ç½®
    MODEL_PATH = "output/models/best_model.pth"
    DLIB_PREDICTOR_PATH = "output/models/shape_predictor_68_face_landmarks.dat"
    SEQUENCE_LENGTH = 30
    FACE_SIZE = (64, 64)

    # åˆ›å»ºæ¨¡æ‹Ÿå‡½æ•°
    def create_model():
        return None

    def extract_face_landmarks(frame, detector, predictor):
        return None, None

    def normalize_landmarks(landmarks, shape):
        return None

    def init_database():
        pass

app = FastAPI(title="ç–²åŠ³æ£€æµ‹ç³»ç»Ÿ - FFmpegä¼˜åŒ–ç‰ˆ")

# é™æ€æ–‡ä»¶å’Œæ¨¡æ¿
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")

class FFmpegVideoProcessor:
    """FFmpegè§†é¢‘å¤„ç†å™¨"""
    
    def __init__(self):
        self.temp_dir = tempfile.mkdtemp()
        self.frame_counter = 0
        self.ffmpeg_path = None  # å°†å­˜å‚¨FFmpegçš„è·¯å¾„

        # FFmpegå‹ç¼©å‚æ•°
        self.compression_settings = {
            'preset': 'veryfast',
            'crf': '28',  # æ’å®šè´¨é‡å› å­ (18-28ä¸ºåˆç†èŒƒå›´)
            'scale': '320:240',
            'fps': '5',
            'format': 'webm'
        }
        
    def check_ffmpeg(self):
        """æ£€æŸ¥FFmpegæ˜¯å¦å¯ç”¨"""
        # é¦–å…ˆå°è¯•PATHä¸­çš„ffmpeg
        try:
            result = subprocess.run(['ffmpeg', '-version'],
                                  capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                self.ffmpeg_path = 'ffmpeg'  # ä½¿ç”¨PATHä¸­çš„ffmpeg
                return True
        except:
            pass

        # å¦‚æœPATHä¸­æ²¡æœ‰ï¼Œå°è¯•å¸¸è§çš„Windowså®‰è£…è·¯å¾„
        common_paths = [
            r"C:\ffmpeg\bin\ffmpeg.exe",
            r"C:\Program Files\ffmpeg\bin\ffmpeg.exe",
            r"C:\Program Files (x86)\ffmpeg\bin\ffmpeg.exe",
            r"D:\ffmpeg\bin\ffmpeg.exe"
        ]

        for path in common_paths:
            if os.path.exists(path):
                try:
                    result = subprocess.run([path, '-version'],
                                          capture_output=True, text=True, timeout=5)
                    if result.returncode == 0:
                        self.ffmpeg_path = path  # ä½¿ç”¨æ‰¾åˆ°çš„å®Œæ•´è·¯å¾„
                        print(f"âœ… æ‰¾åˆ°FFmpeg: {path}")
                        return True
                except:
                    continue

        self.ffmpeg_path = None
        return False
    
    def compress_frame(self, frame_data):
        """ä½¿ç”¨FFmpegå‹ç¼©å•å¸§"""
        try:
            # è§£ç base64å›¾åƒ
            if frame_data.startswith('data:image'):
                frame_data = frame_data.split(',')[1]
            
            img_data = base64.b64decode(frame_data)
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            self.frame_counter += 1
            input_file = os.path.join(self.temp_dir, f'input_{self.frame_counter}.png')
            output_file = os.path.join(self.temp_dir, f'output_{self.frame_counter}.webm')
            
            # å†™å…¥è¾“å…¥æ–‡ä»¶
            with open(input_file, 'wb') as f:
                f.write(img_data)
            
            # FFmpegå‹ç¼©å‘½ä»¤
            cmd = [
                self.ffmpeg_path or 'ffmpeg',  # ä½¿ç”¨æ‰¾åˆ°çš„FFmpegè·¯å¾„
                '-y',  # è¦†ç›–è¾“å‡ºæ–‡ä»¶
                '-i', input_file,
                '-c:v', 'libvpx-vp9',  # VP9ç¼–ç å™¨
                '-preset', self.compression_settings['preset'],
                '-crf', self.compression_settings['crf'],
                '-vf', f"scale={self.compression_settings['scale']},fps={self.compression_settings['fps']}",
                '-f', self.compression_settings['format'],
                '-loglevel', 'quiet',  # é™é»˜æ¨¡å¼
                output_file
            ]
            
            # æ‰§è¡Œå‹ç¼©
            result = subprocess.run(cmd, capture_output=True, timeout=10)
            
            if result.returncode == 0 and os.path.exists(output_file):
                # è¯»å–å‹ç¼©åçš„æ–‡ä»¶
                with open(output_file, 'rb') as f:
                    compressed_data = f.read()
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                self._cleanup_files([input_file, output_file])
                
                # è®¡ç®—å‹ç¼©æ¯”
                original_size = len(img_data)
                compressed_size = len(compressed_data)
                compression_ratio = (1 - compressed_size / original_size) * 100
                
                return {
                    'data': base64.b64encode(compressed_data).decode('utf-8'),
                    'original_size': original_size,
                    'compressed_size': compressed_size,
                    'compression_ratio': compression_ratio
                }
            else:
                self._cleanup_files([input_file, output_file])
                return None
                
        except Exception as e:
            print(f"FFmpegå‹ç¼©å¤±è´¥: {e}")
            return None
    
    def compress_frame_fast(self, frame_data):
        """å¿«é€Ÿå‹ç¼©æ¨¡å¼ - ä½¿ç”¨æ›´æ¿€è¿›çš„å‹ç¼©å‚æ•°"""
        try:
            # è§£ç å¹¶è½¬æ¢ä¸ºOpenCVæ ¼å¼
            if frame_data.startswith('data:image'):
                frame_data = frame_data.split(',')[1]

            img_data = base64.b64decode(frame_data)
            nparr = np.frombuffer(img_data, np.uint8)
            frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

            if frame is None:
                return None

            # è·å–ç›®æ ‡åˆ†è¾¨ç‡
            scale_parts = self.compression_settings['scale'].split(':')
            new_width, new_height = int(scale_parts[0]), int(scale_parts[1])

            # 1. é™ä½åˆ†è¾¨ç‡
            frame_resized = cv2.resize(frame, (new_width, new_height))

            # 2. å¯é€‰çš„å›¾åƒé¢„å¤„ç†
            if self.compression_settings.get('denoise', False):
                frame_resized = cv2.fastNlMeansDenoisingColored(frame_resized)

            # 3. æ ¹æ®CRFå€¼è°ƒæ•´JPEGè´¨é‡
            crf = int(self.compression_settings.get('crf', '28'))
            jpeg_quality = max(10, min(95, 100 - crf * 2))  # CRFè½¬JPEGè´¨é‡

            # 4. é«˜å‹ç¼©ç¼–ç 
            encode_params = [cv2.IMWRITE_JPEG_QUALITY, jpeg_quality]
            _, buffer = cv2.imencode('.jpg', frame_resized, encode_params)

            compressed_data = buffer.tobytes()
            original_size = len(img_data)
            compressed_size = len(compressed_data)
            compression_ratio = (1 - compressed_size / original_size) * 100

            return {
                'data': base64.b64encode(compressed_data).decode('utf-8'),
                'original_size': original_size,
                'compressed_size': compressed_size,
                'compression_ratio': compression_ratio,
                'method': 'opencv_fast'
            }

        except Exception as e:
            print(f"å¿«é€Ÿå‹ç¼©å¤±è´¥: {e}")
            return None

    def compress_frame_webp(self, frame_data):
        """ä½¿ç”¨WebPæ ¼å¼è¿›è¡Œå‹ç¼© - æ›´å¥½çš„å‹ç¼©æ¯”"""
        try:
            if frame_data.startswith('data:image'):
                frame_data = frame_data.split(',')[1]

            img_data = base64.b64decode(frame_data)
            nparr = np.frombuffer(img_data, np.uint8)
            frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

            if frame is None:
                return None

            # è·å–ç›®æ ‡åˆ†è¾¨ç‡
            scale_parts = self.compression_settings['scale'].split(':')
            new_width, new_height = int(scale_parts[0]), int(scale_parts[1])
            frame_resized = cv2.resize(frame, (new_width, new_height))

            # WebPå‹ç¼©å‚æ•°
            crf = int(self.compression_settings.get('crf', '28'))
            webp_quality = max(10, min(100, 100 - crf))

            encode_params = [cv2.IMWRITE_WEBP_QUALITY, webp_quality]
            _, buffer = cv2.imencode('.webp', frame_resized, encode_params)

            compressed_data = buffer.tobytes()
            original_size = len(img_data)
            compressed_size = len(compressed_data)
            compression_ratio = (1 - compressed_size / original_size) * 100

            return {
                'data': base64.b64encode(compressed_data).decode('utf-8'),
                'original_size': original_size,
                'compressed_size': compressed_size,
                'compression_ratio': compression_ratio,
                'method': 'webp'
            }

        except Exception as e:
            print(f"WebPå‹ç¼©å¤±è´¥: {e}")
            return None

    def compress_frame_adaptive(self, frame_data):
        """è‡ªé€‚åº”å‹ç¼© - æ ¹æ®å†…å®¹å¤æ‚åº¦é€‰æ‹©å‹ç¼©ç­–ç•¥"""
        try:
            if frame_data.startswith('data:image'):
                frame_data = frame_data.split(',')[1]

            img_data = base64.b64decode(frame_data)
            nparr = np.frombuffer(img_data, np.uint8)
            frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

            if frame is None:
                return None

            # åˆ†æå›¾åƒå¤æ‚åº¦
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()

            # æ ¹æ®å¤æ‚åº¦è°ƒæ•´å‹ç¼©å‚æ•°
            if laplacian_var > 1000:  # é«˜å¤æ‚åº¦å›¾åƒ
                quality_factor = 0.8
            elif laplacian_var > 500:  # ä¸­ç­‰å¤æ‚åº¦
                quality_factor = 0.6
            else:  # ä½å¤æ‚åº¦å›¾åƒ
                quality_factor = 0.4

            # è·å–ç›®æ ‡åˆ†è¾¨ç‡
            scale_parts = self.compression_settings['scale'].split(':')
            new_width, new_height = int(scale_parts[0]), int(scale_parts[1])
            frame_resized = cv2.resize(frame, (new_width, new_height))

            # è‡ªé€‚åº”è´¨é‡
            base_crf = int(self.compression_settings.get('crf', '28'))
            adaptive_quality = max(10, min(95, int((100 - base_crf) * quality_factor)))

            encode_params = [cv2.IMWRITE_JPEG_QUALITY, adaptive_quality]
            _, buffer = cv2.imencode('.jpg', frame_resized, encode_params)

            compressed_data = buffer.tobytes()
            original_size = len(img_data)
            compressed_size = len(compressed_data)
            compression_ratio = (1 - compressed_size / original_size) * 100

            return {
                'data': base64.b64encode(compressed_data).decode('utf-8'),
                'original_size': original_size,
                'compressed_size': compressed_size,
                'compression_ratio': compression_ratio,
                'method': 'adaptive',
                'complexity': laplacian_var,
                'quality_used': adaptive_quality
            }

        except Exception as e:
            print(f"è‡ªé€‚åº”å‹ç¼©å¤±è´¥: {e}")
            return None
    
    def _cleanup_files(self, files):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        for file in files:
            try:
                if os.path.exists(file):
                    os.remove(file)
            except:
                pass
    
    def update_settings(self, settings):
        """æ›´æ–°å‹ç¼©è®¾ç½®"""
        self.compression_settings.update(settings)

class OptimizedFatigueDetectionSystem:
    """ä¼˜åŒ–çš„ç–²åŠ³æ£€æµ‹ç³»ç»Ÿ - é›†æˆå®Œæ•´AIæ£€æµ‹åŠŸèƒ½"""

    def __init__(self, model_path: str):
        self.model_path = model_path
        self.ai_available = AI_MODULES_AVAILABLE

        if self.ai_available:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")

            # åŠ è½½AIæ¨¡å‹
            self.model = self._load_model()

            # åˆå§‹åŒ–dlib
            self.detector = dlib.get_frontal_face_detector()
            # æ£€æŸ¥ä¸¤ä¸ªå¯èƒ½çš„dlibæ¨¡å‹è·¯å¾„
            dlib_paths = [
                DLIB_PREDICTOR_PATH,  # config.pyä¸­å®šä¹‰çš„è·¯å¾„
                "output/models/shape_predictor_68_face_landmarks.dat"  # å¤‡ç”¨è·¯å¾„
            ]

            self.predictor = None
            for dlib_path in dlib_paths:
                if os.path.exists(dlib_path):
                    self.predictor = dlib.shape_predictor(dlib_path)
                    print(f"âœ… dlibæ¨¡å‹åŠ è½½æˆåŠŸ: {dlib_path}")
                    break

            if self.predictor is None:
                print(f"âš ï¸  dlibæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ£€æŸ¥è·¯å¾„: {dlib_paths}")
                print("ç³»ç»Ÿå°†åœ¨æ¨¡æ‹Ÿæ¨¡å¼ä¸‹è¿è¡Œ")
                self.ai_available = False
        else:
            print("âš ï¸  AIæ¨¡å—ä¸å¯ç”¨ï¼Œç³»ç»Ÿå°†åœ¨æ¨¡æ‹Ÿæ¨¡å¼ä¸‹è¿è¡Œ")
            self.device = None
            self.model = None
            self.detector = None
            self.predictor = None

        # æ£€æµ‹å‚æ•°ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        class SimpleVar:
            def __init__(self, value):
                self._value = value
            def get(self):
                return self._value
            def set(self, value):
                self._value = value

        self.yawn_threshold = SimpleVar(0.6)
        self.mar_threshold = SimpleVar(0.5)  # é»˜è®¤ä½¿ç”¨å¹³è¡¡æ¨¡å¼çš„MARé˜ˆå€¼
        self.ear_threshold = SimpleVar(0.18)  # ç»Ÿä¸€EARé˜ˆå€¼ï¼Œé€‚åº”æ‰€æœ‰çœ¼å‹ç”¨æˆ·
        self.alert_cooldown = SimpleVar(5.0)

        # æ£€æµ‹çŠ¶æ€
        self.is_detecting = False
        self.current_user = None
        self.current_mode = "å¹³è¡¡æ¨¡å¼"

        # ç–²åŠ³çŠ¶æ€è¯„ä¼°ç›¸å…³ï¼ˆä¸PyQtç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰
        self.recent_yawns = []
        self.recent_blinks = []
        self.fatigue_window = 30  # 30ç§’çª—å£
        self.last_fatigue_status = "æ­£å¸¸"  # è®°å½•ä¸Šä¸€æ¬¡çš„ç–²åŠ³çŠ¶æ€
        self.last_blink_time = 0
        self.eye_closed_frames = 0
        self.eye_closed_threshold = 10
        self.long_eye_closed_threshold = 60
        self.eye_closed_start_time = None

        # ç¼“å†²åŒºï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        self.face_buffer = deque(maxlen=SEQUENCE_LENGTH)
        self.landmark_buffer = deque(maxlen=SEQUENCE_LENGTH)

        # ç»Ÿè®¡å˜é‡ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        self.session_start_time = None
        self.yawn_count = 0
        self.blink_count = 0
        self.total_predictions = 0
        self.consecutive_yawns = 0
        self.consecutive_threshold = 15  # é»˜è®¤ä½¿ç”¨å¹³è¡¡æ¨¡å¼çš„è¿ç»­æ£€æµ‹é˜ˆå€¼
        self.last_yawn_time = 0
        self.last_detection_time = 0
        self.no_detection_frames = 0
        self.decay_rate = 2.0

        # çœ¨çœ¼æ£€æµ‹å˜é‡ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        self.eye_closed_frames = 0
        self.eye_closed_threshold = 3
        self.long_eye_closed_threshold = 30
        self.eye_closed_start_time = None
        self.last_blink_time = 0
        self.recent_blinks = []
        self.recent_yawns = []
        self.fatigue_window = 30.0
        self.last_fatigue_status = "æ­£å¸¸"

        # å†…éƒ¨çŠ¶æ€å˜é‡
        self._last_mar = 0.0
        self._last_ear = 0.3

        # éŸ³é¢‘ç³»ç»Ÿ
        self.audio_path = "static/warning.mp3"
        self.audio_initialized = False
        self.warning_sound = None
        self._init_audio()

        # FFmpegå¤„ç†å™¨
        self.video_processor = FFmpegVideoProcessor()
        self.use_ffmpeg = self.video_processor.check_ffmpeg()

        if self.use_ffmpeg:
            print("âœ… FFmpegå¯ç”¨ï¼Œå°†ä½¿ç”¨FFmpegè¿›è¡Œè§†é¢‘å‹ç¼©")
        else:
            print("âš ï¸ FFmpegä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨å¿«é€Ÿå‹ç¼©æ¨¡å¼")

        # å¤„ç†é˜Ÿåˆ—
        self.frame_queue = Queue(maxsize=5)
        self.processing_thread = None

        # å‹ç¼©ç»Ÿè®¡
        self.compression_stats = {
            'total_original_size': 0,
            'total_compressed_size': 0,
            'frames_processed': 0,
            'avg_compression_ratio': 0
        }

        # æœ€æ–°ç»“æœ
        self.latest_results = {
            'frame': None,
            'face_detected': False,
            'yawn_prob': 0.0,
            'prediction': 0,
            'mar': 0.0,
            'ear': 0.0,
            'fatigue_status': 'æ­£å¸¸',
            'consecutive_yawns': 0,
            'session_time': '00:00',
            'buffer_status': '0/30',
            'total_predictions': 0,
            'yawn_count': 0,
            'blink_count': 0,
            'progress': 0,
            'compression_stats': self.compression_stats.copy()
        }

    def _load_model(self):
        """åŠ è½½AIæ¨¡å‹ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰"""
        if not self.ai_available:
            return None

        try:
            if not os.path.exists(self.model_path):
                print(f"âš ï¸  æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}")
                print("ç³»ç»Ÿå°†åœ¨æ¨¡æ‹Ÿæ¨¡å¼ä¸‹è¿è¡Œ")
                return None

            model = create_model().to(self.device)
            checkpoint = torch.load(self.model_path, map_location=self.device)
            model.load_state_dict(checkpoint['model_state_dict'])
            model.eval()
            print("âœ… AIæ¨¡å‹åŠ è½½æˆåŠŸ")
            return model
        except Exception as e:
            print(f"âŒ AIæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ç³»ç»Ÿå°†åœ¨æ¨¡æ‹Ÿæ¨¡å¼ä¸‹è¿è¡Œ")
            return None

    def _init_audio(self):
        """åˆå§‹åŒ–éŸ³é¢‘ç³»ç»Ÿ"""
        try:
            pygame.mixer.init()
            if os.path.exists(self.audio_path):
                self.warning_sound = pygame.mixer.Sound(self.audio_path)
                self.audio_initialized = True
                print("âœ… éŸ³é¢‘ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
            else:
                print(f"âŒ è­¦å‘ŠéŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {self.audio_path}")
                self.audio_initialized = False
        except Exception as e:
            print(f"âŒ éŸ³é¢‘ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            self.audio_initialized = False

    def _play_warning_sound(self):
        """æ’­æ”¾è­¦å‘ŠéŸ³é¢‘"""
        if self.audio_initialized and self.warning_sound:
            try:
                self.warning_sound.play()
                print("ğŸ”Š æ’­æ”¾è­¦å‘ŠéŸ³é¢‘")
            except Exception as e:
                print(f"âŒ æ’­æ”¾éŸ³é¢‘å¤±è´¥: {e}")

    def _preprocess_frame(self, frame):
        """é¢„å¤„ç†å¸§ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰"""
        if self.predictor is None:
            return None, None, None

        face_img, landmarks = extract_face_landmarks(frame, self.detector, self.predictor)

        if face_img is None or landmarks is None:
            return None, None, None

        face_resized = cv2.resize(face_img, FACE_SIZE)
        landmarks_norm = normalize_landmarks(landmarks, face_img.shape[:2])

        # è·å–äººè„¸åŒºåŸŸ
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = self.detector(gray)
        face_rect = faces[0] if len(faces) > 0 else None

        return face_resized, landmarks_norm, face_rect

    def _predict_yawn(self):
        """é¢„æµ‹æ‰“å“ˆæ¬ ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰"""
        if self.model is None or len(self.face_buffer) < SEQUENCE_LENGTH:
            return 0.0, 0

        faces = np.array(list(self.face_buffer))
        landmarks = np.array(list(self.landmark_buffer))

        faces_tensor = torch.from_numpy(faces).float().unsqueeze(0)
        landmarks_tensor = torch.from_numpy(landmarks).float().unsqueeze(0)

        faces_tensor = faces_tensor.permute(0, 1, 4, 2, 3)
        landmarks_tensor = landmarks_tensor.reshape(1, SEQUENCE_LENGTH, -1)
        faces_tensor = faces_tensor / 255.0

        faces_tensor = faces_tensor.to(self.device)
        landmarks_tensor = landmarks_tensor.to(self.device)

        with torch.no_grad():
            outputs = self.model(faces_tensor, landmarks_tensor)
            probabilities = torch.softmax(outputs, dim=1)
            yawn_prob = probabilities[0, 1].item()
            prediction = 1 if yawn_prob > self.yawn_threshold.get() else 0

        return yawn_prob, prediction

    def _calculate_mouth_aspect_ratio(self, landmarks):
        """è®¡ç®—å˜´éƒ¨é•¿å®½æ¯”(MAR)ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰"""
        try:
            mouth_points = landmarks[48:68]
            A = np.linalg.norm(mouth_points[13] - mouth_points[19])
            B = np.linalg.norm(mouth_points[14] - mouth_points[18])
            C = np.linalg.norm(mouth_points[15] - mouth_points[17])
            D = np.linalg.norm(mouth_points[0] - mouth_points[6])
            mar = (A + B + C) / (3.0 * D)
            self._last_mar = mar
            return mar
        except:
            return 0.0

    def _calculate_eye_aspect_ratio(self, landmarks):
        """è®¡ç®—çœ¼éƒ¨é•¿å®½æ¯”(EAR)ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰"""
        try:
            left_eye = landmarks[36:42]
            right_eye = landmarks[42:48]

            def eye_aspect_ratio(eye_points):
                A = np.linalg.norm(eye_points[1] - eye_points[5])
                B = np.linalg.norm(eye_points[2] - eye_points[4])
                C = np.linalg.norm(eye_points[0] - eye_points[3])
                ear = (A + B) / (2.0 * C)
                return ear

            left_ear = eye_aspect_ratio(left_eye)
            right_ear = eye_aspect_ratio(right_eye)
            avg_ear = (left_ear + right_ear) / 2.0
            self._last_ear = avg_ear

            return avg_ear
        except:
            return 0.3

    def _detect_blink(self, ear):
        """æ£€æµ‹çœ¨çœ¼å’Œé•¿æ—¶é—´é—­çœ¼ï¼ˆä½¿ç”¨å¯é…ç½®çš„EARé˜ˆå€¼ï¼‰"""
        ear_threshold = self.ear_threshold.get()
        current_time = time.time()

        if ear < ear_threshold:
            if self.eye_closed_frames == 0:
                self.eye_closed_start_time = current_time
            self.eye_closed_frames += 1
        else:
            if self.eye_closed_frames >= self.eye_closed_threshold:
                if current_time - self.last_blink_time > 0.3:
                    self.blink_count += 1
                    self.last_blink_time = current_time
                    self.recent_blinks.append(current_time)
                    return True

            self.eye_closed_frames = 0
            self.eye_closed_start_time = None

        return False

    def _evaluate_fatigue_status(self):
        """è¯„ä¼°ç–²åŠ³çŠ¶æ€ï¼ˆä¸PyQtç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰"""
        current_time = time.time()

        # æ¸…ç†è¿‡æœŸçš„è®°å½•ï¼ˆ30ç§’çª—å£ï¼‰
        self.recent_yawns = [t for t in self.recent_yawns if current_time - t <= self.fatigue_window]
        self.recent_blinks = [t for t in self.recent_blinks if current_time - t <= self.fatigue_window]

        yawn_count_30s = len(self.recent_yawns)  # 30ç§’çª—å£å†…çš„æ‰“å“ˆæ¬ æ¬¡æ•°
        long_eye_closed = self.eye_closed_frames >= self.long_eye_closed_threshold

        # ç–²åŠ³çŠ¶æ€åˆ¤æ–­é€»è¾‘ï¼ˆä¸PyQtç‰ˆæœ¬å®Œå…¨ä¸€è‡´ï¼‰
        if yawn_count_30s >= 3 or long_eye_closed:
            return "é‡åº¦ç–²åŠ³"
        elif yawn_count_30s >= 2:
            return "ä¸­åº¦ç–²åŠ³"
        elif yawn_count_30s >= 1:
            return "è½»åº¦ç–²åŠ³"
        else:
            return "æ­£å¸¸"

    def _draw_face_landmarks(self, frame, face_rect, landmarks_norm):
        """åœ¨äººè„¸ä¸Šç»˜åˆ¶ç‰¹å¾ç‚¹å’Œäººè„¸æ¡†ï¼ˆå¢å¼ºæ˜¾ç¤ºæ•ˆæœï¼‰"""
        if face_rect is None:
            print("âš ï¸  face_rectä¸ºNoneï¼Œè·³è¿‡ç»˜åˆ¶")
            return frame

        # ç»˜åˆ¶äººè„¸æ¡† - ä½¿ç”¨æ›´ç²—çš„çº¿æ¡å’Œæ›´äº®çš„é¢œè‰²
        x, y, w, h = face_rect.left(), face_rect.top(), face_rect.width(), face_rect.height()
        print(f"ğŸ¨ å¼€å§‹ç»˜åˆ¶äººè„¸æ¡†: ({x}, {y}, {w}, {h})")

        # ç»˜åˆ¶å¤šå±‚äººè„¸æ¡†ä»¥ç¡®ä¿å¯è§æ€§ - ä½¿ç”¨æ›´ç²—çš„çº¿æ¡
        cv2.rectangle(frame, (x-3, y-3), (x + w + 3, y + h + 3), (0, 255, 0), 6)  # å¤–å±‚ç»¿æ¡†ï¼ˆæ›´ç²—ï¼‰
        cv2.rectangle(frame, (x-1, y-1), (x + w + 1, y + h + 1), (255, 255, 255), 4)  # ä¸­å±‚ç™½æ¡†
        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # å†…å±‚ç»¿æ¡†
        print(f"âœ… äººè„¸æ¡†ç»˜åˆ¶å®Œæˆ")

        # å¦‚æœæœ‰å½’ä¸€åŒ–çš„landmarksï¼Œéœ€è¦è½¬æ¢å›åŸå§‹åæ ‡
        if landmarks_norm is not None and self.ai_available:
            print(f"ğŸ¯ å¼€å§‹ç»˜åˆ¶ç‰¹å¾ç‚¹ï¼ŒAIå¯ç”¨: {self.ai_available}")
            # é‡æ–°è·å–åŸå§‹landmarksæ¥ç»˜åˆ¶
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            faces = self.detector(gray)
            if len(faces) > 0:
                landmarks = self.predictor(gray, faces[0])
                print(f"ğŸ¯ è·å–åˆ°landmarksï¼Œå¼€å§‹ç»˜åˆ¶68ä¸ªç‰¹å¾ç‚¹")

                # ç»˜åˆ¶68ä¸ªç‰¹å¾ç‚¹ - ä½¿ç”¨æ›´å¤§æ›´æ˜æ˜¾çš„ç‚¹
                for i in range(68):
                    x_point = landmarks.part(i).x
                    y_point = landmarks.part(i).y

                    # æ ¹æ®ä¸åŒåŒºåŸŸä½¿ç”¨ä¸åŒé¢œè‰²å’Œå¤§å°
                    if i < 17:  # ä¸‹å·´è½®å»“
                        color = (255, 255, 0)  # é’è‰²
                        radius = 3
                    elif i < 22:  # å³çœ‰æ¯›
                        color = (0, 255, 255)  # é»„è‰²
                        radius = 3
                    elif i < 27:  # å·¦çœ‰æ¯›
                        color = (0, 255, 255)  # é»„è‰²
                        radius = 3
                    elif i < 36:  # é¼»å­
                        color = (255, 0, 255)  # ç´«è‰²
                        radius = 3
                    elif i < 42:  # å³çœ¼
                        color = (255, 0, 0)    # è“è‰²
                        radius = 4  # çœ¼éƒ¨ç‰¹å¾ç‚¹ç¨å¤§
                    elif i < 48:  # å·¦çœ¼
                        color = (255, 0, 0)    # è“è‰²
                        radius = 4  # çœ¼éƒ¨ç‰¹å¾ç‚¹ç¨å¤§
                    else:  # å˜´éƒ¨
                        color = (0, 0, 255)    # çº¢è‰²
                        radius = 4  # å˜´éƒ¨ç‰¹å¾ç‚¹ç¨å¤§

                    # ç»˜åˆ¶æ›´å¤§çš„ç‰¹å¾ç‚¹ï¼Œå¸¦é»‘è‰²è¾¹æ¡†å¢å¼ºå¯¹æ¯”åº¦
                    cv2.circle(frame, (x_point, y_point), radius + 2, (0, 0, 0), -1)  # é»‘è‰²åº•
                    cv2.circle(frame, (x_point, y_point), radius + 1, (255, 255, 255), -1)  # ç™½è‰²ä¸­å±‚
                    cv2.circle(frame, (x_point, y_point), radius, color, -1)  # å½©è‰²ç‚¹

                # ç»˜åˆ¶å…³é”®åŒºåŸŸçš„è¿çº¿ - ä½¿ç”¨æ›´ç²—çš„çº¿æ¡å’Œæ›´å¥½çš„å¯¹æ¯”åº¦
                # çœ¼éƒ¨è½®å»“
                for eye_start, eye_end in [(36, 42), (42, 48)]:
                    eye_points = []
                    for i in range(eye_start, eye_end):
                        eye_points.append((landmarks.part(i).x, landmarks.part(i).y))
                    eye_points = np.array(eye_points, np.int32)
                    cv2.polylines(frame, [eye_points], True, (0, 0, 0), 5)  # é»‘è‰²åº•çº¿
                    cv2.polylines(frame, [eye_points], True, (255, 255, 255), 3)  # ç™½è‰²ä¸­çº¿
                    cv2.polylines(frame, [eye_points], True, (255, 0, 0), 2)  # è“è‰²çº¿

                # å˜´éƒ¨è½®å»“
                mouth_points = []
                for i in range(48, 68):
                    mouth_points.append((landmarks.part(i).x, landmarks.part(i).y))
                mouth_points = np.array(mouth_points, np.int32)
                cv2.polylines(frame, [mouth_points], True, (0, 0, 0), 5)  # é»‘è‰²åº•çº¿
                cv2.polylines(frame, [mouth_points], True, (255, 255, 255), 3)  # ç™½è‰²ä¸­çº¿
                cv2.polylines(frame, [mouth_points], True, (0, 0, 255), 2)  # çº¢è‰²çº¿
        else:
            # å¦‚æœAIä¸å¯ç”¨ï¼Œåªç»˜åˆ¶äººè„¸æ¡†ï¼Œä¸æ·»åŠ æ–‡å­—æ ‡è¯†
            print(f"âš ï¸  AIä¸å¯ç”¨æˆ–landmarksä¸ºNoneï¼Œåªç»˜åˆ¶äººè„¸æ¡†")

        print(f"âœ… äººè„¸æ¡†å’Œç‰¹å¾ç‚¹ç»˜åˆ¶å®Œæˆ")

        return frame

    def _record_fatigue_status(self, fatigue_status):
        """è®°å½•ç–²åŠ³çŠ¶æ€åˆ°æ•°æ®åº“ï¼ˆä¸PyQtç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰"""
        # åªåœ¨ç–²åŠ³çŠ¶æ€å‘ç”Ÿå˜åŒ–æ—¶è®°å½•ï¼ˆä¸PyQtç‰ˆæœ¬é€»è¾‘ä¸€è‡´ï¼‰
        if fatigue_status != self.last_fatigue_status:
            if fatigue_status == "è½»åº¦ç–²åŠ³" and self.last_fatigue_status == "æ­£å¸¸":
                self._save_fatigue_record("è½»åº¦ç–²åŠ³")
                print("âš ï¸ è½»åº¦ç–²åŠ³è­¦å‘Š")
            elif fatigue_status == "ä¸­åº¦ç–²åŠ³" and self.last_fatigue_status in ["æ­£å¸¸", "è½»åº¦ç–²åŠ³"]:
                self._save_fatigue_record("ä¸­åº¦ç–²åŠ³")
                print("âš ï¸âš ï¸ ä¸­åº¦ç–²åŠ³è­¦å‘Š")
            elif fatigue_status == "é‡åº¦ç–²åŠ³" and self.last_fatigue_status in ["æ­£å¸¸", "è½»åº¦ç–²åŠ³", "ä¸­åº¦ç–²åŠ³"]:
                self._save_fatigue_record("é‡åº¦ç–²åŠ³")
                print("ğŸš¨ é‡åº¦ç–²åŠ³è­¦å‘Š")
            elif fatigue_status == "æ­£å¸¸":
                print("âœ… ç–²åŠ³çŠ¶æ€æ¢å¤æ­£å¸¸")

            # æ’­æ”¾è­¦å‘ŠéŸ³é¢‘ï¼ˆå¦‚æœçŠ¶æ€å˜åŒ–ä¸”éæ­£å¸¸ï¼‰
            if fatigue_status != "æ­£å¸¸":
                self._play_warning_sound()

            # æ›´æ–°ä¸Šä¸€æ¬¡ç–²åŠ³çŠ¶æ€
            self.last_fatigue_status = fatigue_status

    def _save_fatigue_record(self, fatigue_level):
        """ä¿å­˜ç–²åŠ³è®°å½•åˆ°æ•°æ®åº“ï¼ˆä¸PyQtç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰"""
        if not self.current_user:
            return

        try:
            if AI_MODULES_AVAILABLE:
                with get_db_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute('''
                        INSERT INTO fatigue_records
                        (username, timestamp, fatigue_level)
                        VALUES (%s, %s, %s)
                    ''', (
                        self.current_user['username'],
                        datetime.now(),
                        fatigue_level
                    ))
                    conn.commit()

                print(f"ğŸ’¾ ç–²åŠ³è®°å½•å·²ä¿å­˜: {self.current_user['username']} - {fatigue_level}")
            else:
                print(f"ğŸ’¾ ç–²åŠ³è®°å½•ï¼ˆæ¨¡æ‹Ÿï¼‰: {self.current_user['username']} - {fatigue_level}")

        except Exception as e:
            print(f"âŒ ä¿å­˜ç–²åŠ³è®°å½•å¤±è´¥: {e}")

    def apply_preset(self, mode):
        """åº”ç”¨é¢„è®¾æ¨¡å¼ï¼ˆEARé˜ˆå€¼ä¿æŒä¸å˜ï¼‰"""
        if mode == 'sensitive':
            self.yawn_threshold.set(0.6)  # ä¿æŒæ¨¡å‹é˜ˆå€¼ä¸å˜
            self.mar_threshold.set(0.45)  # MARé˜ˆå€¼è°ƒæ•´ä¸º0.45
            # EARé˜ˆå€¼ä¿æŒä¸å˜ï¼Œç»Ÿä¸€ä¸º0.18
            self.consecutive_threshold = 10  # è¿ç»­æ£€æµ‹é˜ˆå€¼10å¸§
            self.alert_cooldown.set(3.0)
            self.current_mode = "æ•æ„Ÿæ¨¡å¼"
        elif mode == 'balanced':
            self.yawn_threshold.set(0.6)  # ä¿æŒæ¨¡å‹é˜ˆå€¼ä¸å˜
            self.mar_threshold.set(0.5)   # MARé˜ˆå€¼è°ƒæ•´ä¸º0.5
            # EARé˜ˆå€¼ä¿æŒä¸å˜ï¼Œç»Ÿä¸€ä¸º0.18
            self.consecutive_threshold = 15  # è¿ç»­æ£€æµ‹é˜ˆå€¼15å¸§
            self.alert_cooldown.set(5.0)
            self.current_mode = "å¹³è¡¡æ¨¡å¼"
        elif mode == 'conservative':
            self.yawn_threshold.set(0.6)  # ä¿æŒæ¨¡å‹é˜ˆå€¼ä¸å˜
            self.mar_threshold.set(0.55)  # MARé˜ˆå€¼è°ƒæ•´ä¸º0.55
            # EARé˜ˆå€¼ä¿æŒä¸å˜ï¼Œç»Ÿä¸€ä¸º0.18
            self.consecutive_threshold = 20  # è¿ç»­æ£€æµ‹é˜ˆå€¼20å¸§
            self.alert_cooldown.set(8.0)
            self.current_mode = "ä¿å®ˆæ¨¡å¼"

    def start_detection(self):
        """å¼€å§‹æ£€æµ‹"""
        if self.is_detecting:
            return False

        self.is_detecting = True
        self.session_start_time = time.time()

        # é‡ç½®AIæ£€æµ‹ç»Ÿè®¡æ•°æ®
        self.yawn_count = 0
        self.blink_count = 0
        self.total_predictions = 0
        self.consecutive_yawns = 0
        self.last_yawn_time = 0
        self.last_detection_time = 0
        self.no_detection_frames = 0

        # é‡ç½®ç¼“å†²åŒº
        self.face_buffer.clear()
        self.landmark_buffer.clear()

        # é‡ç½®çœ¨çœ¼æ£€æµ‹
        self.eye_closed_frames = 0
        self.eye_closed_start_time = None
        self.last_blink_time = 0
        self.recent_blinks = []
        self.recent_yawns = []

        # é‡ç½®å‹ç¼©ç»Ÿè®¡
        self.compression_stats = {
            'total_original_size': 0,
            'total_compressed_size': 0,
            'frames_processed': 0,
            'avg_compression_ratio': 0
        }

        # å¯åŠ¨å¤„ç†çº¿ç¨‹
        self.processing_thread = threading.Thread(target=self._processing_loop, daemon=True)
        self.processing_thread.start()

        print("âœ… æ£€æµ‹å·²å¼€å§‹ - AI + FFmpegä¼˜åŒ–æ¨¡å¼")
        return True

    def stop_detection(self):
        """åœæ­¢æ£€æµ‹"""
        self.is_detecting = False
        
        # æ¸…ç©ºé˜Ÿåˆ—
        while not self.frame_queue.empty():
            try:
                self.frame_queue.get_nowait()
            except Empty:
                break
                
        print("âœ… æ£€æµ‹å·²åœæ­¢")

    def add_frame(self, frame_data):
        """æ·»åŠ å¸§åˆ°å¤„ç†é˜Ÿåˆ—"""
        if not self.is_detecting:
            return False
            
        try:
            if self.frame_queue.full():
                try:
                    self.frame_queue.get_nowait()  # ç§»é™¤æœ€è€çš„å¸§
                except Empty:
                    pass
            
            self.frame_queue.put_nowait(frame_data)
            return True
        except:
            return False

    def _processing_loop(self):
        """å¤„ç†å¾ªç¯ - é›†æˆå®Œæ•´AIæ£€æµ‹"""
        compression_methods = ['fast', 'webp', 'adaptive']
        current_method_index = 0

        while self.is_detecting:
            try:
                # è·å–å¸§æ•°æ®
                frame_data = None
                try:
                    frame_data = self.frame_queue.get(timeout=0.5)
                except Empty:
                    continue

                # æ‰§è¡Œå®Œæ•´çš„AIæ£€æµ‹æµç¨‹
                detection_result = self.process_frame_with_ai(frame_data)

                if detection_result:
                    # æ›´æ–°æœ€æ–°ç»“æœ
                    self.latest_results.update(detection_result)
                    self.latest_results['compression_stats'] = self.compression_stats.copy()

                    # åŠ¨æ€è°ƒæ•´å‹ç¼©æ–¹æ³•ï¼ˆåŸºäºæ€§èƒ½ï¼‰
                    if self.compression_stats['frames_processed'] % 50 == 0:
                        self._optimize_compression_method()

            except Exception as e:
                print(f"å¤„ç†å¾ªç¯é”™è¯¯: {e}")
                time.sleep(0.1)

    def process_frame_with_ai(self, frame_data):
        """å¤„ç†è§†é¢‘å¸§å¹¶è¿›è¡ŒAIæ£€æµ‹ï¼ˆå®Œæ•´æ£€æµ‹é€»è¾‘ï¼‰"""
        try:
            # å°†base64æ•°æ®è½¬æ¢ä¸ºOpenCVå›¾åƒ
            if frame_data.startswith('data:image'):
                frame_data = frame_data.split(',')[1]

            # è§£ç base64æ•°æ®
            img_data = base64.b64decode(frame_data)
            nparr = np.frombuffer(img_data, np.uint8)
            frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

            if frame is None:
                return None

            # å…ˆè¿›è¡ŒFFmpegå‹ç¼©
            compressed_result = self.video_processor.compress_frame_fast(frame_data)
            if compressed_result:
                self._update_compression_stats(compressed_result)

            # å¦‚æœAIä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ£€æµ‹
            if not self.ai_available:
                return self._simulate_detection(frame)

            # æ‰§è¡ŒAIæ£€æµ‹é€»è¾‘ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
            face_img, landmarks_norm, face_rect = self._preprocess_frame(frame)
            face_detected = face_img is not None

            # è·å–åŸå§‹landmarksç”¨äºMAR/EARè®¡ç®—
            original_landmarks = None
            if face_detected:
                original_face_img, original_landmarks = extract_face_landmarks(frame, self.detector, self.predictor)
                # ç«‹å³ç»˜åˆ¶äººè„¸æ¡†å’Œç‰¹å¾ç‚¹
                print(f"ğŸ¨ ç»˜åˆ¶äººè„¸æ¡†å’Œç‰¹å¾ç‚¹ï¼Œäººè„¸åŒºåŸŸ: {face_rect.left()}, {face_rect.top()}, {face_rect.width()}, {face_rect.height()}")
                frame = self._draw_face_landmarks(frame, face_rect, landmarks_norm)
                print(f"âœ… äººè„¸æ¡†å’Œç‰¹å¾ç‚¹ç»˜åˆ¶å®Œæˆ")

            yawn_prob = 0.0
            prediction = 0

            if face_detected:
                self.face_buffer.append(face_img)
                self.landmark_buffer.append(landmarks_norm)

                # å¦‚æœç¼“å†²åŒºæ»¡äº†ï¼Œè¿›è¡Œé¢„æµ‹ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
                if len(self.face_buffer) >= SEQUENCE_LENGTH:
                    yawn_prob, model_prediction = self._predict_yawn()
                    self.total_predictions += 1

                    # è®¡ç®—å½“å‰å¸§çš„å˜´éƒ¨é•¿å®½æ¯”å’Œçœ¼éƒ¨é•¿å®½æ¯”ï¼ˆä½¿ç”¨åŸå§‹landmarksï¼‰
                    current_mar = self._calculate_mouth_aspect_ratio(original_landmarks)
                    current_ear = self._calculate_eye_aspect_ratio(original_landmarks)

                    # æ£€æµ‹çœ¨çœ¼ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
                    blink_detected = self._detect_blink(current_ear)

                    # æ–°çš„æ£€æµ‹é€»è¾‘ï¼šæ¨¡å‹é¢„æµ‹ + MARé˜ˆå€¼çš„ç»„åˆåˆ¤æ–­ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
                    model_says_yawn = yawn_prob > self.yawn_threshold.get()
                    mar_says_yawn = current_mar > self.mar_threshold.get()

                    # æœ€ç»ˆåˆ¤æ–­ï¼šä¸¤ä¸ªæ¡ä»¶éƒ½æ»¡è¶³æ‰è®¤ä¸ºæ˜¯æ‰“å“ˆæ¬ ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
                    final_prediction = 1 if (model_says_yawn and mar_says_yawn) else 0

                    # æ›´æ–°è¿ç»­æ£€æµ‹è®¡æ•° - ä½¿ç”¨å¹³æ»‘è¡°å‡æœºåˆ¶ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
                    current_time = time.time()
                    if final_prediction == 1:
                        # æ£€æµ‹åˆ°æ‰“å“ˆæ¬ ï¼šå¢åŠ è®¡æ•°ï¼Œæ›´æ–°æœ€åæ£€æµ‹æ—¶é—´
                        self.consecutive_yawns += 1
                        self.last_detection_time = current_time
                        self.no_detection_frames = 0  # é‡ç½®æœªæ£€æµ‹å¸§æ•°
                        print(f"ğŸ” æ‰“å“ˆæ¬ æ£€æµ‹: æ¨¡å‹={yawn_prob:.3f}({'âœ“' if model_says_yawn else 'âœ—'}), MAR={current_mar:.3f}({'âœ“' if mar_says_yawn else 'âœ—'}), è¿ç»­={self.consecutive_yawns}")
                    else:
                        # æœªæ£€æµ‹åˆ°æ‰“å“ˆæ¬ ï¼šä½¿ç”¨å¹³æ»‘è¡°å‡
                        self.no_detection_frames += 1

                        # å¦‚æœæœ‰ä¹‹å‰çš„æ£€æµ‹è®°å½•ï¼Œåˆ™å¼€å§‹è¡°å‡
                        if self.consecutive_yawns > 0:
                            # è®¡ç®—è¡°å‡é‡ï¼šåŸºäºæ—¶é—´çš„è¡°å‡
                            if self.last_detection_time > 0:
                                time_since_last = current_time - self.last_detection_time
                                # æ¯ç§’è¡°å‡decay_rateå¸§ï¼Œä½†è‡³å°‘ä¿æŒ1ç§’ä¸è¡°å‡
                                if time_since_last > 1.0:  # 1ç§’åå¼€å§‹è¡°å‡
                                    decay_amount = int((time_since_last - 1.0) * self.decay_rate)
                                    self.consecutive_yawns = max(0, self.consecutive_yawns - decay_amount)

                                    if self.consecutive_yawns == 0:
                                        print(f"ğŸ“‰ è¿›åº¦æ¡è¡°å‡è‡³é›¶ï¼ˆæœªæ£€æµ‹{self.no_detection_frames}å¸§ï¼Œæ—¶é—´é—´éš”{time_since_last:.1f}ç§’ï¼‰")
                                    else:
                                        print(f"ğŸ“‰ è¿›åº¦æ¡è¡°å‡: {self.consecutive_yawns}ï¼ˆæœªæ£€æµ‹{self.no_detection_frames}å¸§ï¼‰")
                            else:
                                # å¦‚æœæ²¡æœ‰æ—¶é—´è®°å½•ï¼Œç«‹å³å¼€å§‹è¡°å‡
                                if self.no_detection_frames > 30:  # 30å¸§åå¼€å§‹è¡°å‡ï¼ˆçº¦1ç§’ï¼‰
                                    self.consecutive_yawns = max(0, self.consecutive_yawns - 1)
                        else:
                            # å¦‚æœconsecutive_yawnså·²ç»æ˜¯0ï¼Œä¿æŒä¸º0
                            self.consecutive_yawns = 0

                    # æ£€æŸ¥æ˜¯å¦è§¦å‘è­¦æŠ¥ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
                    if (self.consecutive_yawns >= self.consecutive_threshold and
                        (current_time - self.last_yawn_time) > self.alert_cooldown.get()):
                        self.yawn_count += 1
                        self.last_yawn_time = current_time
                        self.recent_yawns.append(current_time)
                        print(f"ğŸš¨ è§¦å‘è­¦æŠ¥ï¼è¿ç»­{self.consecutive_yawns}å¸§æ£€æµ‹åˆ°æ‰“å“ˆæ¬ ")
                        self._play_warning_sound()

                    # æ›´æ–°predictionå˜é‡ç”¨äºGUIæ˜¾ç¤º
                    prediction = final_prediction

                # äººè„¸æ¡†å’Œç‰¹å¾ç‚¹å·²åœ¨å‰é¢ç»˜åˆ¶
            else:
                # æœªæ£€æµ‹åˆ°äººè„¸æ—¶çš„è¡°å‡é€»è¾‘ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
                if self.consecutive_yawns > 0:
                    current_time = time.time()
                    self.no_detection_frames += 1

                    # å¦‚æœæœ‰ä¹‹å‰çš„æ£€æµ‹è®°å½•ï¼Œåˆ™å¼€å§‹è¡°å‡
                    if self.last_detection_time > 0:
                        time_since_last = current_time - self.last_detection_time
                        # æœªæ£€æµ‹åˆ°äººè„¸æ—¶ï¼Œè¡°å‡æ›´å¿«ä¸€äº›
                        if time_since_last > 0.5:  # 0.5ç§’åå¼€å§‹è¡°å‡
                            decay_amount = int((time_since_last - 0.5) * self.decay_rate * 1.5)  # è¡°å‡é€Ÿåº¦1.5å€
                            old_consecutive = self.consecutive_yawns
                            self.consecutive_yawns = max(0, self.consecutive_yawns - decay_amount)

                            if old_consecutive != self.consecutive_yawns:
                                if self.consecutive_yawns == 0:
                                    print(f"ğŸ“‰ æœªæ£€æµ‹åˆ°äººè„¸ï¼Œè¿›åº¦æ¡è¡°å‡è‡³é›¶ï¼ˆæœªæ£€æµ‹{self.no_detection_frames}å¸§ï¼‰")
                                else:
                                    print(f"ğŸ“‰ æœªæ£€æµ‹åˆ°äººè„¸ï¼Œè¿›åº¦æ¡è¡°å‡: {self.consecutive_yawns}")
                    else:
                        # å¦‚æœæ²¡æœ‰æ—¶é—´è®°å½•ï¼Œè¾ƒå¿«è¡°å‡
                        if self.no_detection_frames > 15:  # 15å¸§åå¼€å§‹è¡°å‡ï¼ˆçº¦0.5ç§’ï¼‰
                            self.consecutive_yawns = max(0, self.consecutive_yawns - 1)

            # è¯„ä¼°ç–²åŠ³çŠ¶æ€
            fatigue_status = self._evaluate_fatigue_status()

            # è®°å½•ç–²åŠ³çŠ¶æ€åˆ°æ•°æ®åº“
            self._record_fatigue_status(fatigue_status)

            # æ›´æ–°æœ€æ–°ç»“æœç”¨äºWebæ˜¾ç¤º
            return self._update_latest_results(frame, face_detected, yawn_prob, prediction, fatigue_status)

        except Exception as e:
            print(f"âŒ AIæ£€æµ‹å¤„ç†é”™è¯¯: {e}")
            return None

    def _update_latest_results(self, frame, face_detected, yawn_prob, prediction, fatigue_status):
        """æ›´æ–°æœ€æ–°çš„æ£€æµ‹ç»“æœç”¨äºWebæ˜¾ç¤º"""
        # æ³¨æ„ï¼šframeå·²ç»åŒ…å«äº†äººè„¸æ¡†å’Œç‰¹å¾ç‚¹ï¼ˆåœ¨_draw_face_landmarksä¸­ç»˜åˆ¶ï¼‰
        # ä¸å†æ·»åŠ é¢å¤–çš„æ–‡æœ¬ä¿¡æ¯ï¼Œåªä¿ç•™äººè„¸æ¡†å’Œç‰¹å¾ç‚¹

        # å°†frameè½¬æ¢ä¸ºbase64ç”¨äºWebæ˜¾ç¤ºï¼Œä½¿ç”¨æ›´é«˜è´¨é‡
        _, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 90])
        frame_base64 = base64.b64encode(buffer).decode('utf-8')

        # è®¡ç®—ä¼šè¯æ—¶é—´
        session_time = "00:00"
        if self.session_start_time:
            elapsed = int(time.time() - self.session_start_time)
            minutes = elapsed // 60
            seconds = elapsed % 60
            session_time = f"{minutes:02d}:{seconds:02d}"

        return {
            'frame': frame_base64,
            'face_detected': face_detected,
            'yawn_prob': round(yawn_prob, 3),
            'prediction': prediction,
            'mar': round(self._last_mar, 3),
            'ear': round(self._last_ear, 3),
            'fatigue_status': fatigue_status,
            'consecutive_yawns': self.consecutive_yawns,
            'session_time': session_time,
            'buffer_status': f"{min(SEQUENCE_LENGTH, len(self.face_buffer))}/{SEQUENCE_LENGTH}",
            'total_predictions': self.total_predictions,
            'yawn_count': self.yawn_count,
            'blink_count': self.blink_count,
            'progress': min(100, int((self.consecutive_yawns / self.consecutive_threshold) * 100))
        }

    def _simulate_detection(self, frame):
        """æ¨¡æ‹ŸAIæ£€æµ‹ï¼ˆå½“AIæ¨¡å—ä¸å¯ç”¨æ—¶ï¼‰"""
        import random

        # æ¨¡æ‹Ÿæ£€æµ‹ç»“æœ
        face_detected = random.choice([True, False, True, True])  # 75%æ¦‚ç‡æ£€æµ‹åˆ°äººè„¸
        yawn_prob = random.uniform(0.0, 1.0)
        prediction = 1 if yawn_prob > 0.7 else 0

        # æ›´æ–°ç»Ÿè®¡
        self.total_predictions += 1
        if prediction == 1:
            self.yawn_count += 1

        # æ¨¡æ‹Ÿçœ¨çœ¼æ£€æµ‹
        if random.random() < 0.1:  # 10%æ¦‚ç‡æ£€æµ‹åˆ°çœ¨çœ¼
            self.blink_count += 1

        # æ¨¡æ‹Ÿç–²åŠ³çŠ¶æ€
        if self.yawn_count >= 3:
            fatigue_status = "ä¸­åº¦ç–²åŠ³"
        elif self.yawn_count >= 1:
            fatigue_status = "è½»åº¦ç–²åŠ³"
        else:
            fatigue_status = "æ­£å¸¸"

        # åœ¨frameä¸Šç»˜åˆ¶æ¨¡æ‹Ÿæ£€æµ‹æ¡†å’Œç‰¹å¾ç‚¹
        if face_detected:
            h, w = frame.shape[:2]
            # ç»˜åˆ¶äººè„¸æ¡†
            face_x, face_y = w//4, h//4
            face_w, face_h = w//2, h//2
            cv2.rectangle(frame, (face_x, face_y), (face_x + face_w, face_y + face_h), (0, 255, 0), 3)

            # ç»˜åˆ¶æ¨¡æ‹Ÿç‰¹å¾ç‚¹
            # çœ¼éƒ¨åŒºåŸŸ
            eye_y = face_y + face_h//3
            left_eye_x = face_x + face_w//4
            right_eye_x = face_x + 3*face_w//4

            # å·¦çœ¼
            for i in range(6):
                angle = i * 60 * np.pi / 180
                x = int(left_eye_x + 15 * np.cos(angle))
                y = int(eye_y + 8 * np.sin(angle))
                cv2.circle(frame, (x, y), 2, (255, 0, 0), -1)

            # å³çœ¼
            for i in range(6):
                angle = i * 60 * np.pi / 180
                x = int(right_eye_x + 15 * np.cos(angle))
                y = int(eye_y + 8 * np.sin(angle))
                cv2.circle(frame, (x, y), 2, (255, 0, 0), -1)

            # å˜´éƒ¨åŒºåŸŸ
            mouth_y = face_y + 2*face_h//3
            mouth_x = face_x + face_w//2

            # å˜´éƒ¨è½®å»“
            for i in range(8):
                angle = i * 45 * np.pi / 180
                x = int(mouth_x + 20 * np.cos(angle))
                y = int(mouth_y + 10 * np.sin(angle))
                cv2.circle(frame, (x, y), 2, (0, 0, 255), -1)

            # é¼»å­
            nose_y = face_y + face_h//2
            nose_x = face_x + face_w//2
            cv2.circle(frame, (nose_x, nose_y), 3, (255, 0, 255), -1)
            cv2.circle(frame, (nose_x-5, nose_y+5), 2, (255, 0, 255), -1)
            cv2.circle(frame, (nose_x+5, nose_y+5), 2, (255, 0, 255), -1)

        # è®°å½•ç–²åŠ³çŠ¶æ€åˆ°æ•°æ®åº“
        self._record_fatigue_status(fatigue_status)

        return self._update_latest_results(frame, face_detected, yawn_prob, prediction, fatigue_status)

    def _optimize_compression_method(self):
        """æ ¹æ®æ€§èƒ½åŠ¨æ€ä¼˜åŒ–å‹ç¼©æ–¹æ³•"""
        try:
            avg_ratio = self.compression_stats['avg_compression_ratio']

            # å¦‚æœå‹ç¼©æ¯”ä¸å¤Ÿå¥½ï¼Œåˆ‡æ¢åˆ°æ›´æ¿€è¿›çš„å‹ç¼©
            if avg_ratio < 70:  # å‹ç¼©æ¯”ä½äº70%
                new_crf = min(35, int(self.video_processor.compression_settings['crf']) + 2)
                self.video_processor.compression_settings['crf'] = str(new_crf)
                print(f"ğŸ”§ è‡ªåŠ¨ä¼˜åŒ–: æé«˜å‹ç¼©æ¯”ï¼ŒCRFè°ƒæ•´ä¸º {new_crf}")

            # å¦‚æœå‹ç¼©æ¯”å¤ªé«˜å¯èƒ½å½±å“è´¨é‡ï¼Œé€‚å½“é™ä½
            elif avg_ratio > 90:  # å‹ç¼©æ¯”é«˜äº90%
                new_crf = max(18, int(self.video_processor.compression_settings['crf']) - 1)
                self.video_processor.compression_settings['crf'] = str(new_crf)
                print(f"ğŸ”§ è‡ªåŠ¨ä¼˜åŒ–: ä¿æŒè´¨é‡ï¼ŒCRFè°ƒæ•´ä¸º {new_crf}")

        except Exception as e:
            print(f"ä¼˜åŒ–å‹ç¼©æ–¹æ³•å¤±è´¥: {e}")

    def _update_compression_stats(self, result):
        """æ›´æ–°å‹ç¼©ç»Ÿè®¡"""
        self.compression_stats['total_original_size'] += result['original_size']
        self.compression_stats['total_compressed_size'] += result['compressed_size']
        self.compression_stats['frames_processed'] += 1
        
        if self.compression_stats['total_original_size'] > 0:
            self.compression_stats['avg_compression_ratio'] = (
                (1 - self.compression_stats['total_compressed_size'] / 
                 self.compression_stats['total_original_size']) * 100
            )



    def get_latest_results(self):
        """è·å–æœ€æ–°ç»“æœ"""
        return self.latest_results.copy()

    def update_compression_settings(self, settings):
        """æ›´æ–°å‹ç¼©è®¾ç½®"""
        self.video_processor.update_settings(settings)

# å…¨å±€æ£€æµ‹ç³»ç»Ÿå®ä¾‹
detection_system = OptimizedFatigueDetectionSystem(MODEL_PATH)

# åœ¨åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–æ•°æ®åº“
@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨äº‹ä»¶"""
    if AI_MODULES_AVAILABLE:
        try:
            init_database()
            print("âœ… æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸  æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
    else:
        print("âš ï¸  è·³è¿‡æ•°æ®åº“åˆå§‹åŒ–ï¼ˆAIæ¨¡å—ä¸å¯ç”¨ï¼‰")

# è·¯ç”±å®šä¹‰
@app.get("/", response_class=HTMLResponse)
async def login_page(request: Request):
    return templates.TemplateResponse("login.html", {"request": request})

@app.post("/login")
async def login(username: str = Form(...), password: str = Form(...)):
    """ç”¨æˆ·ç™»å½• - ä½¿ç”¨æ•°æ®åº“è®¤è¯"""
    try:
        if AI_MODULES_AVAILABLE:
            # ä½¿ç”¨æ•°æ®åº“è®¤è¯
            with get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT username, password FROM users WHERE username = %s", (username,))
                user = cursor.fetchone()

            if user and user[1] == password:
                detection_system.current_user = {
                    'username': user[0],
                    'full_name': user[0]
                }
                return RedirectResponse(url="/dashboard", status_code=302)
            else:
                return JSONResponse({"success": False, "message": "ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯"})
        else:
            # å¦‚æœæ•°æ®åº“ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–è®¤è¯
            simple_users = {"test": "123456", "admin": "admin"}
            if username in simple_users and simple_users[username] == password:
                detection_system.current_user = {
                    'username': username,
                    'full_name': username
                }
                return RedirectResponse(url="/dashboard", status_code=302)
            else:
                return JSONResponse({"success": False, "message": "ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯"})

    except Exception as e:
        print(f"âŒ ç™»å½•å¤±è´¥: {e}")
        return JSONResponse({"success": False, "message": f"ç™»å½•å¤±è´¥: {e}"})

@app.get("/register", response_class=HTMLResponse)
async def register_page(request: Request):
    """æ³¨å†Œé¡µé¢"""
    return templates.TemplateResponse("register.html", {"request": request})

@app.post("/register")
async def register(username: str = Form(...), password: str = Form(...), confirm_password: str = Form(...)):
    """ç”¨æˆ·æ³¨å†Œ - ä½¿ç”¨æ•°æ®åº“å­˜å‚¨"""
    if len(password) < 6:
        return JSONResponse({"success": False, "message": "å¯†ç é•¿åº¦è‡³å°‘6ä½"})

    if password != confirm_password:
        return JSONResponse({"success": False, "message": "ä¸¤æ¬¡è¾“å…¥çš„å¯†ç ä¸ä¸€è‡´"})

    try:
        if AI_MODULES_AVAILABLE:
            # ä½¿ç”¨æ•°æ®åº“å­˜å‚¨
            with get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("INSERT INTO users (username, password) VALUES (%s, %s)", (username, password))
                conn.commit()

            return JSONResponse({"success": True, "message": f"ç”¨æˆ· {username} æ³¨å†ŒæˆåŠŸï¼"})
        else:
            # å¦‚æœæ•°æ®åº“ä¸å¯ç”¨ï¼Œè¿”å›æç¤º
            return JSONResponse({"success": False, "message": "æ•°æ®åº“ä¸å¯ç”¨ï¼Œæ— æ³•æ³¨å†Œæ–°ç”¨æˆ·"})

    except Exception as e:
        if "Duplicate entry" in str(e):
            return JSONResponse({"success": False, "message": "ç”¨æˆ·åå·²å­˜åœ¨ï¼Œè¯·é€‰æ‹©å…¶ä»–ç”¨æˆ·å"})
        else:
            return JSONResponse({"success": False, "message": f"æ³¨å†Œå¤±è´¥: {e}"})

@app.get("/dashboard", response_class=HTMLResponse)
async def dashboard(request: Request):
    if not detection_system.current_user:
        return RedirectResponse(url="/", status_code=302)
    return templates.TemplateResponse("dashboard_backend_ffmpeg.html", {
        "request": request,
        "user": detection_system.current_user
    })

@app.post("/api/start_detection")
async def start_detection():
    if detection_system.start_detection():
        return JSONResponse({"success": True, "message": "æ£€æµ‹å·²å¼€å§‹"})
    else:
        return JSONResponse({"success": False, "message": "å¯åŠ¨æ£€æµ‹å¤±è´¥"})

@app.post("/api/stop_detection")
async def stop_detection():
    detection_system.stop_detection()
    return JSONResponse({"success": True, "message": "æ£€æµ‹å·²åœæ­¢"})

@app.post("/api/update_compression")
async def update_compression(
    preset: str = Form(...),
    crf: str = Form(...),
    scale: str = Form(...),
    fps: str = Form(...)
):
    """æ›´æ–°å‹ç¼©è®¾ç½®"""
    settings = {
        'preset': preset,
        'crf': crf,
        'scale': scale,
        'fps': fps
    }
    detection_system.update_compression_settings(settings)
    return JSONResponse({"success": True, "message": "å‹ç¼©è®¾ç½®å·²æ›´æ–°"})

@app.get("/api/compression_stats")
async def get_compression_stats():
    """è·å–å‹ç¼©ç»Ÿè®¡"""
    return JSONResponse(detection_system.compression_stats)

@app.post("/api/reset_stats")
async def reset_stats():
    """é‡ç½®ç»Ÿè®¡æ•°æ®"""
    detection_system.compression_stats = {
        'total_original_size': 0,
        'total_compressed_size': 0,
        'frames_processed': 0,
        'avg_compression_ratio': 0
    }
    return JSONResponse({"success": True, "message": "ç»Ÿè®¡æ•°æ®å·²é‡ç½®"})

@app.get("/api/system_info")
async def get_system_info():
    """è·å–ç³»ç»Ÿä¿¡æ¯"""
    import psutil
    import platform

    try:
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')

        system_info = {
            "platform": platform.system(),
            "platform_version": platform.version(),
            "python_version": platform.python_version(),
            "cpu_count": psutil.cpu_count(),
            "cpu_usage": cpu_percent,
            "memory_total": memory.total,
            "memory_used": memory.used,
            "memory_percent": memory.percent,
            "disk_total": disk.total,
            "disk_used": disk.used,
            "disk_percent": (disk.used / disk.total) * 100,
            "ffmpeg_available": detection_system.use_ffmpeg
        }

        return JSONResponse(system_info)
    except Exception as e:
        return JSONResponse({"error": str(e)})

@app.post("/api/benchmark")
async def run_benchmark():
    """è¿è¡Œå‹ç¼©æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    try:
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        test_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
        _, buffer = cv2.imencode('.png', test_image)
        test_data = base64.b64encode(buffer).decode('utf-8')

        # æµ‹è¯•ä¸åŒå‹ç¼©æ–¹æ³•
        methods = ['fast', 'webp', 'adaptive']
        results = {}

        for method in methods:
            start_time = time.time()

            if method == 'fast':
                result = detection_system.video_processor.compress_frame_fast(f"data:image/png;base64,{test_data}")
            elif method == 'webp':
                result = detection_system.video_processor.compress_frame_webp(f"data:image/png;base64,{test_data}")
            elif method == 'adaptive':
                result = detection_system.video_processor.compress_frame_adaptive(f"data:image/png;base64,{test_data}")

            end_time = time.time()

            if result:
                results[method] = {
                    "compression_ratio": result['compression_ratio'],
                    "processing_time": (end_time - start_time) * 1000,  # ms
                    "original_size": result['original_size'],
                    "compressed_size": result['compressed_size']
                }
            else:
                results[method] = {"error": "å‹ç¼©å¤±è´¥"}

        return JSONResponse({
            "success": True,
            "benchmark_results": results,
            "test_image_size": f"{test_image.shape[1]}x{test_image.shape[0]}"
        })

    except Exception as e:
        return JSONResponse({"success": False, "error": str(e)})

@app.get("/api/performance_tips")
async def get_performance_tips():
    """è·å–æ€§èƒ½ä¼˜åŒ–å»ºè®®"""
    tips = []

    # åŸºäºå½“å‰ç»Ÿè®¡ç»™å‡ºå»ºè®®
    if detection_system.compression_stats['frames_processed'] > 0:
        avg_ratio = detection_system.compression_stats['avg_compression_ratio']

        if avg_ratio < 50:
            tips.append("å‹ç¼©æ¯”è¾ƒä½ï¼Œå»ºè®®æé«˜CRFå€¼æˆ–é™ä½åˆ†è¾¨ç‡")
        elif avg_ratio > 85:
            tips.append("å‹ç¼©æ¯”å¾ˆé«˜ï¼Œå¯èƒ½å½±å“æ£€æµ‹ç²¾åº¦ï¼Œå»ºè®®é€‚å½“é™ä½å‹ç¼©")

        if detection_system.compression_stats['frames_processed'] > 100:
            tips.append("ç³»ç»Ÿè¿è¡Œç¨³å®šï¼Œå¯ä»¥å°è¯•æé«˜å¸§ç‡")

    # ç³»ç»Ÿç›¸å…³å»ºè®®
    if not detection_system.use_ffmpeg:
        tips.append("å»ºè®®å®‰è£…FFmpegä»¥è·å¾—æ›´å¥½çš„å‹ç¼©æ•ˆæœ")

    tips.extend([
        "åœ¨ç½‘ç»œå¸¦å®½æœ‰é™æ—¶ï¼Œé€‰æ‹©ä¿å®ˆæ¨¡å¼",
        "åœ¨æœ¬åœ°ç½‘ç»œç¯å¢ƒä¸‹ï¼Œå¯ä»¥é€‰æ‹©æ•æ„Ÿæ¨¡å¼è·å¾—æ›´å¥½çš„æ£€æµ‹ç²¾åº¦",
        "å®šæœŸé‡ç½®ç»Ÿè®¡æ•°æ®ä»¥è·å¾—å‡†ç¡®çš„æ€§èƒ½æŒ‡æ ‡"
    ])

    return JSONResponse({"tips": tips})

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """ä¼˜åŒ–çš„WebSocketå¤„ç†"""
    await websocket.accept()
    print("WebSocketè¿æ¥å·²å»ºç«‹")
    
    last_send_time = 0
    send_interval = 0.2  # 200mså‘é€é—´éš”
    
    try:
        while True:
            try:
                # æ¥æ”¶æ¶ˆæ¯
                data = await asyncio.wait_for(websocket.receive_json(), timeout=0.1)
                
                if data.get("type") == "video_frame" and detection_system.is_detecting:
                    # åç«¯ç›´æ¥å¤„ç†åŸå§‹è§†é¢‘å¸§å¹¶è¿›è¡Œå‹ç¼©
                    frame_data = data.get("frame")
                    if frame_data:
                        detection_system.add_frame(frame_data)
                        
            except asyncio.TimeoutError:
                # å®šæœŸå‘é€ç»“æœ
                current_time = time.time()
                if current_time - last_send_time >= send_interval:
                    if detection_system.is_detecting:
                        results = detection_system.get_latest_results()
                        await websocket.send_json({
                            "type": "detection_result",
                            "data": results
                        })
                    last_send_time = current_time
                continue
                
    except WebSocketDisconnect:
        print("WebSocketè¿æ¥æ–­å¼€")
    except Exception as e:
        print(f"WebSocketé”™è¯¯: {e}")



@app.get("/api/status")
async def get_status():
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    return JSONResponse({
        "is_detecting": detection_system.is_detecting,
        "current_mode": detection_system.current_mode,
        "ffmpeg_available": detection_system.use_ffmpeg,
        "results": detection_system.get_latest_results()
    })

@app.post("/api/apply_preset")
async def apply_preset(mode: str = Form(...)):
    """åº”ç”¨é¢„è®¾æ¨¡å¼"""
    # åº”ç”¨æ£€æµ‹å‚æ•°é¢„è®¾
    detection_system.apply_preset(mode)

    # åŒæ—¶è®¾ç½®FFmpegå‹ç¼©å‚æ•°
    if mode == 'sensitive':
        # æ•æ„Ÿæ¨¡å¼ï¼šé«˜è´¨é‡ä½å»¶è¿Ÿè®¾ç½®
        settings = {
            'preset': 'ultrafast',
            'crf': '23',
            'scale': '480:360',
            'fps': '10'
        }
    elif mode == 'balanced':
        # å¹³è¡¡æ¨¡å¼ï¼šå¹³è¡¡è®¾ç½®
        settings = {
            'preset': 'veryfast',
            'crf': '28',
            'scale': '320:240',
            'fps': '5'
        }
    elif mode == 'conservative':
        # ä¿å®ˆæ¨¡å¼ï¼šé«˜å‹ç¼©ä½å¸¦å®½è®¾ç½®
        settings = {
            'preset': 'fast',
            'crf': '32',
            'scale': '240:180',
            'fps': '3'
        }

    detection_system.update_compression_settings(settings)

    # è¿”å›è¯¦ç»†çš„å‚æ•°ä¿¡æ¯
    return JSONResponse({
        "success": True,
        "message": f"å·²åˆ‡æ¢åˆ°{detection_system.current_mode}",
        "parameters": {
            "mar_threshold": detection_system.mar_threshold.get(),
            "ear_threshold": detection_system.ear_threshold.get(),
            "consecutive_threshold": detection_system.consecutive_threshold,
            "alert_cooldown": detection_system.alert_cooldown.get()
        }
    })

@app.get("/logout")
async def logout():
    """ç”¨æˆ·é€€å‡º"""
    detection_system.stop_detection()
    detection_system.current_user = None
    return RedirectResponse(url="/", status_code=302)

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨ç–²åŠ³æ£€æµ‹Webåº”ç”¨ - FFmpegä¼˜åŒ–ç‰ˆ")
    print("ğŸ“± è¯·åœ¨æµè§ˆå™¨ä¸­è®¿é—®: http://localhost:8000")
    if AI_MODULES_AVAILABLE:
        print("ğŸ”‘ æ•°æ®åº“è®¤è¯å·²å¯ç”¨")
        print("ğŸ“ å¯ä»¥æ³¨å†Œæ–°ç”¨æˆ·æˆ–ä½¿ç”¨ç°æœ‰è´¦æˆ·")
    else:
        print("ğŸ”‘ æµ‹è¯•è´¦æˆ·: test/123456, admin/admin")
    print("âš¡ FFmpegä¼˜åŒ–ç‰¹æ€§:")
    print("   - VP9è§†é¢‘ç¼–ç ")
    print("   - å¯è°ƒèŠ‚å‹ç¼©è´¨é‡")
    print("   - æ™ºèƒ½åˆ†è¾¨ç‡ç¼©æ”¾")
    print("   - å¸§ç‡ä¼˜åŒ–")
    print("   - å®æ—¶å‹ç¼©ç»Ÿè®¡")

    uvicorn.run(app, host="localhost", port=8000)
